[0m2021.06.26 18:47:12 INFO  Started: Metals version 0.10.4 in workspace '/home/hwait/dev/ht3' for client Visual Studio Code - Insiders 1.58.0-insider.[0m
[0m2021.06.26 18:47:15 INFO  time: initialize in 2.74s[0m
[0m2021.06.26 18:47:15 WARN  Build server is not auto-connectable.[0m
[0m2021.06.27 20:51:06 INFO  Started: Metals version 0.10.4 in workspace '/home/hwait/dev/ht3' for client Visual Studio Code - Insiders 1.58.0-insider.[0m
[0m2021.06.27 20:51:08 INFO  time: initialize in 2.42s[0m
[0m2021.06.27 20:51:08 WARN  Build server is not auto-connectable.[0m
[0m2021.06.27 20:51:09 INFO  skipping build import with status 'Requested'[0m
[0m2021.06.27 21:06:03 WARN  no build target for: /home/hwait/dev/ht3/src/main/scala/example/Hello.scala[0m
[0m2021.06.27 21:06:06 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
[0m2021.06.27 21:06:08 INFO  time: code lens generation in 5.39s[0m
[0m2021.06.27 21:06:08 INFO  time: code lens generation in 2.47s[0m
[0m2021.06.27 21:06:08 INFO  time: code lens generation in 1.62s[0m
[0m2021.06.27 21:06:37 WARN  no build target for: /home/hwait/dev/ht3/src/main/scala/example/Hello.scala[0m
[0m2021.06.27 21:07:10 WARN  no build target for: /home/hwait/dev/ht3/build.sbt[0m
Jun 27, 2021 9:07:25 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 86
[0m2021.06.27 21:09:26 WARN  no build target for: /home/hwait/dev/ht3/src/main/scala/example/Hello.scala[0m
[0m2021.06.27 21:10:05 WARN  no build target for: /home/hwait/dev/ht3/src/test/scala/example/HelloSpec.scala[0m
Jun 27, 2021 9:10:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 180
Jun 27, 2021 9:29:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 196
Jun 27, 2021 9:30:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 205
[0m2021.06.27 21:30:05 WARN  no build target for: /home/hwait/dev/ht3/build.sbt[0m
[0m2021.06.27 21:39:29 WARN  no build target for: /home/hwait/dev/ht3/build.sbt[0m
[0m2021.06.27 21:39:31 INFO  running '/usr/lib/jvm/java-11-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals11873049887499368935/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.06.27 21:39:32 WARN  no build target for: /home/hwait/dev/ht3/project/metals.sbt[0m
[0m2021.06.27 21:39:32 WARN  no build target for: /home/hwait/dev/ht3/project/project/metals.sbt[0m
[0m2021.06.27 21:39:33 INFO  [info] welcome to sbt 1.5.4 (Ubuntu Java 11.0.11)[0m
[0m2021.06.27 21:39:35 INFO  [info] loading global plugins from /home/hwait/.sbt/1.0/plugins[0m
[0m2021.06.27 21:39:38 INFO  [info] loading settings for project ht3-build-build from metals.sbt ...[0m
[0m2021.06.27 21:39:38 INFO  [info] loading project definition from /home/hwait/dev/ht3/project/project[0m
[0m2021.06.27 21:39:40 INFO  [info] loading settings for project ht3-build from metals.sbt ...[0m
[0m2021.06.27 21:39:40 INFO  [info] loading project definition from /home/hwait/dev/ht3/project[0m
[0m2021.06.27 21:39:44 INFO  [success] Generated .bloop/ht3-build.json[0m
[0m2021.06.27 21:39:44 INFO  [info] compiling 1 Scala source to /home/hwait/dev/ht3/project/target/scala-2.12/sbt-1.0/classes ...[0m
[0m2021.06.27 21:39:48 INFO  [info] done compiling[0m
[0m2021.06.27 21:39:48 INFO  [success] Total time: 8 s, completed Jun 27, 2021, 9:39:48 PM[0m
[0m2021.06.27 21:39:50 ERROR /home/hwait/dev/ht3/build.sbt:13: error: not found: value kafkaVer[0m
[0m2021.06.27 21:39:50 ERROR     "org.apache.kafka" % "kafka-streams" % kafkaVer,[0m
[0m2021.06.27 21:39:50 ERROR                                            ^[0m
[0m2021.06.27 21:39:50 ERROR /home/hwait/dev/ht3/build.sbt:14: error: not found: value kafkaVer[0m
[0m2021.06.27 21:39:50 ERROR     "org.apache.kafka" %% "kafka-streams-scala" % kafkaVer,[0m
[0m2021.06.27 21:39:50 ERROR                                                   ^[0m
[0m2021.06.27 21:39:50 INFO  [error] Type error in expression[0m
[0m2021.06.27 21:39:50 INFO  [warn] Project loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? (default: r)[0m
[0m2021.06.27 21:39:50 INFO  sbt bloopInstall exit: 1[0m
[0m2021.06.27 21:39:50 INFO  time: ran 'sbt bloopInstall' in 18s[0m
[0m2021.06.27 21:39:50 ERROR sbt command failed: /usr/lib/jvm/java-11-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals11873049887499368935/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall[0m
[0m2021.06.27 21:40:27 INFO  skipping build import with status 'Failed'[0m
[0m2021.06.27 21:41:53 WARN  no build target for: /home/hwait/dev/ht3/build.sbt[0m
[0m2021.06.27 21:41:55 WARN  no build target for: /home/hwait/dev/ht3/build.sbt[0m
[0m2021.06.27 21:41:56 INFO  running '/usr/lib/jvm/java-11-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals10416316176171581795/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.06.27 21:41:58 INFO  [info] welcome to sbt 1.5.4 (Ubuntu Java 11.0.11)[0m
[0m2021.06.27 21:41:59 INFO  [info] loading global plugins from /home/hwait/.sbt/1.0/plugins[0m
[0m2021.06.27 21:42:01 INFO  [info] loading settings for project ht3-build-build from metals.sbt ...[0m
[0m2021.06.27 21:42:01 INFO  [info] loading project definition from /home/hwait/dev/ht3/project/project[0m
[0m2021.06.27 21:42:01 INFO  [info] loading settings for project ht3-build from metals.sbt ...[0m
[0m2021.06.27 21:42:01 INFO  [info] loading project definition from /home/hwait/dev/ht3/project[0m
[0m2021.06.27 21:42:03 INFO  [success] Generated .bloop/ht3-build.json[0m
[0m2021.06.27 21:42:03 INFO  [success] Total time: 2 s, completed Jun 27, 2021, 9:42:04 PM[0m
[0m2021.06.27 21:42:06 INFO  [info] loading settings for project ht3 from build.sbt ...[0m
[0m2021.06.27 21:42:06 INFO  [info] set current project to ht3 (in build file:/home/hwait/dev/ht3/)[0m
[0m2021.06.27 21:42:09 INFO  [warn] [0m
[0m2021.06.27 21:42:09 INFO  [warn] 	Note: Unresolved dependencies path:[0m
[0m2021.06.27 21:42:09 INFO  [error] sbt.librarymanagement.ResolveException: Error downloading org.apache.kafka:kafka-streams-scala_2.12:2.13.0[0m
[0m2021.06.27 21:42:09 INFO  [error]   Not found[0m
[0m2021.06.27 21:42:09 INFO  [error]   Not found[0m
[0m2021.06.27 21:42:09 INFO  [error]   not found: /home/hwait/.ivy2/localorg.apache.kafka/kafka-streams-scala_2.12/2.13.0/ivys/ivy.xml[0m
[0m2021.06.27 21:42:09 INFO  [error]   not found: https://repo1.maven.org/maven2/org/apache/kafka/kafka-streams-scala_2.12/2.13.0/kafka-streams-scala_2.12-2.13.0.pom[0m
[0m2021.06.27 21:42:09 INFO  [error] Error downloading org.apache.kafka:kafka-streams:2.13.0[0m
[0m2021.06.27 21:42:09 INFO  [error]   Not found[0m
[0m2021.06.27 21:42:09 INFO  [error]   Not found[0m
[0m2021.06.27 21:42:09 INFO  [error]   not found: /home/hwait/.ivy2/localorg.apache.kafka/kafka-streams/2.13.0/ivys/ivy.xml[0m
[0m2021.06.27 21:42:09 INFO  [error]   not found: https://repo1.maven.org/maven2/org/apache/kafka/kafka-streams/2.13.0/kafka-streams-2.13.0.pom[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at lmcoursier.CoursierDependencyResolution.unresolvedWarningOrThrow(CoursierDependencyResolution.scala:258)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at lmcoursier.CoursierDependencyResolution.$anonfun$update$38(CoursierDependencyResolution.scala:227)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at scala.util.Either$LeftProjection.map(Either.scala:573)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at lmcoursier.CoursierDependencyResolution.update(CoursierDependencyResolution.scala:227)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.librarymanagement.DependencyResolution.update(DependencyResolution.scala:60)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.internal.LibraryManagement$.resolve$1(LibraryManagement.scala:59)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$12(LibraryManagement.scala:133)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.util.Tracked$.$anonfun$lastOutput$1(Tracked.scala:73)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$20(LibraryManagement.scala:146)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at scala.util.control.Exception$Catch.apply(Exception.scala:228)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11(LibraryManagement.scala:146)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11$adapted(LibraryManagement.scala:127)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.util.Tracked$.$anonfun$inputChangedW$1(Tracked.scala:219)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.internal.LibraryManagement$.cachedUpdate(LibraryManagement.scala:160)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.Classpaths$.$anonfun$updateTask0$1(Defaults.scala:3678)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:68)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:282)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:23)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.Execute.work(Execute.scala:291)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:282)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:265)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:64)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.06.27 21:42:09 INFO  [error] 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.06.27 21:42:09 INFO  [error] (update) sbt.librarymanagement.ResolveException: Error downloading org.apache.kafka:kafka-streams-scala_2.12:2.13.0[0m
[0m2021.06.27 21:42:09 INFO  [error]   Not found[0m
[0m2021.06.27 21:42:09 INFO  [error]   Not found[0m
[0m2021.06.27 21:42:09 INFO  [error]   not found: /home/hwait/.ivy2/localorg.apache.kafka/kafka-streams-scala_2.12/2.13.0/ivys/ivy.xml[0m
[0m2021.06.27 21:42:09 INFO  [error]   not found: https://repo1.maven.org/maven2/org/apache/kafka/kafka-streams-scala_2.12/2.13.0/kafka-streams-scala_2.12-2.13.0.pom[0m
[0m2021.06.27 21:42:09 INFO  [error] Error downloading org.apache.kafka:kafka-streams:2.13.0[0m
[0m2021.06.27 21:42:09 INFO  [error]   Not found[0m
[0m2021.06.27 21:42:09 INFO  [error]   Not found[0m
[0m2021.06.27 21:42:09 INFO  [error]   not found: /home/hwait/.ivy2/localorg.apache.kafka/kafka-streams/2.13.0/ivys/ivy.xml[0m
[0m2021.06.27 21:42:09 INFO  [error]   not found: https://repo1.maven.org/maven2/org/apache/kafka/kafka-streams/2.13.0/kafka-streams-2.13.0.pom[0m
[0m2021.06.27 21:42:09 INFO  [error] Total time: 3 s, completed Jun 27, 2021, 9:42:09 PM[0m
[0m2021.06.27 21:42:09 INFO  sbt bloopInstall exit: 1[0m
[0m2021.06.27 21:42:09 INFO  time: ran 'sbt bloopInstall' in 12s[0m
[0m2021.06.27 21:42:09 ERROR sbt command failed: /usr/lib/jvm/java-11-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals10416316176171581795/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall[0m
[0m2021.06.27 22:54:13 WARN  no build target for: /home/hwait/dev/ht3/build.sbt[0m
[0m2021.06.27 22:54:14 INFO  running '/usr/lib/jvm/java-11-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals15251398710193552234/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.06.27 22:54:16 INFO  [info] welcome to sbt 1.5.4 (Ubuntu Java 11.0.11)[0m
[0m2021.06.27 22:54:18 INFO  [info] loading global plugins from /home/hwait/.sbt/1.0/plugins[0m
[0m2021.06.27 22:54:19 INFO  [info] loading settings for project ht3-build-build from metals.sbt ...[0m
[0m2021.06.27 22:54:19 INFO  [info] loading project definition from /home/hwait/dev/ht3/project/project[0m
[0m2021.06.27 22:54:19 INFO  [info] loading settings for project ht3-build from metals.sbt ...[0m
[0m2021.06.27 22:54:19 INFO  [info] loading project definition from /home/hwait/dev/ht3/project[0m
[0m2021.06.27 22:54:22 INFO  [success] Generated .bloop/ht3-build.json[0m
[0m2021.06.27 22:54:22 INFO  [success] Total time: 2 s, completed Jun 27, 2021, 10:54:22 PM[0m
[0m2021.06.27 22:54:26 INFO  [info] loading settings for project ht3 from build.sbt ...[0m
[0m2021.06.27 22:54:26 INFO  [info] set current project to ht3 (in build file:/home/hwait/dev/ht3/)[0m
[0m2021.06.27 22:54:45 INFO  [success] Generated .bloop/ht3.json[0m
[0m2021.06.27 22:54:45 INFO  [success] Generated .bloop/ht3-test.json[0m
[0m2021.06.27 22:54:45 INFO  [success] Total time: 19 s, completed Jun 27, 2021, 10:54:45 PM[0m
[0m2021.06.27 22:54:45 INFO  sbt bloopInstall exit: 0[0m
[0m2021.06.27 22:54:46 INFO  time: ran 'sbt bloopInstall' in 31s[0m
[0m2021.06.27 22:54:46 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4965899895743349791/bsp.socket'...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8-19-4d9f966b...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.12.10.
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.6/jline-2.14.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.12/jansi-1.12.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.10/scala-compiler-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.10/scala-reflect-2.12.10.jar
[0m[32m[D][0m Missing analysis file for project 'ht3'
[0m[32m[D][0m Missing analysis file for project 'ht3-test'
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4965899895743349791/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4965899895743349791/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.06.27 22:54:49 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.06.27 22:54:49 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher17316966161774544299/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher17316966161774544299/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher17316966161774544299/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.06.27 22:54:50 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
Jun 27, 2021 10:54:54 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 689
[0m2021.06.27 22:54:55 INFO  time: Connected to build server in 9.21s[0m
[0m2021.06.27 22:54:55 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
[0m2021.06.27 22:54:55 INFO  time: Imported build in 0.13s[0m
[0m2021.06.27 22:54:58 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.06.27 22:55:03 INFO  time: indexed workspace in 7.88s[0m
[0m2021.06.27 22:55:04 INFO  compiling ht3-build (1 scala source)[0m
[0m2021.06.27 22:55:04 INFO  compiling ht3 (1 scala source)[0m
[0m2021.06.27 22:55:04 INFO  Non-compiled module 'compiler-bridge_2.12' for Scala 2.12.14. Compiling...[0m
[0m2021.06.27 22:55:08 INFO  time: compiled ht3 in 3.62s[0m
[0m2021.06.27 22:55:17 INFO    Compilation completed in 13.012s.[0m
[0m2021.06.27 22:55:18 INFO  compiling ht3 (1 scala source)[0m
[0m2021.06.27 22:55:18 INFO  time: compiled ht3 in 0.5s[0m
[0m2021.06.27 22:55:19 INFO  time: compiled ht3-build in 15s[0m
Jun 27, 2021 11:41:04 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 749
[0m2021.06.27 23:41:13 INFO  running '/usr/lib/jvm/java-11-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals245606218725605779/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.06.27 23:41:14 INFO  [info] welcome to sbt 1.5.4 (Ubuntu Java 11.0.11)[0m
[0m2021.06.27 23:41:16 INFO  [info] loading global plugins from /home/hwait/.sbt/1.0/plugins[0m
[0m2021.06.27 23:41:17 INFO  [info] loading settings for project ht3-build-build from metals.sbt ...[0m
[0m2021.06.27 23:41:17 INFO  [info] loading project definition from /home/hwait/dev/ht3/project/project[0m
[0m2021.06.27 23:41:17 INFO  [info] loading settings for project ht3-build from metals.sbt ...[0m
[0m2021.06.27 23:41:17 INFO  [info] loading project definition from /home/hwait/dev/ht3/project[0m
[0m2021.06.27 23:41:20 INFO  [success] Generated .bloop/ht3-build.json[0m
[0m2021.06.27 23:41:20 INFO  [success] Total time: 2 s, completed Jun 27, 2021, 11:41:20 PM[0m
[0m2021.06.27 23:41:23 INFO  [info] loading settings for project ht3 from build.sbt ...[0m
[0m2021.06.27 23:41:23 INFO  [info] set current project to ht3 (in build file:/home/hwait/dev/ht3/)[0m
[0m2021.06.27 23:41:32 INFO  [success] Generated .bloop/ht3-test.json[0m
[0m2021.06.27 23:41:32 INFO  [success] Generated .bloop/ht3.json[0m
[0m2021.06.27 23:41:32 INFO  [success] Total time: 9 s, completed Jun 27, 2021, 11:41:32 PM[0m
[0m2021.06.27 23:41:32 INFO  sbt bloopInstall exit: 0[0m
[0m2021.06.27 23:41:32 INFO  time: ran 'sbt bloopInstall' in 19s[0m
[0m2021.06.27 23:41:32 INFO  Disconnecting from Bloop session...[0m
[0m2021.06.27 23:41:32 INFO  Shut down connection with build server.[0m
[0m2021.06.27 23:41:32 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.06.27 23:41:32 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher51754703775984726/bsp.socket'...
Waiting for the bsp connection to come up...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Configured SemanticDB in projects 'ht3', 'ht3-test'
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher51754703775984726/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher51754703775984726/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.06.27 23:41:32 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.06.27 23:41:32 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4217170820255712177/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4217170820255712177/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4217170820255712177/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.06.27 23:41:32 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.06.27 23:41:32 INFO  time: Connected to build server in 0.22s[0m
[0m2021.06.27 23:41:32 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
[0m2021.06.27 23:41:36 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.06.27 23:41:36 INFO  time: indexed workspace in 3.41s[0m
[0m2021.06.27 23:42:24 INFO  compiling ht3 (1 scala source)[0m
[0m2021.06.27 23:42:29 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.06.27 23:42:29 INFO  time: compiled ht3 in 5.05s[0m
[0m2021.06.27 23:42:29 INFO  time: compiled ht3-test in 0.39s[0m
Jun 27, 2021 11:42:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 784
Jun 27, 2021 11:50:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 810
Jun 27, 2021 11:52:58 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 825
Jun 27, 2021 11:53:12 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 832
Jun 27, 2021 11:53:28 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 867
Jun 27, 2021 11:53:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 877
Jun 27, 2021 11:54:12 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 916
[0m2021.06.27 23:54:12 INFO  compiling ht3 (1 scala source)[0m
[0m2021.06.27 23:54:15 INFO  time: compiled ht3 in 2.85s[0m
[0m2021.06.27 23:56:16 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.06.27 23:56:16 INFO  time: compiled ht3-test in 0.46s[0m
[0m2021.06.27 23:56:21 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.06.27 23:56:21 INFO  time: compiled ht3-test in 0.45s[0m
[0m2021.06.27 23:56:22 ERROR code navigation does not work for the file '/home/hwait/dev/ht3/src/test/scala/ru.otus.bigdataml.ht3/HelloSpec.scala' because the SemanticDB file '/home/hwait/dev/ht3/.bloop/ht3/bloop-bsp-clients-classes/test-classes-Metals-FECruDOCQYiVkn6ZYNADAw==/META-INF/semanticdb/src/test/scala/ru.otus.bigdataml.ht3/HelloSpec.scala.semanticdb' doesn't exist. There can be many reasons for this error. [0m
[0m2021.06.27 23:56:35 WARN  no build target for: /home/hwait/dev/ht3/project/metals.sbt[0m
[0m2021.06.27 23:56:37 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
Jun 27, 2021 11:56:37 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 951
[0m2021.06.27 23:56:37 INFO  time: code lens generation in 1.84s[0m
Jun 27, 2021 11:56:42 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 960
[0m2021.06.27 23:56:50 INFO  running '/usr/lib/jvm/java-11-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals13427807900978225748/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
Jun 27, 2021 11:56:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 982
[0m2021.06.27 23:56:52 INFO  [info] welcome to sbt 1.5.4 (Ubuntu Java 11.0.11)[0m
[0m2021.06.27 23:56:53 INFO  [info] loading global plugins from /home/hwait/.sbt/1.0/plugins[0m
[0m2021.06.27 23:56:55 INFO  [info] loading settings for project ht3-build-build from metals.sbt ...[0m
[0m2021.06.27 23:56:55 INFO  [info] loading project definition from /home/hwait/dev/ht3/project/project[0m
Jun 27, 2021 11:56:55 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 988
[0m2021.06.27 23:56:55 INFO  [info] loading settings for project ht3-build from metals.sbt ...[0m
[0m2021.06.27 23:56:55 INFO  [info] loading project definition from /home/hwait/dev/ht3/project[0m
[0m2021.06.27 23:56:58 INFO  [success] Generated .bloop/ht3-build.json[0m
[0m2021.06.27 23:56:58 INFO  [success] Total time: 2 s, completed Jun 27, 2021, 11:56:58 PM[0m
[0m2021.06.27 23:57:00 INFO  [info] loading settings for project ht3 from build.sbt ...[0m
[0m2021.06.27 23:57:00 INFO  [info] set current project to ht3 (in build file:/home/hwait/dev/ht3/)[0m
[0m2021.06.27 23:57:11 INFO  compiling ht3 (1 scala source)[0m
Jun 27, 2021 11:57:11 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 994
[0m2021.06.27 23:57:12 INFO  [success] Generated .bloop/ht3-test.json[0m
[0m2021.06.27 23:57:12 INFO  [success] Generated .bloop/ht3.json[0m
[0m2021.06.27 23:57:12 INFO  [success] Total time: 11 s, completed Jun 27, 2021, 11:57:12 PM[0m
[0m2021.06.27 23:57:12 INFO  sbt bloopInstall exit: 0[0m
[0m2021.06.27 23:57:12 INFO  time: ran 'sbt bloopInstall' in 22s[0m
[0m2021.06.27 23:57:12 INFO  Disconnecting from Bloop session...[0m
[0m2021.06.27 23:57:12 INFO  Shut down connection with build server.[0m
[0m2021.06.27 23:57:12 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.06.27 23:57:12 INFO  Attempting to connect to the build server...[0m
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher15365477609636561549/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Configured SemanticDB in projects 'ht3', 'ht3-test'
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher15365477609636561549/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher15365477609636561549/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.06.27 23:57:12 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.06.27 23:57:12 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher17816975192267340595/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher17816975192267340595/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher17816975192267340595/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.06.27 23:57:12 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.06.27 23:57:12 INFO  time: Connected to build server in 0.18s[0m
[0m2021.06.27 23:57:12 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
[0m2021.06.27 23:57:16 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.06.27 23:57:16 INFO  time: indexed workspace in 4s[0m
[0m2021.06.27 23:57:17 INFO  compiling ht3 (1 scala source)[0m
[0m2021.06.27 23:57:18 INFO  time: compiled ht3 in 1.11s[0m
[0m2021.06.27 23:57:18 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.06.27 23:57:19 INFO  time: compiled ht3-test in 1.57s[0m
Jun 27, 2021 11:57:24 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 9 is not a valid line number, allowed [0..7]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 9 is not a valid line number, allowed [0..7]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 9 is not a valid line number, allowed [0..7]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:80)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jun 27, 2021 11:57:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 9 is not a valid line number, allowed [0..7]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 9 is not a valid line number, allowed [0..7]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 9 is not a valid line number, allowed [0..7]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:80)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jun 27, 2021 11:57:32 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 10 is not a valid line number, allowed [0..7]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 10 is not a valid line number, allowed [0..7]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 10 is not a valid line number, allowed [0..7]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:80)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jun 27, 2021 11:57:32 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 10 is not a valid line number, allowed [0..7]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 10 is not a valid line number, allowed [0..7]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 10 is not a valid line number, allowed [0..7]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:80)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.06.27 23:58:02 INFO  compiling ht3 (1 scala source)[0m
[0m2021.06.27 23:58:02 INFO  time: compiled ht3 in 0.91s[0m
[0m2021.06.27 23:58:05 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.06.27 23:58:05 INFO  time: compiled ht3-test in 0.33s[0m
[0m2021.06.27 23:58:05 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.06.27 23:58:05 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.06.27 23:58:05 INFO  Listening for transport dt_socket at address: 36577[0m
[0m2021.06.27 23:58:05 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.06.27 23:58:05 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProducer][0m
[0m2021.06.27 23:58:05 INFO  Trying to attach to remote debuggee VM localhost:36577 .[0m
[0m2021.06.27 23:58:05 INFO  time: compiled ht3-test in 0.29s[0m
[0m2021.06.27 23:58:06 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.06.27 23:58:06 INFO  spark_ml_poc[0m
[0m2021.06.27 23:58:06 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProducer][0m
Jun 27, 2021 11:58:10 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1132
[0m2021.06.27 23:58:25 INFO  compiling ht3-test (1 scala source)[0m
Jun 27, 2021 11:58:25 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1137
[0m2021.06.27 23:58:25 INFO  time: compiled ht3-test in 0.22s[0m
[0m2021.06.27 23:58:25 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.06.27 23:58:25 INFO  time: compiled ht3-test in 0.15s[0m
[0m2021.06.27 23:58:42 INFO  compiling ht3 (1 scala source)[0m
[0m2021.06.27 23:58:43 INFO  time: compiled ht3 in 1.69s[0m
[0m2021.06.27 23:58:48 ERROR code navigation does not work for the file '/home/hwait/dev/ht3/src/test/scala/ru.otus.bigdataml.ht3/DataProviderSpec.scala' because the SemanticDB file '/home/hwait/dev/ht3/.bloop/ht3/bloop-bsp-clients-classes/test-classes-Metals-0sBPVQCwT7yu0e93yCySkg==/META-INF/semanticdb/src/test/scala/ru.otus.bigdataml.ht3/DataProviderSpec.scala.semanticdb' doesn't exist. There can be many reasons for this error. [0m
Jun 27, 2021 11:58:50 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1160
[0m2021.06.27 23:58:51 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.06.27 23:58:51 INFO  time: compiled ht3-test in 0.24s[0m
[0m2021.06.27 23:58:51 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.06.27 23:58:51 INFO  time: compiled ht3-test in 0.15s[0m
[0m2021.06.27 23:58:53 INFO  compiling ht3 (1 scala source)[0m
[0m2021.06.27 23:58:53 INFO  time: compiled ht3 in 0.73s[0m
Jun 27, 2021 11:58:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1180
[0m2021.06.27 23:58:56 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.06.27 23:58:56 INFO  time: compiled ht3-test in 0.26s[0m
[0m2021.06.27 23:58:56 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.06.27 23:58:56 INFO  time: compiled ht3-test in 0.15s[0m
Jun 27, 2021 11:59:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1205
[0m2021.06.28 00:05:27 INFO  compiling ht3 (1 scala source)[0m
[0m2021.06.28 00:05:30 INFO  time: compiled ht3 in 2.83s[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala in Array[String]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala, 227, 227, 240)
[0m2021.06.28 00:12:03 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.06.28 00:12:03 INFO  time: compiled ht3-test in 0.95s[0m
[0m2021.06.28 00:14:00 INFO  compiling ht3 (1 scala source)[0m
[0m2021.06.28 00:14:01 INFO  time: compiled ht3 in 1.3s[0m
Jun 28, 2021 12:14:02 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1405
[0m2021.06.28 00:14:02 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.06.28 00:14:02 INFO  time: compiled ht3-test in 0.27s[0m
Jun 28, 2021 12:18:12 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1414
[0m2021.06.28 00:20:04 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 22m 51.35s)[0m
[0m2021.06.28 00:20:04 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.06.28 00:20:05 INFO  time: compiled ht3-test in 1.34s[0m
[0m2021.06.28 00:20:05 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.06.28 00:20:05 INFO  Listening for transport dt_socket at address: 55431[0m
[0m2021.06.28 00:20:05 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.06.28 00:20:05 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.06.28 00:20:06 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.06.28 00:20:05 INFO  Trying to attach to remote debuggee VM localhost:55431 .[0m
[0m2021.06.28 00:20:05 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.06.28 00:20:05 INFO  time: compiled ht3-test in 0.43s[0m
[0m2021.06.28 00:20:06 ERROR log4j:WARN No appenders could be found for logger (org.apache.kafka.clients.producer.ProducerConfig).[0m
[0m2021.06.28 00:20:06 ERROR log4j:WARN Please initialize the log4j system properly.[0m
[0m2021.06.28 00:20:06 ERROR log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.[0m
[0m2021.06.28 00:20:06 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.SymbolIndexBucket.query0(SymbolIndexBucket.scala:149)
	at scala.meta.internal.mtags.SymbolIndexBucket.query(SymbolIndexBucket.scala:129)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.$anonfun$findSymbolDefinition$1(OnDemandSymbolIndex.scala:116)
	at scala.collection.immutable.List.flatMap(List.scala:366)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:116)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definitions(OnDemandSymbolIndex.scala:48)
	at scala.meta.internal.metals.DestinationProvider.definition(DefinitionProvider.scala:296)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:334)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:354)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:103)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.06.28 00:20:07 INFO  Pushed Setosa Record: {"SepalLength":1}[0m
[0m2021.06.28 00:20:07 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.06.28 01:29:08 INFO  Started: Metals version 0.10.2 in workspace '/home/hwait/dev/ht3' for client vscode 1.57.1.[0m
[0m2021.06.28 01:29:11 INFO  time: initialize in 2.35s[0m
[0m2021.06.28 01:29:12 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5102488747930909662/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.06.28 01:29:12 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8-19-4d9f966b...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.12.10.
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.6/jline-2.14.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.12/jansi-1.12.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.10/scala-compiler-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.10/scala-reflect-2.12.10.jar
[0m[32m[D][0m Configured SemanticDB in projects 'ht3', 'ht3-test'
[0m[32m[D][0m Loading previous analysis for 'ht3-test' from '/home/hwait/dev/ht3/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Loading previous analysis for 'ht3' from '/home/hwait/dev/ht3/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5102488747930909662/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5102488747930909662/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.06.28 01:29:15 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.06.28 01:29:15 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4275256480306301578/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4275256480306301578/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4275256480306301578/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.06.28 01:29:16 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.06.28 01:29:16 INFO  time: Connected to build server in 4.35s[0m
[0m2021.06.28 01:29:16 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
[0m2021.06.28 01:29:16 INFO  time: Imported build in 0.25s[0m
[0m2021.06.28 01:29:20 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.06.28 01:29:20 INFO  time: indexed workspace in 4.29s[0m
[0m2021.06.28 01:30:31 INFO  compiling ht3 (1 scala source)[0m
[0m2021.06.28 01:30:35 INFO  time: compiled ht3 in 4s[0m
[0m2021.06.28 01:30:36 WARN  unsupported Scala 2.12.14[0m
[0m2021.06.28 01:30:36 WARN  unsupported Scala 2.12.14[0m
[0m2021.06.28 01:30:36 WARN  unsupported Scala 2.12.14[0m
[0m2021.06.28 01:30:36 INFO  compiling ht3-build (1 scala source)[0m
[0m2021.06.28 01:30:37 WARN  unsupported Scala 2.12.14[0m
[0m2021.06.28 01:30:38 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
[0m2021.06.28 01:30:39 INFO  time: compiled ht3-build in 3.04s[0m
[0m2021.06.28 01:30:39 INFO  time: code lens generation in 3.25s[0m
[0m2021.06.28 01:32:12 WARN  unsupported Scala 2.12.14[0m
[0m2021.06.28 01:32:15 WARN  unsupported Scala 2.12.14[0m
Jun 28, 2021 1:32:15 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 19
[0m2021.06.28 01:32:17 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.06.28 01:32:17 INFO  time: compiled ht3-test in 0.65s[0m
[0m2021.06.28 01:32:18 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.06.28 01:32:17 INFO  Listening for transport dt_socket at address: 52469[0m
[0m2021.06.28 01:32:18 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.06.28 01:32:18 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.06.28 01:32:18 INFO  Trying to attach to remote debuggee VM localhost:52469 .[0m
[0m2021.06.28 01:32:18 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.06.28 01:32:18 ERROR log4j:WARN No appenders could be found for logger (org.apache.kafka.clients.producer.ProducerConfig).[0m
[0m2021.06.28 01:32:18 ERROR log4j:WARN Please initialize the log4j system properly.[0m
[0m2021.06.28 01:32:18 ERROR log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.[0m
[0m2021.06.28 01:32:18 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.06.28 01:32:19 INFO  Pushed Setosa Record: {"SepalLength":1}[0m
[0m2021.06.28 01:32:19 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.06.28 01:32:40 INFO  shutting down Metals[0m
[0m2021.06.28 01:32:40 INFO  Shut down connection with build server.[0m
[0m2021.06.28 01:32:40 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.07.01 11:32:07 INFO  Started: Metals version 0.10.2 in workspace '/home/hwait/dev/ht3' for client vscode 1.57.1.[0m
[0m2021.07.01 11:32:09 INFO  time: initialize in 2.06s[0m
[0m2021.07.01 11:32:09 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.07.01 11:32:09 WARN  no build target for: /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5631738374637175366/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.07.01 11:32:09 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.07.01 11:32:12 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8-19-4d9f966b...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.12.10.
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.6/jline-2.14.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.12/jansi-1.12.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.10/scala-compiler-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.10/scala-reflect-2.12.10.jar
[0m[32m[D][0m Configured SemanticDB in projects 'ht3-test', 'ht3'
[0m[32m[D][0m Loading previous analysis for 'ht3' from '/home/hwait/dev/ht3/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Loading previous analysis for 'ht3-test' from '/home/hwait/dev/ht3/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5631738374637175366/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5631738374637175366/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.01 11:32:13 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.01 11:32:13 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher14089667291526349886/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher14089667291526349886/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher14089667291526349886/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.01 11:32:14 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.01 11:32:14 INFO  time: Connected to build server in 4.24s[0m
[0m2021.07.01 11:32:14 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
[0m2021.07.01 11:32:14 INFO  time: Imported build in 0.31s[0m
[0m2021.07.01 11:32:16 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
[0m2021.07.01 11:32:16 INFO  time: code lens generation in 6.39s[0m
[0m2021.07.01 11:32:16 INFO  time: code lens generation in 5.19s[0m
[0m2021.07.01 11:32:17 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.07.01 11:32:17 INFO  time: indexed workspace in 4.11s[0m
[0m2021.07.01 11:32:20 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 11:32:22 INFO  time: compiled ht3-test in 1.92s[0m
[0m2021.07.01 12:22:02 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 49m 48.13s)[0m
[0m2021.07.01 12:22:02 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 12:22:03 INFO  time: compiled ht3-test in 1.41s[0m
[0m2021.07.01 12:22:03 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 12:22:03 INFO  Listening for transport dt_socket at address: 59097[0m
[0m2021.07.01 12:22:03 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 12:22:04 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 12:22:03 INFO  Trying to attach to remote debuggee VM localhost:59097 .[0m
[0m2021.07.01 12:22:03 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 12:22:04 ERROR log4j:WARN No appenders could be found for logger (org.apache.kafka.clients.producer.ProducerConfig).[0m
[0m2021.07.01 12:22:04 ERROR log4j:WARN Please initialize the log4j system properly.[0m
[0m2021.07.01 12:22:04 ERROR log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.[0m
[0m2021.07.01 12:22:04 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 12:22:05 INFO  Pushed Setosa Record: {"SepalLength":1}[0m
[0m2021.07.01 12:22:05 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 12:48:57 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 12:48:59 INFO  time: compiled ht3 in 2.19s[0m
[0m2021.07.01 12:50:31 INFO  compiling ht3 (1 scala source)[0m
Jul 01, 2021 12:50:32 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 110
[0m2021.07.01 12:50:31 INFO  time: compiled ht3 in 0.9s[0m
Jul 01, 2021 12:52:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 226
[0m2021.07.01 12:53:07 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 12:53:07 INFO  time: compiled ht3 in 0.55s[0m
Jul 01, 2021 12:53:28 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 261
Jul 01, 2021 12:53:36 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 274
[0m2021.07.01 12:53:51 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 12:53:51 INFO  time: compiled ht3 in 0.47s[0m
[0m2021.07.01 12:53:59 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 12:53:59 INFO  time: compiled ht3 in 0.32s[0m
[0m2021.07.01 12:55:10 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:10 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:10 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:11 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:45 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:46 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:47 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:47 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:49 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:49 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:51 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:52 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:52 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:52 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:52 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:53 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:54 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:58 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:55:59 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:00 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:00 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:02 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:02 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:03 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:03 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:04 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:04 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:05 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:05 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:05 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:05 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:07 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:10 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:10 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:11 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:13 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:13 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:13 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:13 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:13 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:16 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:16 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:19 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:19 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:19 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:21 INFO  running '/usr/lib/jvm/java-11-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals10531301631937043867/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.07.01 12:56:23 INFO  [info] welcome to sbt 1.5.4 (Ubuntu Java 11.0.11)[0m
Jul 01, 2021 12:56:23 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 436
[0m2021.07.01 12:56:24 INFO  [info] loading global plugins from /home/hwait/.sbt/1.0/plugins[0m
[0m2021.07.01 12:56:26 INFO  [info] loading settings for project ht3-build-build from metals.sbt ...[0m
[0m2021.07.01 12:56:26 INFO  [info] loading project definition from /home/hwait/dev/ht3/project/project[0m
[0m2021.07.01 12:56:26 INFO  [info] loading settings for project ht3-build from metals.sbt ...[0m
[0m2021.07.01 12:56:26 INFO  [info] loading project definition from /home/hwait/dev/ht3/project[0m
[0m2021.07.01 12:56:28 INFO  [success] Generated .bloop/ht3-build.json[0m
[0m2021.07.01 12:56:28 INFO  [success] Total time: 2 s, completed Jul 1, 2021, 12:56:29 PM[0m
Jul 01, 2021 12:56:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 443
[0m2021.07.01 12:56:31 INFO  [info] loading settings for project ht3 from build.sbt ...[0m
[0m2021.07.01 12:56:31 INFO  [info] set current project to ht3 (in build file:/home/hwait/dev/ht3/)[0m
Jul 01, 2021 12:56:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 449
[0m2021.07.01 12:56:42 INFO  [success] Generated .bloop/ht3.json[0m
[0m2021.07.01 12:56:42 INFO  [success] Generated .bloop/ht3-test.json[0m
[0m2021.07.01 12:56:42 INFO  [success] Total time: 10 s, completed Jul 1, 2021, 12:56:42 PM[0m
[0m2021.07.01 12:56:42 INFO  sbt bloopInstall exit: 0[0m
[0m2021.07.01 12:56:43 INFO  time: ran 'sbt bloopInstall' in 21s[0m
[0m2021.07.01 12:56:43 INFO  Disconnecting from Bloop session...[0m
[0m2021.07.01 12:56:43 INFO  Shut down connection with build server.[0m
[0m2021.07.01 12:56:43 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.07.01 12:56:43 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher12486309361481213745/bsp.socket'...
Waiting for the bsp connection to come up...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Configured SemanticDB in projects 'ht3-test', 'ht3'
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher12486309361481213745/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher12486309361481213745/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.01 12:56:43 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.01 12:56:43 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6489364064223324973/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6489364064223324973/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6489364064223324973/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.01 12:56:43 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.01 12:56:43 INFO  time: Connected to build server in 0.19s[0m
[0m2021.07.01 12:56:43 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
[0m2021.07.01 12:56:43 ERROR code navigation does not work for the file '/home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLRegressionFromKafka.scala' because the SemanticDB file '/home/hwait/dev/ht3/.bloop/ht3/bloop-bsp-clients-classes/classes-Metals-iXOnJzdkSUaNnQVm87Y4Qg==/META-INF/semanticdb/src/main/scala/ru.otus.bigdataml.ht3/SparkMLRegressionFromKafka.scala.semanticdb' doesn't exist. There can be many reasons for this error. [0m
[0m2021.07.01 12:56:46 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.07.01 12:56:46 INFO  time: indexed workspace in 3.38s[0m
[0m2021.07.01 12:56:46 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:56:46 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 3.616s)[0m
[0m2021.07.01 12:56:46 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 12:56:46 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 12:56:48 INFO  time: compiled ht3 in 1.18s[0m
[0m2021.07.01 12:57:32 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 12:57:34 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Jul 01, 2021 12:57:34 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 484
[0m2021.07.01 12:57:40 WARN  unsupported Scala 2.12.14[0m
Jul 01, 2021 12:57:41 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 492
Jul 01, 2021 12:57:42 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 498
[0m2021.07.01 12:59:13 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 12:59:13 INFO  time: compiled ht3 in 0.41s[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLRegressionFromKafka.scala in Array[String]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLRegressionFromKafka.scala, 839, 839, 852)
Jul 01, 2021 1:02:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 644
[0m2021.07.01 13:02:08 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:02:08 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:02:11 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:02:11 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:02:39 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:02:42 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:02:42 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:02:44 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:02:44 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:02:45 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 13:02:45 INFO  time: compiled ht3 in 0.79s[0m
[0m2021.07.01 13:02:47 INFO  running '/usr/lib/jvm/java-11-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals2263593967846942298/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.07.01 13:02:48 INFO  [info] welcome to sbt 1.5.4 (Ubuntu Java 11.0.11)[0m
[0m2021.07.01 13:02:50 INFO  [info] loading global plugins from /home/hwait/.sbt/1.0/plugins[0m
[0m2021.07.01 13:02:51 INFO  [info] loading settings for project ht3-build-build from metals.sbt ...[0m
[0m2021.07.01 13:02:51 INFO  [info] loading project definition from /home/hwait/dev/ht3/project/project[0m
[0m2021.07.01 13:02:51 INFO  [info] loading settings for project ht3-build from metals.sbt ...[0m
[0m2021.07.01 13:02:51 INFO  [info] loading project definition from /home/hwait/dev/ht3/project[0m
[0m2021.07.01 13:02:54 INFO  [success] Generated .bloop/ht3-build.json[0m
[0m2021.07.01 13:02:54 INFO  [success] Total time: 2 s, completed Jul 1, 2021, 1:02:54 PM[0m
[0m2021.07.01 13:02:56 INFO  [info] loading settings for project ht3 from build.sbt ...[0m
[0m2021.07.01 13:02:56 INFO  [info] set current project to ht3 (in build file:/home/hwait/dev/ht3/)[0m
[0m2021.07.01 13:03:07 INFO  [success] Generated .bloop/ht3.json[0m
[0m2021.07.01 13:03:07 INFO  [success] Generated .bloop/ht3-test.json[0m
[0m2021.07.01 13:03:07 INFO  [success] Total time: 10 s, completed Jul 1, 2021, 1:03:07 PM[0m
[0m2021.07.01 13:03:07 INFO  sbt bloopInstall exit: 0[0m
[0m2021.07.01 13:03:08 INFO  time: ran 'sbt bloopInstall' in 21s[0m
[0m2021.07.01 13:03:08 INFO  Disconnecting from Bloop session...[0m
[0m2021.07.01 13:03:08 INFO  Shut down connection with build server.[0m
[0m2021.07.01 13:03:08 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.07.01 13:03:08 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8356700938131283067/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Configured SemanticDB in projects 'ht3', 'ht3-test'
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8356700938131283067/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8356700938131283067/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.01 13:03:08 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.01 13:03:08 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher10831528220561354396/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher10831528220561354396/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher10831528220561354396/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.01 13:03:08 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.01 13:03:08 INFO  time: Connected to build server in 0.21s[0m
[0m2021.07.01 13:03:08 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
Jul 01, 2021 1:03:10 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 677
[0m2021.07.01 13:03:11 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.07.01 13:03:11 INFO  time: indexed workspace in 3.39s[0m
[0m2021.07.01 13:03:11 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:03:12 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 13:03:12 INFO  time: compiled ht3 in 0.5s[0m
[0m2021.07.01 13:03:20 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 13:03:20 INFO  time: compiled ht3 in 0.46s[0m
Jul 01, 2021 1:03:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 714
[0m2021.07.01 13:03:39 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 31.383s)[0m
[0m2021.07.01 13:03:39 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 13:03:39 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 13:03:39 INFO  time: compiled ht3 in 0.32s[0m
[0m2021.07.01 13:03:39 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 13:03:39 INFO  time: compiled ht3 in 0.27s[0m
Jul 01, 2021 1:03:40 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
java.util.concurrent.CompletionException: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:29)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:26)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:237)
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:229)
	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417)
	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
	... 4 more

Jul 01, 2021 1:03:46 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 736
Jul 01, 2021 1:04:02 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 751
[0m2021.07.01 13:04:34 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:04:35 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Jul 01, 2021 1:04:36 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 758
[0m2021.07.01 13:04:36 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:04:42 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:04:42 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:04:43 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:04:43 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:04:45 WARN  unsupported Scala 2.12.14[0m
Jul 01, 2021 1:04:45 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 772
Jul 01, 2021 1:05:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 789
[0m2021.07.01 13:05:06 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:06 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:06 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:07 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:07 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:09 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:09 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:12 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:12 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:20 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:20 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:27 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:27 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:28 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:28 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:28 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:28 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:29 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:29 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:29 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:31 INFO  running '/usr/lib/jvm/java-11-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals10746637203007093135/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.07.01 13:05:32 INFO  [info] welcome to sbt 1.5.4 (Ubuntu Java 11.0.11)[0m
[0m2021.07.01 13:05:34 INFO  [info] loading global plugins from /home/hwait/.sbt/1.0/plugins[0m
[0m2021.07.01 13:05:36 INFO  [info] loading settings for project ht3-build-build from metals.sbt ...[0m
[0m2021.07.01 13:05:36 INFO  [info] loading project definition from /home/hwait/dev/ht3/project/project[0m
[0m2021.07.01 13:05:36 INFO  [info] loading settings for project ht3-build from metals.sbt ...[0m
[0m2021.07.01 13:05:36 INFO  [info] loading project definition from /home/hwait/dev/ht3/project[0m
[0m2021.07.01 13:05:38 INFO  [success] Generated .bloop/ht3-build.json[0m
[0m2021.07.01 13:05:38 INFO  [success] Total time: 2 s, completed Jul 1, 2021, 1:05:39 PM[0m
[0m2021.07.01 13:05:41 INFO  [info] loading settings for project ht3 from build.sbt ...[0m
[0m2021.07.01 13:05:41 INFO  [info] set current project to ht3 (in build file:/home/hwait/dev/ht3/)[0m
[0m2021.07.01 13:05:50 INFO  [success] Generated .bloop/ht3-test.json[0m
[0m2021.07.01 13:05:50 INFO  [success] Generated .bloop/ht3.json[0m
[0m2021.07.01 13:05:50 INFO  [success] Total time: 9 s, completed Jul 1, 2021, 1:05:50 PM[0m
[0m2021.07.01 13:05:50 INFO  sbt bloopInstall exit: 0[0m
[0m2021.07.01 13:05:51 INFO  time: ran 'sbt bloopInstall' in 19s[0m
[0m2021.07.01 13:05:51 INFO  Disconnecting from Bloop session...[0m
[0m2021.07.01 13:05:51 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
[0m2021.07.01 13:05:51 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...[0m
2021.07.01 13:05:51 INFO  Attempting to connect to the build server...[0m
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
Starting the bsp launcher for bloop...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher9574697456135940308/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Configured SemanticDB in projects 'ht3', 'ht3-test'
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher9574697456135940308/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher9574697456135940308/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.01 13:05:51 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.01 13:05:51 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher10969996843434660888/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher10969996843434660888/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher10969996843434660888/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.01 13:05:51 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.01 13:05:51 INFO  time: Connected to build server in 0.21s[0m
[0m2021.07.01 13:05:51 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
[0m2021.07.01 13:05:54 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.07.01 13:05:54 INFO  time: indexed workspace in 2.75s[0m
[0m2021.07.01 13:05:54 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:54 WARN  unsupported Scala 2.12.14[0m
[0m2021.07.01 13:05:56 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
[0m2021.07.01 13:05:56 INFO  time: code lens generation in 1.74s[0m
[0m2021.07.01 13:06:09 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 13:06:13 INFO  time: compiled ht3 in 3.98s[0m
[0m2021.07.01 13:06:13 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 13:06:13 INFO  time: compiled ht3-test in 0.18s[0m
Jul 01, 2021 1:06:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 827
[0m2021.07.01 13:06:18 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 27.009s)[0m
[0m2021.07.01 13:06:18 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 13:06:18 INFO  time: compiled ht3-test in 0.25s[0m
[0m2021.07.01 13:06:18 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 13:06:18 INFO  Listening for transport dt_socket at address: 46231[0m
[0m2021.07.01 13:06:18 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 13:06:18 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 13:06:18 INFO  Trying to attach to remote debuggee VM localhost:46231 .[0m
[0m2021.07.01 13:06:18 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 13:06:18 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:06:19 ERROR log4j:WARN No appenders could be found for logger (org.apache.kafka.clients.producer.ProducerConfig).[0m
[0m2021.07.01 13:06:19 ERROR log4j:WARN Please initialize the log4j system properly.[0m
[0m2021.07.01 13:06:19 ERROR log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.[0m
[0m2021.07.01 13:06:20 INFO  Pushed Setosa Record: {"SepalLength":1}[0m
[0m2021.07.01 13:06:20 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
Jul 01, 2021 1:06:24 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 842
[0m2021.07.01 13:06:29 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 37.81s)[0m
[0m2021.07.01 13:06:29 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 13:06:29 INFO  time: compiled ht3-test in 0.25s[0m
[0m2021.07.01 13:06:29 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 13:06:29 INFO  Listening for transport dt_socket at address: 57779[0m
[0m2021.07.01 13:06:29 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 13:06:29 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.01 13:06:29 INFO  Trying to attach to remote debuggee VM localhost:57779 .[0m
[0m2021.07.01 13:06:29 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 13:06:30 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 13:06:30 ERROR 21/07/01 13:06:30 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 13:06:30 ERROR 21/07/01 13:06:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 13:06:31 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 13:06:31 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 13:06:31 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 13:06:31 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 13:06:31 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 13:06:31 ERROR 21/07/01 13:06:32 INFO SparkContext: Running Spark version 3.1.1[0m
[0m2021.07.01 13:06:31 ERROR 21/07/01 13:06:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 13:06:31 ERROR 21/07/01 13:06:32 INFO ResourceUtils: ==============================================================[0m
[0m2021.07.01 13:06:31 ERROR 21/07/01 13:06:32 INFO ResourceUtils: No custom resources configured for spark.driver.[0m
[0m2021.07.01 13:06:31 ERROR 21/07/01 13:06:32 INFO ResourceUtils: ==============================================================[0m
[0m2021.07.01 13:06:31 ERROR 21/07/01 13:06:32 INFO SparkContext: Submitted application: SPARK_STREAMING_TO_HDFS[0m
[0m2021.07.01 13:06:31 ERROR 21/07/01 13:06:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)[0m
[0m2021.07.01 13:06:32 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:06:31 ERROR 21/07/01 13:06:32 INFO ResourceProfile: Limiting resource is cpu[0m
[0m2021.07.01 13:06:31 ERROR 21/07/01 13:06:32 INFO ResourceProfileManager: Added ResourceProfile id: 0[0m
[0m2021.07.01 13:06:31 ERROR 21/07/01 13:06:32 INFO SecurityManager: Changing view acls to: hwait[0m
[0m2021.07.01 13:06:31 ERROR 21/07/01 13:06:32 INFO SecurityManager: Changing modify acls to: hwait[0m
[0m2021.07.01 13:06:31 ERROR 21/07/01 13:06:32 INFO SecurityManager: Changing view acls groups to: [0m
[0m2021.07.01 13:06:31 ERROR 21/07/01 13:06:32 INFO SecurityManager: Changing modify acls groups to: [0m
[0m2021.07.01 13:06:31 ERROR 21/07/01 13:06:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hwait); groups with view permissions: Set(); users  with modify permissions: Set(hwait); groups with modify permissions: Set()[0m
[0m2021.07.01 13:06:32 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:32 INFO Utils: Successfully started service 'sparkDriver' on port 37743.[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:32 INFO SparkEnv: Registering MapOutputTracker[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:32 INFO SparkEnv: Registering BlockManagerMaster[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a0b3e096-5a91-46e8-ba1b-1373e54bac79[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:32 INFO MemoryStore: MemoryStore started with capacity 2.2 GiB[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:32 INFO SparkEnv: Registering OutputCommitCoordinator[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:33 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.19.152.53:4040[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:33 INFO Executor: Starting executor ID driver on host 172.19.152.53[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35835.[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:33 INFO NettyBlockTransferService: Server created on 172.19.152.53:35835[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:33 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.19.152.53, 35835, None)[0m
[0m2021.07.01 13:06:33 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:33 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.152.53:35835 with 2.2 GiB RAM, BlockManagerId(driver, 172.19.152.53, 35835, None)[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.19.152.53, 35835, None)[0m
[0m2021.07.01 13:06:33 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:06:32 ERROR 21/07/01 13:06:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.19.152.53, 35835, None)[0m
[0m2021.07.01 13:06:33 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:33 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 WARN KafkaUtils: overriding enable.auto.commit to false for executor[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 WARN KafkaUtils: overriding auto.offset.reset to none for executor[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 WARN KafkaUtils: overriding executor group.id to spark-executor-spark_ml_testing[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO DirectKafkaInputDStream: Slide time = 30000 ms[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO DirectKafkaInputDStream: Checkpoint interval = null[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO DirectKafkaInputDStream: Remember interval = 30000 ms[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@1df4dac7[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO MappedDStream: Slide time = 30000 ms[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO MappedDStream: Storage level = Serialized 1x Replicated[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO MappedDStream: Checkpoint interval = null[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO MappedDStream: Remember interval = 30000 ms[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@36a5e0ee[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO ForEachDStream: Slide time = 30000 ms[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO ForEachDStream: Storage level = Serialized 1x Replicated[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO ForEachDStream: Checkpoint interval = null[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO ForEachDStream: Remember interval = 30000 ms[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@34b1198b[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO ConsumerConfig: ConsumerConfig values: [0m
[0m2021.07.01 13:06:33 ERROR 	allow.auto.create.topics = true[0m
[0m2021.07.01 13:06:33 ERROR 	auto.commit.interval.ms = 5000[0m
[0m2021.07.01 13:06:33 ERROR 	auto.offset.reset = earliest[0m
[0m2021.07.01 13:06:33 ERROR 	bootstrap.servers = [localhost:9092][0m
[0m2021.07.01 13:06:33 ERROR 	check.crcs = true[0m
[0m2021.07.01 13:06:33 ERROR 	client.dns.lookup = use_all_dns_ips[0m
[0m2021.07.01 13:06:33 ERROR 	client.id = consumer-spark_ml_testing-1[0m
[0m2021.07.01 13:06:33 ERROR 	client.rack = [0m
[0m2021.07.01 13:06:33 ERROR 	connections.max.idle.ms = 540000[0m
[0m2021.07.01 13:06:33 ERROR 	default.api.timeout.ms = 60000[0m
[0m2021.07.01 13:06:33 ERROR 	enable.auto.commit = false[0m
[0m2021.07.01 13:06:33 ERROR 	exclude.internal.topics = true[0m
[0m2021.07.01 13:06:33 ERROR 	fetch.max.bytes = 52428800[0m
[0m2021.07.01 13:06:33 ERROR 	fetch.max.wait.ms = 500[0m
[0m2021.07.01 13:06:33 ERROR 	fetch.min.bytes = 1[0m
[0m2021.07.01 13:06:33 ERROR 	group.id = spark_ml_testing[0m
[0m2021.07.01 13:06:33 ERROR 	group.instance.id = null[0m
[0m2021.07.01 13:06:33 ERROR 	heartbeat.interval.ms = 3000[0m
[0m2021.07.01 13:06:33 ERROR 	interceptor.classes = [][0m
[0m2021.07.01 13:06:33 ERROR 	internal.leave.group.on.close = true[0m
[0m2021.07.01 13:06:33 ERROR 	internal.throw.on.fetch.stable.offset.unsupported = false[0m
[0m2021.07.01 13:06:33 ERROR 	isolation.level = read_uncommitted[0m
[0m2021.07.01 13:06:33 ERROR 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer[0m
[0m2021.07.01 13:06:33 ERROR 	max.partition.fetch.bytes = 1048576[0m
[0m2021.07.01 13:06:33 ERROR 	max.poll.interval.ms = 300000[0m
[0m2021.07.01 13:06:33 ERROR 	max.poll.records = 500[0m
[0m2021.07.01 13:06:33 ERROR 	metadata.max.age.ms = 300000[0m
[0m2021.07.01 13:06:33 ERROR 	metric.reporters = [][0m
[0m2021.07.01 13:06:33 ERROR 	metrics.num.samples = 2[0m
[0m2021.07.01 13:06:33 ERROR 	metrics.recording.level = INFO[0m
[0m2021.07.01 13:06:33 ERROR 	metrics.sample.window.ms = 30000[0m
[0m2021.07.01 13:06:33 ERROR 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor][0m
[0m2021.07.01 13:06:33 ERROR 	receive.buffer.bytes = 65536[0m
[0m2021.07.01 13:06:33 ERROR 	reconnect.backoff.max.ms = 1000[0m
[0m2021.07.01 13:06:33 ERROR 	reconnect.backoff.ms = 50[0m
[0m2021.07.01 13:06:33 ERROR 	request.timeout.ms = 30000[0m
[0m2021.07.01 13:06:33 ERROR 	retry.backoff.ms = 100[0m
[0m2021.07.01 13:06:33 ERROR 	sasl.client.callback.handler.class = null[0m
[0m2021.07.01 13:06:33 ERROR 	sasl.jaas.config = null[0m
[0m2021.07.01 13:06:33 ERROR 	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m2021.07.01 13:06:33 ERROR 	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m2021.07.01 13:06:33 ERROR 	sasl.kerberos.service.name = null[0m
[0m2021.07.01 13:06:33 ERROR 	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m2021.07.01 13:06:33 ERROR 	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m2021.07.01 13:06:33 ERROR 	sasl.login.callback.handler.class = null[0m
[0m2021.07.01 13:06:33 ERROR 	sasl.login.class = null[0m
[0m2021.07.01 13:06:33 ERROR 	sasl.login.refresh.buffer.seconds = 300[0m
[0m2021.07.01 13:06:33 ERROR 	sasl.login.refresh.min.period.seconds = 60[0m
[0m2021.07.01 13:06:33 ERROR 	sasl.login.refresh.window.factor = 0.8[0m
[0m2021.07.01 13:06:33 ERROR 	sasl.login.refresh.window.jitter = 0.05[0m
[0m2021.07.01 13:06:33 ERROR 	sasl.mechanism = GSSAPI[0m
[0m2021.07.01 13:06:33 ERROR 	security.protocol = PLAINTEXT[0m
[0m2021.07.01 13:06:33 ERROR 	security.providers = null[0m
[0m2021.07.01 13:06:33 ERROR 	send.buffer.bytes = 131072[0m
[0m2021.07.01 13:06:33 ERROR 	session.timeout.ms = 10000[0m
[0m2021.07.01 13:06:33 ERROR 	ssl.cipher.suites = null[0m
[0m2021.07.01 13:06:33 ERROR 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3][0m
[0m2021.07.01 13:06:33 ERROR 	ssl.endpoint.identification.algorithm = https[0m
[0m2021.07.01 13:06:33 ERROR 	ssl.engine.factory.class = null[0m
[0m2021.07.01 13:06:33 ERROR 	ssl.key.password = null[0m
[0m2021.07.01 13:06:33 ERROR 	ssl.keymanager.algorithm = SunX509[0m
[0m2021.07.01 13:06:33 ERROR 	ssl.keystore.location = null[0m
[0m2021.07.01 13:06:33 ERROR 	ssl.keystore.password = null[0m
[0m2021.07.01 13:06:33 ERROR 	ssl.keystore.type = JKS[0m
[0m2021.07.01 13:06:33 ERROR 	ssl.protocol = TLSv1.3[0m
[0m2021.07.01 13:06:33 ERROR 	ssl.provider = null[0m
[0m2021.07.01 13:06:33 ERROR 	ssl.secure.random.implementation = null[0m
[0m2021.07.01 13:06:33 ERROR 	ssl.trustmanager.algorithm = PKIX[0m
[0m2021.07.01 13:06:33 ERROR 	ssl.truststore.location = null[0m
[0m2021.07.01 13:06:33 ERROR 	ssl.truststore.password = null[0m
[0m2021.07.01 13:06:33 ERROR 	ssl.truststore.type = JKS[0m
[0m2021.07.01 13:06:33 ERROR 	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer[0m
[0m2021.07.01 13:06:33 ERROR [0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO AppInfoParser: Kafka version: 2.6.0[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO AppInfoParser: Kafka commitId: 62abe01bee039651[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO AppInfoParser: Kafka startTimeMs: 1625123194420[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO KafkaConsumer: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Subscribed to topic(s): spark_ml_poc[0m
[0m2021.07.01 13:06:33 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO Metadata: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Cluster ID: -TctB5CtQT2vWcpApmpk2w[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Discovered group coordinator DESKTOP-G76NQH1.localdomain:9092 (id: 2147483647 rack: null)[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] (Re-)joining group[0m
[0m2021.07.01 13:06:34 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] (Re-)joining group[0m
[0m2021.07.01 13:06:34 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Finished assignment for group at generation 1: {consumer-spark_ml_testing-1-f7e4b33d-e2a7-49d8-9913-11db81f1d68a=Assignment(partitions=[spark_ml_poc-0])}[0m
[0m2021.07.01 13:06:34 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Successfully joined group with generation 1[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Notifying assignor about the new Assignment(partitions=[spark_ml_poc-0])[0m
[0m2021.07.01 13:06:34 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Adding newly assigned partitions: spark_ml_poc-0[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Found no committed offset for partition spark_ml_poc-0[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Resetting offset for partition spark_ml_poc-0 to offset 0.[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO RecurringTimer: Started timer for JobGenerator at time 1625123220000[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO JobGenerator: Started JobGenerator at 1625123220000 ms[0m
[0m2021.07.01 13:06:33 ERROR 21/07/01 13:06:34 INFO JobScheduler: Started JobScheduler[0m
[0m2021.07.01 13:06:34 ERROR 21/07/01 13:06:34 INFO StreamingContext: StreamingContext started[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Seeking to LATEST offset of partition spark_ml_poc-0[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Resetting offset for partition spark_ml_poc-0 to offset 5.[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO JobScheduler: Added jobs for time 1625123220000 ms[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO JobScheduler: Starting job streaming job 1625123220000 ms.0 from job set of time 1625123220000 ms[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO SparkContext: Starting job: isEmpty at SparkMLRegressionFromKafka.scala:60[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO DAGScheduler: Got job 0 (isEmpty at SparkMLRegressionFromKafka.scala:60) with 1 output partitions[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO DAGScheduler: Final stage: ResultStage 0 (isEmpty at SparkMLRegressionFromKafka.scala:60)[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 13:07:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 13:07:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkMLRegressionFromKafka.scala:57), which has no missing parents[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.0 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:07:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:07:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.152.53:35835 (size: 2.4 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkMLRegressionFromKafka.scala:57) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4360 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO KafkaRDD: Computing topic spark_ml_poc, partition 0 offsets 0 -> 5[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO KafkaDataConsumer: Initializing cache 16 64 0.75[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO ConsumerConfig: ConsumerConfig values: [0m
[0m2021.07.01 13:07:00 ERROR 	allow.auto.create.topics = true[0m
[0m2021.07.01 13:07:00 ERROR 	auto.commit.interval.ms = 5000[0m
[0m2021.07.01 13:07:00 ERROR 	auto.offset.reset = none[0m
[0m2021.07.01 13:07:00 ERROR 	bootstrap.servers = [localhost:9092][0m
[0m2021.07.01 13:07:00 ERROR 	check.crcs = true[0m
[0m2021.07.01 13:07:00 ERROR 	client.dns.lookup = use_all_dns_ips[0m
[0m2021.07.01 13:07:00 ERROR 	client.id = consumer-spark-executor-spark_ml_testing-2[0m
[0m2021.07.01 13:07:00 ERROR 	client.rack = [0m
[0m2021.07.01 13:07:00 ERROR 	connections.max.idle.ms = 540000[0m
[0m2021.07.01 13:07:00 ERROR 	default.api.timeout.ms = 60000[0m
[0m2021.07.01 13:07:00 ERROR 	enable.auto.commit = false[0m
[0m2021.07.01 13:07:00 ERROR 	exclude.internal.topics = true[0m
[0m2021.07.01 13:07:00 ERROR 	fetch.max.bytes = 52428800[0m
[0m2021.07.01 13:07:00 ERROR 	fetch.max.wait.ms = 500[0m
[0m2021.07.01 13:07:00 ERROR 	fetch.min.bytes = 1[0m
[0m2021.07.01 13:07:00 ERROR 	group.id = spark-executor-spark_ml_testing[0m
[0m2021.07.01 13:07:00 ERROR 	group.instance.id = null[0m
[0m2021.07.01 13:07:00 ERROR 	heartbeat.interval.ms = 3000[0m
[0m2021.07.01 13:07:00 ERROR 	interceptor.classes = [][0m
[0m2021.07.01 13:07:00 ERROR 	internal.leave.group.on.close = true[0m
[0m2021.07.01 13:07:00 ERROR 	internal.throw.on.fetch.stable.offset.unsupported = false[0m
[0m2021.07.01 13:07:00 ERROR 	isolation.level = read_uncommitted[0m
[0m2021.07.01 13:07:00 ERROR 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer[0m
[0m2021.07.01 13:07:00 ERROR 	max.partition.fetch.bytes = 1048576[0m
[0m2021.07.01 13:07:00 ERROR 	max.poll.interval.ms = 300000[0m
[0m2021.07.01 13:07:00 ERROR 	max.poll.records = 500[0m
[0m2021.07.01 13:07:00 ERROR 	metadata.max.age.ms = 300000[0m
[0m2021.07.01 13:07:00 ERROR 	metric.reporters = [][0m
[0m2021.07.01 13:07:00 ERROR 	metrics.num.samples = 2[0m
[0m2021.07.01 13:07:00 ERROR 	metrics.recording.level = INFO[0m
[0m2021.07.01 13:07:00 ERROR 	metrics.sample.window.ms = 30000[0m
[0m2021.07.01 13:07:00 ERROR 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor][0m
[0m2021.07.01 13:07:00 ERROR 	receive.buffer.bytes = 65536[0m
[0m2021.07.01 13:07:00 ERROR 	reconnect.backoff.max.ms = 1000[0m
[0m2021.07.01 13:07:00 ERROR 	reconnect.backoff.ms = 50[0m
[0m2021.07.01 13:07:00 ERROR 	request.timeout.ms = 30000[0m
[0m2021.07.01 13:07:00 ERROR 	retry.backoff.ms = 100[0m
[0m2021.07.01 13:07:00 ERROR 	sasl.client.callback.handler.class = null[0m
[0m2021.07.01 13:07:00 ERROR 	sasl.jaas.config = null[0m
[0m2021.07.01 13:07:00 ERROR 	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m2021.07.01 13:07:00 ERROR 	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m2021.07.01 13:07:00 ERROR 	sasl.kerberos.service.name = null[0m
[0m2021.07.01 13:07:00 ERROR 	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m2021.07.01 13:07:00 ERROR 	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m2021.07.01 13:07:00 ERROR 	sasl.login.callback.handler.class = null[0m
[0m2021.07.01 13:07:00 ERROR 	sasl.login.class = null[0m
[0m2021.07.01 13:07:00 ERROR 	sasl.login.refresh.buffer.seconds = 300[0m
[0m2021.07.01 13:07:00 ERROR 	sasl.login.refresh.min.period.seconds = 60[0m
[0m2021.07.01 13:07:00 ERROR 	sasl.login.refresh.window.factor = 0.8[0m
[0m2021.07.01 13:07:00 ERROR 	sasl.login.refresh.window.jitter = 0.05[0m
[0m2021.07.01 13:07:00 ERROR 	sasl.mechanism = GSSAPI[0m
[0m2021.07.01 13:07:00 ERROR 	security.protocol = PLAINTEXT[0m
[0m2021.07.01 13:07:00 ERROR 	security.providers = null[0m
[0m2021.07.01 13:07:00 ERROR 	send.buffer.bytes = 131072[0m
[0m2021.07.01 13:07:00 ERROR 	session.timeout.ms = 10000[0m
[0m2021.07.01 13:07:00 ERROR 	ssl.cipher.suites = null[0m
[0m2021.07.01 13:07:00 ERROR 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3][0m
[0m2021.07.01 13:07:00 ERROR 	ssl.endpoint.identification.algorithm = https[0m
[0m2021.07.01 13:07:00 ERROR 	ssl.engine.factory.class = null[0m
[0m2021.07.01 13:07:00 ERROR 	ssl.key.password = null[0m
[0m2021.07.01 13:07:00 ERROR 	ssl.keymanager.algorithm = SunX509[0m
[0m2021.07.01 13:07:00 ERROR 	ssl.keystore.location = null[0m
[0m2021.07.01 13:07:00 ERROR 	ssl.keystore.password = null[0m
[0m2021.07.01 13:07:00 ERROR 	ssl.keystore.type = JKS[0m
[0m2021.07.01 13:07:00 ERROR 	ssl.protocol = TLSv1.3[0m
[0m2021.07.01 13:07:00 ERROR 	ssl.provider = null[0m
[0m2021.07.01 13:07:00 ERROR 	ssl.secure.random.implementation = null[0m
[0m2021.07.01 13:07:00 ERROR 	ssl.trustmanager.algorithm = PKIX[0m
[0m2021.07.01 13:07:00 ERROR 	ssl.truststore.location = null[0m
[0m2021.07.01 13:07:00 ERROR 	ssl.truststore.password = null[0m
[0m2021.07.01 13:07:00 ERROR 	ssl.truststore.type = JKS[0m
[0m2021.07.01 13:07:00 ERROR 	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer[0m
[0m2021.07.01 13:07:00 ERROR [0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO AppInfoParser: Kafka version: 2.6.0[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO AppInfoParser: Kafka commitId: 62abe01bee039651[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO AppInfoParser: Kafka startTimeMs: 1625123220617[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-spark_ml_testing-2, groupId=spark-executor-spark_ml_testing] Subscribed to partition(s): spark_ml_poc-0[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO InternalKafkaConsumer: Initial fetch for spark-executor-spark_ml_testing spark_ml_poc-0 0[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-spark_ml_testing-2, groupId=spark-executor-spark_ml_testing] Seeking to offset 0 for partition spark_ml_poc-0[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO Metadata: [Consumer clientId=consumer-spark-executor-spark_ml_testing-2, groupId=spark-executor-spark_ml_testing] Cluster ID: -TctB5CtQT2vWcpApmpk2w[0m
[0m2021.07.01 13:07:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 900 bytes result sent to driver[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 242 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO DAGScheduler: ResultStage 0 (isEmpty at SparkMLRegressionFromKafka.scala:60) finished in 0.462 s[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO DAGScheduler: Job 0 finished: isEmpty at SparkMLRegressionFromKafka.scala:60, took 0.533092 s[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hwait/dev/ht3/spark-warehouse').[0m
[0m2021.07.01 13:07:00 ERROR 21/07/01 13:07:00 INFO SharedState: Warehouse path is 'file:/home/hwait/dev/ht3/spark-warehouse'.[0m
[0m2021.07.01 13:07:02 ERROR 21/07/01 13:07:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.19.152.53:35835 in memory (size: 2.4 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO CodeGenerator: Code generated in 238.5991 ms[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO SparkContext: Starting job: json at SparkMLRegressionFromKafka.scala:62[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO DAGScheduler: Got job 1 (json at SparkMLRegressionFromKafka.scala:62) with 1 output partitions[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO DAGScheduler: Final stage: ResultStage 1 (json at SparkMLRegressionFromKafka.scala:62)[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 13:07:03 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:03 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at json at SparkMLRegressionFromKafka.scala:62), which has no missing parents[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.6 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:07:03 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:07:03 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.152.53:35835 (size: 7.3 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at json at SparkMLRegressionFromKafka.scala:62) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4360 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO KafkaRDD: Computing topic spark_ml_poc, partition 0 offsets 0 -> 5[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO InternalKafkaConsumer: Initial fetch for spark-executor-spark_ml_testing spark_ml_poc-0 0[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-spark_ml_testing-2, groupId=spark-executor-spark_ml_testing] Seeking to offset 0 for partition spark_ml_poc-0[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2017 bytes result sent to driver[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 91 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO DAGScheduler: ResultStage 1 (json at SparkMLRegressionFromKafka.scala:62) finished in 0.128 s[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:03 INFO DAGScheduler: Job 1 finished: json at SparkMLRegressionFromKafka.scala:62, took 0.135571 s[0m
[0m2021.07.01 13:07:03 INFO  root[0m
[0m2021.07.01 13:07:03 INFO   |-- SepalLength: long (nullable = true)[0m
[0m2021.07.01 13:07:03 INFO   |-- _corrupt_record: string (nullable = true)[0m
[0m2021.07.01 13:07:03 INFO  [0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO CodeGenerator: Code generated in 18.357 ms[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO SparkContext: Starting job: show at SparkMLRegressionFromKafka.scala:65[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO DAGScheduler: Got job 2 (show at SparkMLRegressionFromKafka.scala:65) with 1 output partitions[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO DAGScheduler: Final stage: ResultStage 2 (show at SparkMLRegressionFromKafka.scala:65)[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 13:07:03 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:03 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at show at SparkMLRegressionFromKafka.scala:65), which has no missing parents[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.2 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:07:03 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:07:03 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.152.53:35835 (size: 6.8 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at show at SparkMLRegressionFromKafka.scala:65) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4360 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO KafkaRDD: Computing topic spark_ml_poc, partition 0 offsets 0 -> 5[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO InternalKafkaConsumer: Initial fetch for spark-executor-spark_ml_testing spark_ml_poc-0 0[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-spark_ml_testing-2, groupId=spark-executor-spark_ml_testing] Seeking to offset 0 for partition spark_ml_poc-0[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1485 bytes result sent to driver[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 217 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO DAGScheduler: ResultStage 2 (show at SparkMLRegressionFromKafka.scala:65) finished in 0.233 s[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO DAGScheduler: Job 2 finished: show at SparkMLRegressionFromKafka.scala:65, took 0.242067 s[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO CodeGenerator: Code generated in 19.7331 ms[0m
[0m2021.07.01 13:07:03 INFO  +-----------+---------------+[0m
[0m2021.07.01 13:07:03 INFO  |SepalLength|_corrupt_record|[0m
[0m2021.07.01 13:07:03 INFO  +-----------+---------------+[0m
[0m2021.07.01 13:07:03 INFO  |          1|           null|[0m
[0m2021.07.01 13:07:03 INFO  |       null|             cc|[0m
[0m2021.07.01 13:07:03 INFO  |          1|           null|[0m
[0m2021.07.01 13:07:03 INFO  |          1|           null|[0m
[0m2021.07.01 13:07:03 INFO  |          1|           null|[0m
[0m2021.07.01 13:07:03 INFO  +-----------+---------------+[0m
[0m2021.07.01 13:07:03 INFO  [0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO JobScheduler: Finished job streaming job 1625123220000 ms.0 from job set of time 1625123220000 ms[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO JobScheduler: Total delay: 4.475 s for time 1625123220000 ms (execution: 4.361 s)[0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO ReceivedBlockTracker: Deleting batches: [0m
[0m2021.07.01 13:07:03 ERROR 21/07/01 13:07:04 INFO InputInfoTracker: remove old batch metadata: [0m
Jul 01, 2021 1:07:27 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 854
[0m2021.07.01 13:07:29 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 1m 38.412s)[0m
[0m2021.07.01 13:07:29 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 13:07:29 INFO  time: compiled ht3-test in 0.26s[0m
[0m2021.07.01 13:07:29 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Seeking to LATEST offset of partition spark_ml_poc-0[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Resetting offset for partition spark_ml_poc-0 to offset 5.[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO JobScheduler: Added jobs for time 1625123250000 ms[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO JobScheduler: Starting job streaming job 1625123250000 ms.0 from job set of time 1625123250000 ms[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO SparkContext: Starting job: isEmpty at SparkMLRegressionFromKafka.scala:60[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO DAGScheduler: Got job 3 (isEmpty at SparkMLRegressionFromKafka.scala:60) with 1 output partitions[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO DAGScheduler: Final stage: ResultStage 3 (isEmpty at SparkMLRegressionFromKafka.scala:60)[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 13:07:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at map at SparkMLRegressionFromKafka.scala:57), which has no missing parents[0m
[0m2021.07.01 13:07:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 4.0 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:07:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.19.152.53:35835 (size: 2.4 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:07:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at map at SparkMLRegressionFromKafka.scala:57) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4360 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO KafkaRDD: Beginning offset 5 is the same as ending offset skipping spark_ml_poc 0[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 837 bytes result sent to driver[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 12 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO DAGScheduler: ResultStage 3 (isEmpty at SparkMLRegressionFromKafka.scala:60) finished in 0.025 s[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO DAGScheduler: Job 3 finished: isEmpty at SparkMLRegressionFromKafka.scala:60, took 0.032248 s[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO JobScheduler: Finished job streaming job 1625123250000 ms.0 from job set of time 1625123250000 ms[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO JobScheduler: Total delay: 0.056 s for time 1625123250000 ms (execution: 0.047 s)[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO MapPartitionsRDD: Removing RDD 1 from persistence list[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO BlockManager: Removing RDD 1[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO KafkaRDD: Removing RDD 0 from persistence list[0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO ReceivedBlockTracker: Deleting batches: [0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO InputInfoTracker: remove old batch metadata: [0m
[0m2021.07.01 13:07:29 ERROR 21/07/01 13:07:30 INFO BlockManager: Removing RDD 0[0m
[0m2021.07.01 13:07:29 INFO  Listening for transport dt_socket at address: 36207[0m
[0m2021.07.01 13:07:29 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 13:07:30 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 13:07:29 INFO  Trying to attach to remote debuggee VM localhost:36207 .[0m
[0m2021.07.01 13:07:29 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 13:07:30 ERROR log4j:WARN No appenders could be found for logger (org.apache.kafka.clients.producer.ProducerConfig).[0m
[0m2021.07.01 13:07:30 ERROR log4j:WARN Please initialize the log4j system properly.[0m
[0m2021.07.01 13:07:30 ERROR log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.[0m
[0m2021.07.01 13:07:29 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:30 INFO  Pushed Setosa Record: {"SepalLength":1}[0m
[0m2021.07.01 13:07:31 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 13:07:56 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 2m 4.96s)[0m
[0m2021.07.01 13:07:56 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 13:07:56 INFO  time: compiled ht3-test in 0.21s[0m
[0m2021.07.01 13:07:56 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 13:07:56 INFO  Listening for transport dt_socket at address: 53475[0m
[0m2021.07.01 13:07:56 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 13:07:56 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 13:07:56 INFO  Trying to attach to remote debuggee VM localhost:53475 .[0m
[0m2021.07.01 13:07:56 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 13:07:56 ERROR log4j:WARN No appenders could be found for logger (org.apache.kafka.clients.producer.ProducerConfig).[0m
[0m2021.07.01 13:07:56 ERROR log4j:WARN Please initialize the log4j system properly.[0m
[0m2021.07.01 13:07:56 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:07:56 ERROR log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.[0m
[0m2021.07.01 13:07:58 INFO  Pushed Setosa Record: {"SepalLength":1}[0m
[0m2021.07.01 13:07:58 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Seeking to LATEST offset of partition spark_ml_poc-0[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Resetting offset for partition spark_ml_poc-0 to offset 7.[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO JobScheduler: Added jobs for time 1625123280000 ms[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO JobScheduler: Starting job streaming job 1625123280000 ms.0 from job set of time 1625123280000 ms[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO SparkContext: Starting job: isEmpty at SparkMLRegressionFromKafka.scala:60[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Got job 4 (isEmpty at SparkMLRegressionFromKafka.scala:60) with 1 output partitions[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Final stage: ResultStage 4 (isEmpty at SparkMLRegressionFromKafka.scala:60)[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at map at SparkMLRegressionFromKafka.scala:57), which has no missing parents[0m
[0m2021.07.01 13:08:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 4.0 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:08:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:08:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.19.152.53:35835 (size: 2.4 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at map at SparkMLRegressionFromKafka.scala:57) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4360 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO KafkaRDD: Computing topic spark_ml_poc, partition 0 offsets 5 -> 7[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 857 bytes result sent to driver[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 12 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: ResultStage 4 (isEmpty at SparkMLRegressionFromKafka.scala:60) finished in 0.022 s[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Job 4 finished: isEmpty at SparkMLRegressionFromKafka.scala:60, took 0.027109 s[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO SparkContext: Starting job: json at SparkMLRegressionFromKafka.scala:62[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Got job 5 (json at SparkMLRegressionFromKafka.scala:62) with 1 output partitions[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Final stage: ResultStage 5 (json at SparkMLRegressionFromKafka.scala:62)[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 13:08:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[19] at json at SparkMLRegressionFromKafka.scala:62), which has no missing parents[0m
[0m2021.07.01 13:08:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 14.6 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:08:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:08:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.19.152.53:35835 (size: 7.3 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[19] at json at SparkMLRegressionFromKafka.scala:62) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4360 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO KafkaRDD: Computing topic spark_ml_poc, partition 0 offsets 5 -> 7[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO InternalKafkaConsumer: Initial fetch for spark-executor-spark_ml_testing spark_ml_poc-0 5[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-spark_ml_testing-2, groupId=spark-executor-spark_ml_testing] Seeking to offset 5 for partition spark_ml_poc-0[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1889 bytes result sent to driver[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 435 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: ResultStage 5 (json at SparkMLRegressionFromKafka.scala:62) finished in 0.453 s[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Job 5 finished: json at SparkMLRegressionFromKafka.scala:62, took 0.458438 s[0m
[0m2021.07.01 13:08:00 INFO  root[0m
[0m2021.07.01 13:08:00 INFO   |-- SepalLength: long (nullable = true)[0m
[0m2021.07.01 13:08:00 INFO  [0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO CodeGenerator: Code generated in 15.0578 ms[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO SparkContext: Starting job: show at SparkMLRegressionFromKafka.scala:65[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Got job 6 (show at SparkMLRegressionFromKafka.scala:65) with 1 output partitions[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Final stage: ResultStage 6 (show at SparkMLRegressionFromKafka.scala:65)[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 13:08:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[25] at show at SparkMLRegressionFromKafka.scala:65), which has no missing parents[0m
[0m2021.07.01 13:08:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 12.8 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:08:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:08:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.19.152.53:35835 (size: 6.7 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[25] at show at SparkMLRegressionFromKafka.scala:65) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4360 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO KafkaRDD: Computing topic spark_ml_poc, partition 0 offsets 5 -> 7[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO InternalKafkaConsumer: Initial fetch for spark-executor-spark_ml_testing spark_ml_poc-0 5[0m
[0m2021.07.01 13:08:00 ERROR 21/07/01 13:08:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-spark_ml_testing-2, groupId=spark-executor-spark_ml_testing] Seeking to offset 5 for partition spark_ml_poc-0[0m
[0m2021.07.01 13:08:01 ERROR 21/07/01 13:08:01 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1435 bytes result sent to driver[0m
[0m2021.07.01 13:08:01 ERROR 21/07/01 13:08:01 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 395 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 13:08:01 ERROR 21/07/01 13:08:01 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 13:08:01 ERROR 21/07/01 13:08:01 INFO DAGScheduler: ResultStage 6 (show at SparkMLRegressionFromKafka.scala:65) finished in 0.411 s[0m
[0m2021.07.01 13:08:01 ERROR 21/07/01 13:08:01 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 13:08:01 ERROR 21/07/01 13:08:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished[0m
[0m2021.07.01 13:08:01 ERROR 21/07/01 13:08:01 INFO DAGScheduler: Job 6 finished: show at SparkMLRegressionFromKafka.scala:65, took 0.417697 s[0m
[0m2021.07.01 13:08:01 ERROR 21/07/01 13:08:01 INFO CodeGenerator: Code generated in 17.854 ms[0m
[0m2021.07.01 13:08:01 INFO  +-----------+[0m
[0m2021.07.01 13:08:01 INFO  |SepalLength|[0m
[0m2021.07.01 13:08:01 INFO  +-----------+[0m
[0m2021.07.01 13:08:01 INFO  |          1|[0m
[0m2021.07.01 13:08:01 INFO  |          1|[0m
[0m2021.07.01 13:08:01 INFO  +-----------+[0m
[0m2021.07.01 13:08:01 INFO  [0m
[0m2021.07.01 13:08:01 ERROR 21/07/01 13:08:01 INFO JobScheduler: Finished job streaming job 1625123280000 ms.0 from job set of time 1625123280000 ms[0m
[0m2021.07.01 13:08:01 ERROR 21/07/01 13:08:01 INFO JobScheduler: Total delay: 1.080 s for time 1625123280000 ms (execution: 1.072 s)[0m
[0m2021.07.01 13:08:01 ERROR 21/07/01 13:08:01 INFO MapPartitionsRDD: Removing RDD 13 from persistence list[0m
[0m2021.07.01 13:08:01 ERROR 21/07/01 13:08:01 INFO KafkaRDD: Removing RDD 12 from persistence list[0m
[0m2021.07.01 13:08:01 ERROR 21/07/01 13:08:01 INFO BlockManager: Removing RDD 13[0m
[0m2021.07.01 13:08:01 ERROR 21/07/01 13:08:01 INFO ReceivedBlockTracker: Deleting batches: [0m
[0m2021.07.01 13:08:01 ERROR 21/07/01 13:08:01 INFO InputInfoTracker: remove old batch metadata: 1625123220000 ms[0m
[0m2021.07.01 13:08:01 ERROR 21/07/01 13:08:01 INFO BlockManager: Removing RDD 12[0m
[0m2021.07.01 13:08:18 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 2m 27.599s)[0m
[0m2021.07.01 13:08:18 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 13:08:18 INFO  time: compiled ht3-test in 0.21s[0m
[0m2021.07.01 13:08:19 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 13:08:18 INFO  Listening for transport dt_socket at address: 39145[0m
[0m2021.07.01 13:08:19 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 13:08:19 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 13:08:18 INFO  Trying to attach to remote debuggee VM localhost:39145 .[0m
[0m2021.07.01 13:08:18 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 13:08:18 ERROR log4j:WARN No appenders could be found for logger (org.apache.kafka.clients.producer.ProducerConfig).[0m
[0m2021.07.01 13:08:18 ERROR log4j:WARN Please initialize the log4j system properly.[0m
[0m2021.07.01 13:08:19 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:18 ERROR log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.[0m
[0m2021.07.01 13:08:20 INFO  Pushed Setosa Record: {"SepalLength":1}[0m
[0m2021.07.01 13:08:20 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 13:08:26 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 2m 35.607s)[0m
[0m2021.07.01 13:08:26 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 13:08:26 INFO  time: compiled ht3-test in 0.22s[0m
[0m2021.07.01 13:08:27 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 13:08:26 INFO  Listening for transport dt_socket at address: 48101[0m
[0m2021.07.01 13:08:27 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 13:08:27 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 13:08:26 INFO  Trying to attach to remote debuggee VM localhost:48101 .[0m
[0m2021.07.01 13:08:26 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 13:08:27 ERROR log4j:WARN No appenders could be found for logger (org.apache.kafka.clients.producer.ProducerConfig).[0m
[0m2021.07.01 13:08:27 ERROR log4j:WARN Please initialize the log4j system properly.[0m
[0m2021.07.01 13:08:27 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:27 ERROR log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.[0m
[0m2021.07.01 13:08:27 INFO  Pushed Setosa Record: {"SepalLength":1}[0m
[0m2021.07.01 13:08:28 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Seeking to LATEST offset of partition spark_ml_poc-0[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Resetting offset for partition spark_ml_poc-0 to offset 9.[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO JobScheduler: Added jobs for time 1625123310000 ms[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO JobScheduler: Starting job streaming job 1625123310000 ms.0 from job set of time 1625123310000 ms[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO SparkContext: Starting job: isEmpty at SparkMLRegressionFromKafka.scala:60[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Got job 7 (isEmpty at SparkMLRegressionFromKafka.scala:60) with 1 output partitions[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Final stage: ResultStage 7 (isEmpty at SparkMLRegressionFromKafka.scala:60)[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 13:08:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[27] at map at SparkMLRegressionFromKafka.scala:57), which has no missing parents[0m
[0m2021.07.01 13:08:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 4.0 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:08:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:08:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.19.152.53:35835 (size: 2.4 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[27] at map at SparkMLRegressionFromKafka.scala:57) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4360 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO KafkaRDD: Computing topic spark_ml_poc, partition 0 offsets 7 -> 9[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 857 bytes result sent to driver[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 9 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: ResultStage 7 (isEmpty at SparkMLRegressionFromKafka.scala:60) finished in 0.032 s[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Job 7 finished: isEmpty at SparkMLRegressionFromKafka.scala:60, took 0.035884 s[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO SparkContext: Starting job: json at SparkMLRegressionFromKafka.scala:62[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Got job 8 (json at SparkMLRegressionFromKafka.scala:62) with 1 output partitions[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Final stage: ResultStage 8 (json at SparkMLRegressionFromKafka.scala:62)[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 13:08:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[31] at json at SparkMLRegressionFromKafka.scala:62), which has no missing parents[0m
[0m2021.07.01 13:08:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 14.6 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:08:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:08:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.19.152.53:35835 (size: 7.3 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[31] at json at SparkMLRegressionFromKafka.scala:62) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4360 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO KafkaRDD: Computing topic spark_ml_poc, partition 0 offsets 7 -> 9[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO InternalKafkaConsumer: Initial fetch for spark-executor-spark_ml_testing spark_ml_poc-0 7[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-spark_ml_testing-2, groupId=spark-executor-spark_ml_testing] Seeking to offset 7 for partition spark_ml_poc-0[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 1889 bytes result sent to driver[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 450 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: ResultStage 8 (json at SparkMLRegressionFromKafka.scala:62) finished in 0.464 s[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Job 8 finished: json at SparkMLRegressionFromKafka.scala:62, took 0.468923 s[0m
[0m2021.07.01 13:08:30 INFO  root[0m
[0m2021.07.01 13:08:30 INFO   |-- SepalLength: long (nullable = true)[0m
[0m2021.07.01 13:08:30 INFO  [0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO SparkContext: Starting job: show at SparkMLRegressionFromKafka.scala:65[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Got job 9 (show at SparkMLRegressionFromKafka.scala:65) with 1 output partitions[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Final stage: ResultStage 9 (show at SparkMLRegressionFromKafka.scala:65)[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 13:08:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[37] at show at SparkMLRegressionFromKafka.scala:65), which has no missing parents[0m
[0m2021.07.01 13:08:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 12.8 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:08:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:08:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.19.152.53:35835 (size: 6.7 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[37] at show at SparkMLRegressionFromKafka.scala:65) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4360 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO KafkaRDD: Computing topic spark_ml_poc, partition 0 offsets 7 -> 9[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO InternalKafkaConsumer: Initial fetch for spark-executor-spark_ml_testing spark_ml_poc-0 7[0m
[0m2021.07.01 13:08:30 ERROR 21/07/01 13:08:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-spark_ml_testing-2, groupId=spark-executor-spark_ml_testing] Seeking to offset 7 for partition spark_ml_poc-0[0m
[0m2021.07.01 13:08:31 ERROR 21/07/01 13:08:31 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 1435 bytes result sent to driver[0m
[0m2021.07.01 13:08:31 ERROR 21/07/01 13:08:31 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 424 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 13:08:31 ERROR 21/07/01 13:08:31 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 13:08:31 ERROR 21/07/01 13:08:31 INFO DAGScheduler: ResultStage 9 (show at SparkMLRegressionFromKafka.scala:65) finished in 0.438 s[0m
[0m2021.07.01 13:08:31 ERROR 21/07/01 13:08:31 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 13:08:31 ERROR 21/07/01 13:08:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished[0m
[0m2021.07.01 13:08:31 ERROR 21/07/01 13:08:31 INFO DAGScheduler: Job 9 finished: show at SparkMLRegressionFromKafka.scala:65, took 0.444354 s[0m
[0m2021.07.01 13:08:31 INFO  +-----------+[0m
[0m2021.07.01 13:08:31 INFO  |SepalLength|[0m
[0m2021.07.01 13:08:31 INFO  +-----------+[0m
[0m2021.07.01 13:08:31 INFO  |          1|[0m
[0m2021.07.01 13:08:31 INFO  |          1|[0m
[0m2021.07.01 13:08:31 INFO  +-----------+[0m
[0m2021.07.01 13:08:31 INFO  [0m
[0m2021.07.01 13:08:31 ERROR 21/07/01 13:08:31 INFO JobScheduler: Finished job streaming job 1625123310000 ms.0 from job set of time 1625123310000 ms[0m
[0m2021.07.01 13:08:31 ERROR 21/07/01 13:08:31 INFO JobScheduler: Total delay: 1.072 s for time 1625123310000 ms (execution: 1.065 s)[0m
[0m2021.07.01 13:08:31 ERROR 21/07/01 13:08:31 INFO MapPartitionsRDD: Removing RDD 15 from persistence list[0m
[0m2021.07.01 13:08:31 ERROR 21/07/01 13:08:31 INFO KafkaRDD: Removing RDD 14 from persistence list[0m
[0m2021.07.01 13:08:31 ERROR 21/07/01 13:08:31 INFO BlockManager: Removing RDD 15[0m
[0m2021.07.01 13:08:31 ERROR 21/07/01 13:08:31 INFO ReceivedBlockTracker: Deleting batches: [0m
[0m2021.07.01 13:08:31 ERROR 21/07/01 13:08:31 INFO InputInfoTracker: remove old batch metadata: 1625123250000 ms[0m
[0m2021.07.01 13:08:31 ERROR 21/07/01 13:08:31 INFO BlockManager: Removing RDD 14[0m
[0m2021.07.01 13:08:51 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.01 13:08:51 INFO  Listening for transport dt_socket at address: 42449[0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook[0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO ReceiverTracker: ReceiverTracker stopped[0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO JobGenerator: Stopping JobGenerator immediately[0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO RecurringTimer: Stopped timer for JobGenerator after time 1625123310000[0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Revoke previously assigned partitions spark_ml_poc-0[0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Member consumer-spark_ml_testing-1-f7e4b33d-e2a7-49d8-9913-11db81f1d68a sending LeaveGroup request to coordinator DESKTOP-G76NQH1.localdomain:9092 (id: 2147483647 rack: null) due to the consumer is being closed[0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO JobGenerator: Stopped JobGenerator[0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO JobScheduler: Stopped JobScheduler[0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO StreamingContext: StreamingContext stopped successfully[0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO SparkContext: Invoking stop() from shutdown hook[0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO SparkUI: Stopped Spark web UI at http://172.19.152.53:4040[0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped![0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO MemoryStore: MemoryStore cleared[0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO BlockManager: BlockManager stopped[0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO BlockManagerMaster: BlockManagerMaster stopped[0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped![0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO SparkContext: Successfully stopped SparkContext[0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO ShutdownHookManager: Shutdown hook called[0m
[0m2021.07.01 13:08:51 ERROR 21/07/01 13:08:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-622e30ff-a0f8-4e31-8222-0f995fa5e694[0m
Jul 01, 2021 1:08:57 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 882
[0m2021.07.01 13:09:01 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 3m 10.232s)[0m
[0m2021.07.01 13:09:01 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 13:09:01 INFO  time: compiled ht3-test in 0.22s[0m
[0m2021.07.01 13:09:01 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 13:09:01 INFO  Listening for transport dt_socket at address: 46031[0m
[0m2021.07.01 13:09:01 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 13:09:01 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.01 13:09:01 INFO  Trying to attach to remote debuggee VM localhost:46031 .[0m
[0m2021.07.01 13:09:01 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 13:09:02 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 13:09:02 ERROR 21/07/01 13:09:02 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 13:09:02 ERROR 21/07/01 13:09:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 13:09:03 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 13:09:03 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 13:09:03 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 13:09:03 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 13:09:03 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 13:09:03 ERROR 21/07/01 13:09:04 INFO SparkContext: Running Spark version 3.1.1[0m
[0m2021.07.01 13:09:03 ERROR 21/07/01 13:09:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 13:09:03 ERROR 21/07/01 13:09:04 INFO ResourceUtils: ==============================================================[0m
[0m2021.07.01 13:09:03 ERROR 21/07/01 13:09:04 INFO ResourceUtils: No custom resources configured for spark.driver.[0m
[0m2021.07.01 13:09:03 ERROR 21/07/01 13:09:04 INFO ResourceUtils: ==============================================================[0m
[0m2021.07.01 13:09:03 ERROR 21/07/01 13:09:04 INFO SparkContext: Submitted application: SPARK_STREAMING_TO_HDFS[0m
[0m2021.07.01 13:09:03 ERROR 21/07/01 13:09:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)[0m
[0m2021.07.01 13:09:04 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:03 ERROR 21/07/01 13:09:04 INFO ResourceProfile: Limiting resource is cpu[0m
[0m2021.07.01 13:09:03 ERROR 21/07/01 13:09:04 INFO ResourceProfileManager: Added ResourceProfile id: 0[0m
[0m2021.07.01 13:09:03 ERROR 21/07/01 13:09:04 INFO SecurityManager: Changing view acls to: hwait[0m
[0m2021.07.01 13:09:03 ERROR 21/07/01 13:09:04 INFO SecurityManager: Changing modify acls to: hwait[0m
[0m2021.07.01 13:09:03 ERROR 21/07/01 13:09:04 INFO SecurityManager: Changing view acls groups to: [0m
[0m2021.07.01 13:09:03 ERROR 21/07/01 13:09:04 INFO SecurityManager: Changing modify acls groups to: [0m
[0m2021.07.01 13:09:03 ERROR 21/07/01 13:09:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hwait); groups with view permissions: Set(); users  with modify permissions: Set(hwait); groups with modify permissions: Set()[0m
[0m2021.07.01 13:09:04 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:03 ERROR 21/07/01 13:09:04 INFO Utils: Successfully started service 'sparkDriver' on port 32835.[0m
[0m2021.07.01 13:09:03 ERROR 21/07/01 13:09:04 INFO SparkEnv: Registering MapOutputTracker[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:04 INFO SparkEnv: Registering BlockManagerMaster[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a8863f6b-2ba7-4738-970f-674344521c13[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:05 INFO MemoryStore: MemoryStore started with capacity 2.2 GiB[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:05 INFO SparkEnv: Registering OutputCommitCoordinator[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:05 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.19.152.53:4040[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:05 INFO Executor: Starting executor ID driver on host 172.19.152.53[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33423.[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:05 INFO NettyBlockTransferService: Server created on 172.19.152.53:33423[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.19.152.53, 33423, None)[0m
[0m2021.07.01 13:09:05 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.152.53:33423 with 2.2 GiB RAM, BlockManagerId(driver, 172.19.152.53, 33423, None)[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.19.152.53, 33423, None)[0m
[0m2021.07.01 13:09:05 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.19.152.53, 33423, None)[0m
[0m2021.07.01 13:09:05 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:04 ERROR 21/07/01 13:09:05 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 WARN KafkaUtils: overriding enable.auto.commit to false for executor[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 WARN KafkaUtils: overriding auto.offset.reset to none for executor[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 WARN KafkaUtils: overriding executor group.id to spark-executor-spark_ml_testing[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO DirectKafkaInputDStream: Slide time = 30000 ms[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO DirectKafkaInputDStream: Checkpoint interval = null[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO DirectKafkaInputDStream: Remember interval = 30000 ms[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@2d2f0c4f[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO MappedDStream: Slide time = 30000 ms[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO MappedDStream: Storage level = Serialized 1x Replicated[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO MappedDStream: Checkpoint interval = null[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO MappedDStream: Remember interval = 30000 ms[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@51c621d2[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO ForEachDStream: Slide time = 30000 ms[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO ForEachDStream: Storage level = Serialized 1x Replicated[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO ForEachDStream: Checkpoint interval = null[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO ForEachDStream: Remember interval = 30000 ms[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@535c5b0a[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO ConsumerConfig: ConsumerConfig values: [0m
[0m2021.07.01 13:09:06 ERROR 	allow.auto.create.topics = true[0m
[0m2021.07.01 13:09:06 ERROR 	auto.commit.interval.ms = 5000[0m
[0m2021.07.01 13:09:06 ERROR 	auto.offset.reset = earliest[0m
[0m2021.07.01 13:09:06 ERROR 	bootstrap.servers = [localhost:9092][0m
[0m2021.07.01 13:09:06 ERROR 	check.crcs = true[0m
[0m2021.07.01 13:09:06 ERROR 	client.dns.lookup = use_all_dns_ips[0m
[0m2021.07.01 13:09:06 ERROR 	client.id = consumer-spark_ml_testing-1[0m
[0m2021.07.01 13:09:06 ERROR 	client.rack = [0m
[0m2021.07.01 13:09:06 ERROR 	connections.max.idle.ms = 540000[0m
[0m2021.07.01 13:09:06 ERROR 	default.api.timeout.ms = 60000[0m
[0m2021.07.01 13:09:06 ERROR 	enable.auto.commit = false[0m
[0m2021.07.01 13:09:06 ERROR 	exclude.internal.topics = true[0m
[0m2021.07.01 13:09:06 ERROR 	fetch.max.bytes = 52428800[0m
[0m2021.07.01 13:09:06 ERROR 	fetch.max.wait.ms = 500[0m
[0m2021.07.01 13:09:06 ERROR 	fetch.min.bytes = 1[0m
[0m2021.07.01 13:09:06 ERROR 	group.id = spark_ml_testing[0m
[0m2021.07.01 13:09:06 ERROR 	group.instance.id = null[0m
[0m2021.07.01 13:09:06 ERROR 	heartbeat.interval.ms = 3000[0m
[0m2021.07.01 13:09:06 ERROR 	interceptor.classes = [][0m
[0m2021.07.01 13:09:06 ERROR 	internal.leave.group.on.close = true[0m
[0m2021.07.01 13:09:06 ERROR 	internal.throw.on.fetch.stable.offset.unsupported = false[0m
[0m2021.07.01 13:09:06 ERROR 	isolation.level = read_uncommitted[0m
[0m2021.07.01 13:09:06 ERROR 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer[0m
[0m2021.07.01 13:09:06 ERROR 	max.partition.fetch.bytes = 1048576[0m
[0m2021.07.01 13:09:06 ERROR 	max.poll.interval.ms = 300000[0m
[0m2021.07.01 13:09:06 ERROR 	max.poll.records = 500[0m
[0m2021.07.01 13:09:06 ERROR 	metadata.max.age.ms = 300000[0m
[0m2021.07.01 13:09:06 ERROR 	metric.reporters = [][0m
[0m2021.07.01 13:09:06 ERROR 	metrics.num.samples = 2[0m
[0m2021.07.01 13:09:06 ERROR 	metrics.recording.level = INFO[0m
[0m2021.07.01 13:09:06 ERROR 	metrics.sample.window.ms = 30000[0m
[0m2021.07.01 13:09:06 ERROR 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor][0m
[0m2021.07.01 13:09:06 ERROR 	receive.buffer.bytes = 65536[0m
[0m2021.07.01 13:09:06 ERROR 	reconnect.backoff.max.ms = 1000[0m
[0m2021.07.01 13:09:06 ERROR 	reconnect.backoff.ms = 50[0m
[0m2021.07.01 13:09:06 ERROR 	request.timeout.ms = 30000[0m
[0m2021.07.01 13:09:06 ERROR 	retry.backoff.ms = 100[0m
[0m2021.07.01 13:09:06 ERROR 	sasl.client.callback.handler.class = null[0m
[0m2021.07.01 13:09:06 ERROR 	sasl.jaas.config = null[0m
[0m2021.07.01 13:09:06 ERROR 	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m2021.07.01 13:09:06 ERROR 	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m2021.07.01 13:09:06 ERROR 	sasl.kerberos.service.name = null[0m
[0m2021.07.01 13:09:06 ERROR 	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m2021.07.01 13:09:06 ERROR 	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m2021.07.01 13:09:06 ERROR 	sasl.login.callback.handler.class = null[0m
[0m2021.07.01 13:09:06 ERROR 	sasl.login.class = null[0m
[0m2021.07.01 13:09:06 ERROR 	sasl.login.refresh.buffer.seconds = 300[0m
[0m2021.07.01 13:09:06 ERROR 	sasl.login.refresh.min.period.seconds = 60[0m
[0m2021.07.01 13:09:06 ERROR 	sasl.login.refresh.window.factor = 0.8[0m
[0m2021.07.01 13:09:06 ERROR 	sasl.login.refresh.window.jitter = 0.05[0m
[0m2021.07.01 13:09:06 ERROR 	sasl.mechanism = GSSAPI[0m
[0m2021.07.01 13:09:06 ERROR 	security.protocol = PLAINTEXT[0m
[0m2021.07.01 13:09:06 ERROR 	security.providers = null[0m
[0m2021.07.01 13:09:06 ERROR 	send.buffer.bytes = 131072[0m
[0m2021.07.01 13:09:06 ERROR 	session.timeout.ms = 10000[0m
[0m2021.07.01 13:09:06 ERROR 	ssl.cipher.suites = null[0m
[0m2021.07.01 13:09:06 ERROR 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3][0m
[0m2021.07.01 13:09:06 ERROR 	ssl.endpoint.identification.algorithm = https[0m
[0m2021.07.01 13:09:06 ERROR 	ssl.engine.factory.class = null[0m
[0m2021.07.01 13:09:06 ERROR 	ssl.key.password = null[0m
[0m2021.07.01 13:09:06 ERROR 	ssl.keymanager.algorithm = SunX509[0m
[0m2021.07.01 13:09:06 ERROR 	ssl.keystore.location = null[0m
[0m2021.07.01 13:09:06 ERROR 	ssl.keystore.password = null[0m
[0m2021.07.01 13:09:06 ERROR 	ssl.keystore.type = JKS[0m
[0m2021.07.01 13:09:06 ERROR 	ssl.protocol = TLSv1.3[0m
[0m2021.07.01 13:09:06 ERROR 	ssl.provider = null[0m
[0m2021.07.01 13:09:06 ERROR 	ssl.secure.random.implementation = null[0m
[0m2021.07.01 13:09:06 ERROR 	ssl.trustmanager.algorithm = PKIX[0m
[0m2021.07.01 13:09:06 ERROR 	ssl.truststore.location = null[0m
[0m2021.07.01 13:09:06 ERROR 	ssl.truststore.password = null[0m
[0m2021.07.01 13:09:06 ERROR 	ssl.truststore.type = JKS[0m
[0m2021.07.01 13:09:06 ERROR 	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer[0m
[0m2021.07.01 13:09:06 ERROR [0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO AppInfoParser: Kafka version: 2.6.0[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO AppInfoParser: Kafka commitId: 62abe01bee039651[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO AppInfoParser: Kafka startTimeMs: 1625123346457[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO KafkaConsumer: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Subscribed to topic(s): spark_ml_poc[0m
[0m2021.07.01 13:09:05 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO Metadata: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Cluster ID: -TctB5CtQT2vWcpApmpk2w[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Discovered group coordinator DESKTOP-G76NQH1.localdomain:9092 (id: 2147483647 rack: null)[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] (Re-)joining group[0m
[0m2021.07.01 13:09:06 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] (Re-)joining group[0m
[0m2021.07.01 13:09:06 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Finished assignment for group at generation 3: {consumer-spark_ml_testing-1-9105dbf4-a147-4bfe-b6e0-f3a2ad15e88a=Assignment(partitions=[spark_ml_poc-0])}[0m
[0m2021.07.01 13:09:06 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Successfully joined group with generation 3[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Notifying assignor about the new Assignment(partitions=[spark_ml_poc-0])[0m
[0m2021.07.01 13:09:06 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Adding newly assigned partitions: spark_ml_poc-0[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Found no committed offset for partition spark_ml_poc-0[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Resetting offset for partition spark_ml_poc-0 to offset 0.[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO RecurringTimer: Started timer for JobGenerator at time 1625123370000[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO JobGenerator: Started JobGenerator at 1625123370000 ms[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO JobScheduler: Started JobScheduler[0m
[0m2021.07.01 13:09:06 ERROR 21/07/01 13:09:06 INFO StreamingContext: StreamingContext started[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Seeking to LATEST offset of partition spark_ml_poc-0[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Resetting offset for partition spark_ml_poc-0 to offset 9.[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO JobScheduler: Added jobs for time 1625123370000 ms[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO JobScheduler: Starting job streaming job 1625123370000 ms.0 from job set of time 1625123370000 ms[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO SparkContext: Starting job: isEmpty at SparkMLRegressionFromKafka.scala:60[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO DAGScheduler: Got job 0 (isEmpty at SparkMLRegressionFromKafka.scala:60) with 1 output partitions[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO DAGScheduler: Final stage: ResultStage 0 (isEmpty at SparkMLRegressionFromKafka.scala:60)[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 13:09:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 13:09:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkMLRegressionFromKafka.scala:57), which has no missing parents[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.0 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:09:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:09:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.152.53:33423 (size: 2.4 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkMLRegressionFromKafka.scala:57) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4360 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO KafkaRDD: Computing topic spark_ml_poc, partition 0 offsets 0 -> 9[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO KafkaDataConsumer: Initializing cache 16 64 0.75[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO ConsumerConfig: ConsumerConfig values: [0m
[0m2021.07.01 13:09:30 ERROR 	allow.auto.create.topics = true[0m
[0m2021.07.01 13:09:30 ERROR 	auto.commit.interval.ms = 5000[0m
[0m2021.07.01 13:09:30 ERROR 	auto.offset.reset = none[0m
[0m2021.07.01 13:09:30 ERROR 	bootstrap.servers = [localhost:9092][0m
[0m2021.07.01 13:09:30 ERROR 	check.crcs = true[0m
[0m2021.07.01 13:09:30 ERROR 	client.dns.lookup = use_all_dns_ips[0m
[0m2021.07.01 13:09:30 ERROR 	client.id = consumer-spark-executor-spark_ml_testing-2[0m
[0m2021.07.01 13:09:30 ERROR 	client.rack = [0m
[0m2021.07.01 13:09:30 ERROR 	connections.max.idle.ms = 540000[0m
[0m2021.07.01 13:09:30 ERROR 	default.api.timeout.ms = 60000[0m
[0m2021.07.01 13:09:30 ERROR 	enable.auto.commit = false[0m
[0m2021.07.01 13:09:30 ERROR 	exclude.internal.topics = true[0m
[0m2021.07.01 13:09:30 ERROR 	fetch.max.bytes = 52428800[0m
[0m2021.07.01 13:09:30 ERROR 	fetch.max.wait.ms = 500[0m
[0m2021.07.01 13:09:30 ERROR 	fetch.min.bytes = 1[0m
[0m2021.07.01 13:09:30 ERROR 	group.id = spark-executor-spark_ml_testing[0m
[0m2021.07.01 13:09:30 ERROR 	group.instance.id = null[0m
[0m2021.07.01 13:09:30 ERROR 	heartbeat.interval.ms = 3000[0m
[0m2021.07.01 13:09:30 ERROR 	interceptor.classes = [][0m
[0m2021.07.01 13:09:30 ERROR 	internal.leave.group.on.close = true[0m
[0m2021.07.01 13:09:30 ERROR 	internal.throw.on.fetch.stable.offset.unsupported = false[0m
[0m2021.07.01 13:09:30 ERROR 	isolation.level = read_uncommitted[0m
[0m2021.07.01 13:09:30 ERROR 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer[0m
[0m2021.07.01 13:09:30 ERROR 	max.partition.fetch.bytes = 1048576[0m
[0m2021.07.01 13:09:30 ERROR 	max.poll.interval.ms = 300000[0m
[0m2021.07.01 13:09:30 ERROR 	max.poll.records = 500[0m
[0m2021.07.01 13:09:30 ERROR 	metadata.max.age.ms = 300000[0m
[0m2021.07.01 13:09:30 ERROR 	metric.reporters = [][0m
[0m2021.07.01 13:09:30 ERROR 	metrics.num.samples = 2[0m
[0m2021.07.01 13:09:30 ERROR 	metrics.recording.level = INFO[0m
[0m2021.07.01 13:09:30 ERROR 	metrics.sample.window.ms = 30000[0m
[0m2021.07.01 13:09:30 ERROR 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor][0m
[0m2021.07.01 13:09:30 ERROR 	receive.buffer.bytes = 65536[0m
[0m2021.07.01 13:09:30 ERROR 	reconnect.backoff.max.ms = 1000[0m
[0m2021.07.01 13:09:30 ERROR 	reconnect.backoff.ms = 50[0m
[0m2021.07.01 13:09:30 ERROR 	request.timeout.ms = 30000[0m
[0m2021.07.01 13:09:30 ERROR 	retry.backoff.ms = 100[0m
[0m2021.07.01 13:09:30 ERROR 	sasl.client.callback.handler.class = null[0m
[0m2021.07.01 13:09:30 ERROR 	sasl.jaas.config = null[0m
[0m2021.07.01 13:09:30 ERROR 	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m2021.07.01 13:09:30 ERROR 	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m2021.07.01 13:09:30 ERROR 	sasl.kerberos.service.name = null[0m
[0m2021.07.01 13:09:30 ERROR 	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m2021.07.01 13:09:30 ERROR 	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m2021.07.01 13:09:30 ERROR 	sasl.login.callback.handler.class = null[0m
[0m2021.07.01 13:09:30 ERROR 	sasl.login.class = null[0m
[0m2021.07.01 13:09:30 ERROR 	sasl.login.refresh.buffer.seconds = 300[0m
[0m2021.07.01 13:09:30 ERROR 	sasl.login.refresh.min.period.seconds = 60[0m
[0m2021.07.01 13:09:30 ERROR 	sasl.login.refresh.window.factor = 0.8[0m
[0m2021.07.01 13:09:30 ERROR 	sasl.login.refresh.window.jitter = 0.05[0m
[0m2021.07.01 13:09:30 ERROR 	sasl.mechanism = GSSAPI[0m
[0m2021.07.01 13:09:30 ERROR 	security.protocol = PLAINTEXT[0m
[0m2021.07.01 13:09:30 ERROR 	security.providers = null[0m
[0m2021.07.01 13:09:30 ERROR 	send.buffer.bytes = 131072[0m
[0m2021.07.01 13:09:30 ERROR 	session.timeout.ms = 10000[0m
[0m2021.07.01 13:09:30 ERROR 	ssl.cipher.suites = null[0m
[0m2021.07.01 13:09:30 ERROR 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3][0m
[0m2021.07.01 13:09:30 ERROR 	ssl.endpoint.identification.algorithm = https[0m
[0m2021.07.01 13:09:30 ERROR 	ssl.engine.factory.class = null[0m
[0m2021.07.01 13:09:30 ERROR 	ssl.key.password = null[0m
[0m2021.07.01 13:09:30 ERROR 	ssl.keymanager.algorithm = SunX509[0m
[0m2021.07.01 13:09:30 ERROR 	ssl.keystore.location = null[0m
[0m2021.07.01 13:09:30 ERROR 	ssl.keystore.password = null[0m
[0m2021.07.01 13:09:30 ERROR 	ssl.keystore.type = JKS[0m
[0m2021.07.01 13:09:30 ERROR 	ssl.protocol = TLSv1.3[0m
[0m2021.07.01 13:09:30 ERROR 	ssl.provider = null[0m
[0m2021.07.01 13:09:30 ERROR 	ssl.secure.random.implementation = null[0m
[0m2021.07.01 13:09:30 ERROR 	ssl.trustmanager.algorithm = PKIX[0m
[0m2021.07.01 13:09:30 ERROR 	ssl.truststore.location = null[0m
[0m2021.07.01 13:09:30 ERROR 	ssl.truststore.password = null[0m
[0m2021.07.01 13:09:30 ERROR 	ssl.truststore.type = JKS[0m
[0m2021.07.01 13:09:30 ERROR 	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer[0m
[0m2021.07.01 13:09:30 ERROR [0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO AppInfoParser: Kafka version: 2.6.0[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO AppInfoParser: Kafka commitId: 62abe01bee039651[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO AppInfoParser: Kafka startTimeMs: 1625123370682[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-spark_ml_testing-2, groupId=spark-executor-spark_ml_testing] Subscribed to partition(s): spark_ml_poc-0[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO InternalKafkaConsumer: Initial fetch for spark-executor-spark_ml_testing spark_ml_poc-0 0[0m
[0m2021.07.01 13:09:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-spark_ml_testing-2, groupId=spark-executor-spark_ml_testing] Seeking to offset 0 for partition spark_ml_poc-0[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO Metadata: [Consumer clientId=consumer-spark-executor-spark_ml_testing-2, groupId=spark-executor-spark_ml_testing] Cluster ID: -TctB5CtQT2vWcpApmpk2w[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 900 bytes result sent to driver[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 282 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO DAGScheduler: ResultStage 0 (isEmpty at SparkMLRegressionFromKafka.scala:60) finished in 0.525 s[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO DAGScheduler: Job 0 finished: isEmpty at SparkMLRegressionFromKafka.scala:60, took 0.608366 s[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hwait/dev/ht3/spark-warehouse').[0m
[0m2021.07.01 13:09:30 ERROR 21/07/01 13:09:30 INFO SharedState: Warehouse path is 'file:/home/hwait/dev/ht3/spark-warehouse'.[0m
[0m2021.07.01 13:09:32 ERROR 21/07/01 13:09:32 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.19.152.53:33423 in memory (size: 2.4 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO CodeGenerator: Code generated in 272.8024 ms[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO SparkContext: Starting job: json at SparkMLRegressionFromKafka.scala:62[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO DAGScheduler: Got job 1 (json at SparkMLRegressionFromKafka.scala:62) with 1 output partitions[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO DAGScheduler: Final stage: ResultStage 1 (json at SparkMLRegressionFromKafka.scala:62)[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 13:09:34 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:34 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at json at SparkMLRegressionFromKafka.scala:62), which has no missing parents[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.6 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:09:34 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:09:34 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.152.53:33423 (size: 7.3 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at json at SparkMLRegressionFromKafka.scala:62) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4360 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO KafkaRDD: Computing topic spark_ml_poc, partition 0 offsets 0 -> 9[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO InternalKafkaConsumer: Initial fetch for spark-executor-spark_ml_testing spark_ml_poc-0 0[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-spark_ml_testing-2, groupId=spark-executor-spark_ml_testing] Seeking to offset 0 for partition spark_ml_poc-0[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2017 bytes result sent to driver[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 94 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO DAGScheduler: ResultStage 1 (json at SparkMLRegressionFromKafka.scala:62) finished in 0.131 s[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO DAGScheduler: Job 1 finished: json at SparkMLRegressionFromKafka.scala:62, took 0.138743 s[0m
[0m2021.07.01 13:09:34 INFO  root[0m
[0m2021.07.01 13:09:34 INFO   |-- SepalLength: long (nullable = true)[0m
[0m2021.07.01 13:09:34 INFO   |-- _corrupt_record: string (nullable = true)[0m
[0m2021.07.01 13:09:34 INFO  [0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO CodeGenerator: Code generated in 19.9362 ms[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO SparkContext: Starting job: show at SparkMLRegressionFromKafka.scala:65[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO DAGScheduler: Got job 2 (show at SparkMLRegressionFromKafka.scala:65) with 1 output partitions[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO DAGScheduler: Final stage: ResultStage 2 (show at SparkMLRegressionFromKafka.scala:65)[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 13:09:34 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:34 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at show at SparkMLRegressionFromKafka.scala:65), which has no missing parents[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.2 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:09:34 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:09:34 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.152.53:33423 (size: 6.8 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at show at SparkMLRegressionFromKafka.scala:65) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4360 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO KafkaRDD: Computing topic spark_ml_poc, partition 0 offsets 0 -> 9[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO InternalKafkaConsumer: Initial fetch for spark-executor-spark_ml_testing spark_ml_poc-0 0[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:34 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-spark_ml_testing-2, groupId=spark-executor-spark_ml_testing] Seeking to offset 0 for partition spark_ml_poc-0[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:35 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1485 bytes result sent to driver[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:35 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 207 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:35 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:35 INFO DAGScheduler: ResultStage 2 (show at SparkMLRegressionFromKafka.scala:65) finished in 0.224 s[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:35 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:35 INFO DAGScheduler: Job 2 finished: show at SparkMLRegressionFromKafka.scala:65, took 0.230982 s[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:35 INFO CodeGenerator: Code generated in 18.5714 ms[0m
[0m2021.07.01 13:09:34 INFO  +-----------+---------------+[0m
[0m2021.07.01 13:09:34 INFO  |SepalLength|_corrupt_record|[0m
[0m2021.07.01 13:09:34 INFO  +-----------+---------------+[0m
[0m2021.07.01 13:09:34 INFO  |          1|           null|[0m
[0m2021.07.01 13:09:34 INFO  |       null|             cc|[0m
[0m2021.07.01 13:09:34 INFO  |          1|           null|[0m
[0m2021.07.01 13:09:34 INFO  |          1|           null|[0m
[0m2021.07.01 13:09:34 INFO  |          1|           null|[0m
[0m2021.07.01 13:09:34 INFO  |          1|           null|[0m
[0m2021.07.01 13:09:34 INFO  |          1|           null|[0m
[0m2021.07.01 13:09:34 INFO  |          1|           null|[0m
[0m2021.07.01 13:09:34 INFO  |          1|           null|[0m
[0m2021.07.01 13:09:34 INFO  +-----------+---------------+[0m
[0m2021.07.01 13:09:34 INFO  [0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:35 INFO JobScheduler: Finished job streaming job 1625123370000 ms.0 from job set of time 1625123370000 ms[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:35 INFO JobScheduler: Total delay: 5.147 s for time 1625123370000 ms (execution: 5.039 s)[0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:35 INFO ReceivedBlockTracker: Deleting batches: [0m
[0m2021.07.01 13:09:34 ERROR 21/07/01 13:09:35 INFO InputInfoTracker: remove old batch metadata: [0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Seeking to LATEST offset of partition spark_ml_poc-0[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Resetting offset for partition spark_ml_poc-0 to offset 9.[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO JobScheduler: Added jobs for time 1625123400000 ms[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO JobScheduler: Starting job streaming job 1625123400000 ms.0 from job set of time 1625123400000 ms[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO SparkContext: Starting job: isEmpty at SparkMLRegressionFromKafka.scala:60[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO DAGScheduler: Got job 3 (isEmpty at SparkMLRegressionFromKafka.scala:60) with 1 output partitions[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO DAGScheduler: Final stage: ResultStage 3 (isEmpty at SparkMLRegressionFromKafka.scala:60)[0m
[0m2021.07.01 13:10:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 13:10:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at map at SparkMLRegressionFromKafka.scala:57), which has no missing parents[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 4.0 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:10:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.19.152.53:33423 (size: 2.4 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:10:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at map at SparkMLRegressionFromKafka.scala:57) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4360 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO KafkaRDD: Beginning offset 9 is the same as ending offset skipping spark_ml_poc 0[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 837 bytes result sent to driver[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 14 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO DAGScheduler: ResultStage 3 (isEmpty at SparkMLRegressionFromKafka.scala:60) finished in 0.027 s[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO DAGScheduler: Job 3 finished: isEmpty at SparkMLRegressionFromKafka.scala:60, took 0.035602 s[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO JobScheduler: Finished job streaming job 1625123400000 ms.0 from job set of time 1625123400000 ms[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO JobScheduler: Total delay: 0.088 s for time 1625123400000 ms (execution: 0.060 s)[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO MapPartitionsRDD: Removing RDD 1 from persistence list[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO BlockManager: Removing RDD 1[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO KafkaRDD: Removing RDD 0 from persistence list[0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO ReceivedBlockTracker: Deleting batches: [0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO InputInfoTracker: remove old batch metadata: [0m
[0m2021.07.01 13:10:00 ERROR 21/07/01 13:10:00 INFO BlockManager: Removing RDD 0[0m
[0m2021.07.01 13:10:29 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Seeking to LATEST offset of partition spark_ml_poc-0[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Resetting offset for partition spark_ml_poc-0 to offset 9.[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO JobScheduler: Added jobs for time 1625123430000 ms[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO JobScheduler: Starting job streaming job 1625123430000 ms.0 from job set of time 1625123430000 ms[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO SparkContext: Starting job: isEmpty at SparkMLRegressionFromKafka.scala:60[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO DAGScheduler: Got job 4 (isEmpty at SparkMLRegressionFromKafka.scala:60) with 1 output partitions[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO DAGScheduler: Final stage: ResultStage 4 (isEmpty at SparkMLRegressionFromKafka.scala:60)[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 13:10:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at map at SparkMLRegressionFromKafka.scala:57), which has no missing parents[0m
[0m2021.07.01 13:10:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 4.0 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:10:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:10:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.19.152.53:33423 (size: 2.4 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at map at SparkMLRegressionFromKafka.scala:57) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4360 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO KafkaRDD: Beginning offset 9 is the same as ending offset skipping spark_ml_poc 0[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 837 bytes result sent to driver[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 10 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO DAGScheduler: ResultStage 4 (isEmpty at SparkMLRegressionFromKafka.scala:60) finished in 0.021 s[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO DAGScheduler: Job 4 finished: isEmpty at SparkMLRegressionFromKafka.scala:60, took 0.025077 s[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO JobScheduler: Finished job streaming job 1625123430000 ms.0 from job set of time 1625123430000 ms[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO JobScheduler: Total delay: 0.043 s for time 1625123430000 ms (execution: 0.036 s)[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO MapPartitionsRDD: Removing RDD 13 from persistence list[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO KafkaRDD: Removing RDD 12 from persistence list[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO BlockManager: Removing RDD 13[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO ReceivedBlockTracker: Deleting batches: [0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO BlockManager: Removing RDD 12[0m
[0m2021.07.01 13:10:29 ERROR 21/07/01 13:10:30 INFO InputInfoTracker: remove old batch metadata: 1625123370000 ms[0m
[0m2021.07.01 13:10:30 INFO  time: compiled ht3 in 1.16s[0m
[0m2021.07.01 13:10:44 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 13:10:44 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 13:10:44 INFO  time: compiled ht3 in 0.31s[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Seeking to LATEST offset of partition spark_ml_poc-0[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Resetting offset for partition spark_ml_poc-0 to offset 9.[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO JobScheduler: Added jobs for time 1625123460000 ms[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO JobScheduler: Starting job streaming job 1625123460000 ms.0 from job set of time 1625123460000 ms[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO SparkContext: Starting job: isEmpty at SparkMLRegressionFromKafka.scala:60[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO DAGScheduler: Got job 5 (isEmpty at SparkMLRegressionFromKafka.scala:60) with 1 output partitions[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO DAGScheduler: Final stage: ResultStage 5 (isEmpty at SparkMLRegressionFromKafka.scala:60)[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 13:11:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[17] at map at SparkMLRegressionFromKafka.scala:57), which has no missing parents[0m
[0m2021.07.01 13:11:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 4.0 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:11:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:11:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.19.152.53:33423 (size: 2.4 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at map at SparkMLRegressionFromKafka.scala:57) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4360 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO KafkaRDD: Beginning offset 9 is the same as ending offset skipping spark_ml_poc 0[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 837 bytes result sent to driver[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 9 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO DAGScheduler: ResultStage 5 (isEmpty at SparkMLRegressionFromKafka.scala:60) finished in 0.021 s[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO DAGScheduler: Job 5 finished: isEmpty at SparkMLRegressionFromKafka.scala:60, took 0.027144 s[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO JobScheduler: Finished job streaming job 1625123460000 ms.0 from job set of time 1625123460000 ms[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO JobScheduler: Total delay: 0.052 s for time 1625123460000 ms (execution: 0.043 s)[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO MapPartitionsRDD: Removing RDD 15 from persistence list[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO KafkaRDD: Removing RDD 14 from persistence list[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO BlockManager: Removing RDD 15[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO ReceivedBlockTracker: Deleting batches: [0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO InputInfoTracker: remove old batch metadata: 1625123400000 ms[0m
[0m2021.07.01 13:11:00 ERROR 21/07/01 13:11:00 INFO BlockManager: Removing RDD 14[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Seeking to LATEST offset of partition spark_ml_poc-0[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Resetting offset for partition spark_ml_poc-0 to offset 9.[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO JobScheduler: Added jobs for time 1625123490000 ms[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO JobScheduler: Starting job streaming job 1625123490000 ms.0 from job set of time 1625123490000 ms[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO SparkContext: Starting job: isEmpty at SparkMLRegressionFromKafka.scala:60[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO DAGScheduler: Got job 6 (isEmpty at SparkMLRegressionFromKafka.scala:60) with 1 output partitions[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO DAGScheduler: Final stage: ResultStage 6 (isEmpty at SparkMLRegressionFromKafka.scala:60)[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[19] at map at SparkMLRegressionFromKafka.scala:57), which has no missing parents[0m
[0m2021.07.01 13:11:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:11:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 4.0 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:11:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 2.2 GiB)[0m
[0m2021.07.01 13:11:30 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.19.152.53:33423 (size: 2.4 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[19] at map at SparkMLRegressionFromKafka.scala:57) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4360 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO KafkaRDD: Beginning offset 9 is the same as ending offset skipping spark_ml_poc 0[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 837 bytes result sent to driver[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 10 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO DAGScheduler: ResultStage 6 (isEmpty at SparkMLRegressionFromKafka.scala:60) finished in 0.020 s[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO DAGScheduler: Job 6 finished: isEmpty at SparkMLRegressionFromKafka.scala:60, took 0.025707 s[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO JobScheduler: Finished job streaming job 1625123490000 ms.0 from job set of time 1625123490000 ms[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO MapPartitionsRDD: Removing RDD 17 from persistence list[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO JobScheduler: Total delay: 0.061 s for time 1625123490000 ms (execution: 0.052 s)[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO KafkaRDD: Removing RDD 16 from persistence list[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO BlockManager: Removing RDD 17[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO ReceivedBlockTracker: Deleting batches: [0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO BlockManager: Removing RDD 16[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO InputInfoTracker: remove old batch metadata: 1625123430000 ms[0m
[0m2021.07.01 13:11:30 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.01 13:11:30 INFO  Listening for transport dt_socket at address: 44357[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO ReceiverTracker: ReceiverTracker stopped[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO JobGenerator: Stopping JobGenerator immediately[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO RecurringTimer: Stopped timer for JobGenerator after time 1625123490000[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Revoke previously assigned partitions spark_ml_poc-0[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Member consumer-spark_ml_testing-1-9105dbf4-a147-4bfe-b6e0-f3a2ad15e88a sending LeaveGroup request to coordinator DESKTOP-G76NQH1.localdomain:9092 (id: 2147483647 rack: null) due to the consumer is being closed[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO JobGenerator: Stopped JobGenerator[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO JobScheduler: Stopped JobScheduler[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO StreamingContext: StreamingContext stopped successfully[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO SparkContext: Invoking stop() from shutdown hook[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO SparkUI: Stopped Spark web UI at http://172.19.152.53:4040[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped![0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO MemoryStore: MemoryStore cleared[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO BlockManager: BlockManager stopped[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO BlockManagerMaster: BlockManagerMaster stopped[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped![0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO SparkContext: Successfully stopped SparkContext[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO ShutdownHookManager: Shutdown hook called[0m
[0m2021.07.01 13:11:30 ERROR 21/07/01 13:11:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-2c8058b3-c43c-458f-a418-8c6490aa9cd6[0m
[0m2021.07.01 13:12:48 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 13:12:48 INFO  time: compiled ht3 in 0.36s[0m
[0m2021.07.01 13:12:48 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 13:12:50 INFO  time: compiled ht3 in 1.65s[0m
[0m2021.07.01 13:12:56 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 7m 5.056s)[0m
[0m2021.07.01 13:12:56 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 13:12:56 INFO  time: compiled ht3-test in 0.22s[0m
[0m2021.07.01 13:12:56 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 13:12:56 INFO  Listening for transport dt_socket at address: 37907[0m
[0m2021.07.01 13:12:56 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 13:12:56 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.01 13:12:56 INFO  Trying to attach to remote debuggee VM localhost:37907 .[0m
[0m2021.07.01 13:12:56 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 13:12:58 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 13:12:58 ERROR 21/07/01 13:12:58 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 13:12:58 ERROR 21/07/01 13:12:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 13:12:59 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 13:12:59 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 13:12:59 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 13:12:59 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 13:12:59 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 13:12:59 ERROR 21/07/01 13:12:59 INFO SparkContext: Running Spark version 3.1.1[0m
[0m2021.07.01 13:12:59 ERROR 21/07/01 13:12:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 13:12:59 ERROR 21/07/01 13:12:59 INFO ResourceUtils: ==============================================================[0m
[0m2021.07.01 13:12:59 ERROR 21/07/01 13:12:59 INFO ResourceUtils: No custom resources configured for spark.driver.[0m
[0m2021.07.01 13:12:59 ERROR 21/07/01 13:12:59 INFO ResourceUtils: ==============================================================[0m
[0m2021.07.01 13:12:59 ERROR 21/07/01 13:12:59 INFO SparkContext: Submitted application: SPARK_STREAMING_TO_HDFS[0m
[0m2021.07.01 13:12:59 ERROR 21/07/01 13:12:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)[0m
[0m2021.07.01 13:12:59 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:12:59 ERROR 21/07/01 13:12:59 INFO ResourceProfile: Limiting resource is cpu[0m
[0m2021.07.01 13:12:59 ERROR 21/07/01 13:12:59 INFO ResourceProfileManager: Added ResourceProfile id: 0[0m
[0m2021.07.01 13:12:59 ERROR 21/07/01 13:12:59 INFO SecurityManager: Changing view acls to: hwait[0m
[0m2021.07.01 13:12:59 ERROR 21/07/01 13:12:59 INFO SecurityManager: Changing modify acls to: hwait[0m
[0m2021.07.01 13:12:59 ERROR 21/07/01 13:12:59 INFO SecurityManager: Changing view acls groups to: [0m
[0m2021.07.01 13:12:59 ERROR 21/07/01 13:12:59 INFO SecurityManager: Changing modify acls groups to: [0m
[0m2021.07.01 13:12:59 ERROR 21/07/01 13:12:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hwait); groups with view permissions: Set(); users  with modify permissions: Set(hwait); groups with modify permissions: Set()[0m
[0m2021.07.01 13:12:59 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:12:59 ERROR 21/07/01 13:13:00 INFO Utils: Successfully started service 'sparkDriver' on port 43303.[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO SparkEnv: Registering MapOutputTracker[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO SparkEnv: Registering BlockManagerMaster[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f5071a65-e469-47fe-9c2e-9ddcb409ac72[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO MemoryStore: MemoryStore started with capacity 2.2 GiB[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO SparkEnv: Registering OutputCommitCoordinator[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.19.152.53:4040[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO Executor: Starting executor ID driver on host 172.19.152.53[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39153.[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO NettyBlockTransferService: Server created on 172.19.152.53:39153[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.19.152.53, 39153, None)[0m
[0m2021.07.01 13:13:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.152.53:39153 with 2.2 GiB RAM, BlockManagerId(driver, 172.19.152.53, 39153, None)[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.19.152.53, 39153, None)[0m
[0m2021.07.01 13:13:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.19.152.53, 39153, None)[0m
[0m2021.07.01 13:13:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:13:00 ERROR 21/07/01 13:13:01 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 WARN KafkaUtils: overriding enable.auto.commit to false for executor[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 WARN KafkaUtils: overriding auto.offset.reset to none for executor[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 WARN KafkaUtils: overriding executor group.id to spark-executor-spark_ml_testing[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO DirectKafkaInputDStream: Slide time = 30000 ms[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO DirectKafkaInputDStream: Checkpoint interval = null[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO DirectKafkaInputDStream: Remember interval = 30000 ms[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@a225ef9[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO MappedDStream: Slide time = 30000 ms[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO MappedDStream: Storage level = Serialized 1x Replicated[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO MappedDStream: Checkpoint interval = null[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO MappedDStream: Remember interval = 30000 ms[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@40d5deab[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO ForEachDStream: Slide time = 30000 ms[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO ForEachDStream: Storage level = Serialized 1x Replicated[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO ForEachDStream: Checkpoint interval = null[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO ForEachDStream: Remember interval = 30000 ms[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@6a79be66[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO ConsumerConfig: ConsumerConfig values: [0m
[0m2021.07.01 13:13:01 ERROR 	allow.auto.create.topics = true[0m
[0m2021.07.01 13:13:01 ERROR 	auto.commit.interval.ms = 5000[0m
[0m2021.07.01 13:13:01 ERROR 	auto.offset.reset = earliest[0m
[0m2021.07.01 13:13:01 ERROR 	bootstrap.servers = [localhost:9092][0m
[0m2021.07.01 13:13:01 ERROR 	check.crcs = true[0m
[0m2021.07.01 13:13:01 ERROR 	client.dns.lookup = use_all_dns_ips[0m
[0m2021.07.01 13:13:01 ERROR 	client.id = consumer-spark_ml_testing-1[0m
[0m2021.07.01 13:13:01 ERROR 	client.rack = [0m
[0m2021.07.01 13:13:01 ERROR 	connections.max.idle.ms = 540000[0m
[0m2021.07.01 13:13:01 ERROR 	default.api.timeout.ms = 60000[0m
[0m2021.07.01 13:13:01 ERROR 	enable.auto.commit = false[0m
[0m2021.07.01 13:13:01 ERROR 	exclude.internal.topics = true[0m
[0m2021.07.01 13:13:01 ERROR 	fetch.max.bytes = 52428800[0m
[0m2021.07.01 13:13:01 ERROR 	fetch.max.wait.ms = 500[0m
[0m2021.07.01 13:13:01 ERROR 	fetch.min.bytes = 1[0m
[0m2021.07.01 13:13:01 ERROR 	group.id = spark_ml_testing[0m
[0m2021.07.01 13:13:01 ERROR 	group.instance.id = null[0m
[0m2021.07.01 13:13:01 ERROR 	heartbeat.interval.ms = 3000[0m
[0m2021.07.01 13:13:01 ERROR 	interceptor.classes = [][0m
[0m2021.07.01 13:13:01 ERROR 	internal.leave.group.on.close = true[0m
[0m2021.07.01 13:13:01 ERROR 	internal.throw.on.fetch.stable.offset.unsupported = false[0m
[0m2021.07.01 13:13:01 ERROR 	isolation.level = read_uncommitted[0m
[0m2021.07.01 13:13:01 ERROR 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer[0m
[0m2021.07.01 13:13:01 ERROR 	max.partition.fetch.bytes = 1048576[0m
[0m2021.07.01 13:13:01 ERROR 	max.poll.interval.ms = 300000[0m
[0m2021.07.01 13:13:01 ERROR 	max.poll.records = 500[0m
[0m2021.07.01 13:13:01 ERROR 	metadata.max.age.ms = 300000[0m
[0m2021.07.01 13:13:01 ERROR 	metric.reporters = [][0m
[0m2021.07.01 13:13:01 ERROR 	metrics.num.samples = 2[0m
[0m2021.07.01 13:13:01 ERROR 	metrics.recording.level = INFO[0m
[0m2021.07.01 13:13:01 ERROR 	metrics.sample.window.ms = 30000[0m
[0m2021.07.01 13:13:01 ERROR 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor][0m
[0m2021.07.01 13:13:01 ERROR 	receive.buffer.bytes = 65536[0m
[0m2021.07.01 13:13:01 ERROR 	reconnect.backoff.max.ms = 1000[0m
[0m2021.07.01 13:13:01 ERROR 	reconnect.backoff.ms = 50[0m
[0m2021.07.01 13:13:01 ERROR 	request.timeout.ms = 30000[0m
[0m2021.07.01 13:13:01 ERROR 	retry.backoff.ms = 100[0m
[0m2021.07.01 13:13:01 ERROR 	sasl.client.callback.handler.class = null[0m
[0m2021.07.01 13:13:01 ERROR 	sasl.jaas.config = null[0m
[0m2021.07.01 13:13:01 ERROR 	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m2021.07.01 13:13:01 ERROR 	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m2021.07.01 13:13:01 ERROR 	sasl.kerberos.service.name = null[0m
[0m2021.07.01 13:13:01 ERROR 	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m2021.07.01 13:13:01 ERROR 	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m2021.07.01 13:13:01 ERROR 	sasl.login.callback.handler.class = null[0m
[0m2021.07.01 13:13:01 ERROR 	sasl.login.class = null[0m
[0m2021.07.01 13:13:01 ERROR 	sasl.login.refresh.buffer.seconds = 300[0m
[0m2021.07.01 13:13:01 ERROR 	sasl.login.refresh.min.period.seconds = 60[0m
[0m2021.07.01 13:13:01 ERROR 	sasl.login.refresh.window.factor = 0.8[0m
[0m2021.07.01 13:13:01 ERROR 	sasl.login.refresh.window.jitter = 0.05[0m
[0m2021.07.01 13:13:01 ERROR 	sasl.mechanism = GSSAPI[0m
[0m2021.07.01 13:13:01 ERROR 	security.protocol = PLAINTEXT[0m
[0m2021.07.01 13:13:01 ERROR 	security.providers = null[0m
[0m2021.07.01 13:13:01 ERROR 	send.buffer.bytes = 131072[0m
[0m2021.07.01 13:13:01 ERROR 	session.timeout.ms = 10000[0m
[0m2021.07.01 13:13:01 ERROR 	ssl.cipher.suites = null[0m
[0m2021.07.01 13:13:01 ERROR 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3][0m
[0m2021.07.01 13:13:01 ERROR 	ssl.endpoint.identification.algorithm = https[0m
[0m2021.07.01 13:13:01 ERROR 	ssl.engine.factory.class = null[0m
[0m2021.07.01 13:13:01 ERROR 	ssl.key.password = null[0m
[0m2021.07.01 13:13:01 ERROR 	ssl.keymanager.algorithm = SunX509[0m
[0m2021.07.01 13:13:01 ERROR 	ssl.keystore.location = null[0m
[0m2021.07.01 13:13:01 ERROR 	ssl.keystore.password = null[0m
[0m2021.07.01 13:13:01 ERROR 	ssl.keystore.type = JKS[0m
[0m2021.07.01 13:13:01 ERROR 	ssl.protocol = TLSv1.3[0m
[0m2021.07.01 13:13:01 ERROR 	ssl.provider = null[0m
[0m2021.07.01 13:13:01 ERROR 	ssl.secure.random.implementation = null[0m
[0m2021.07.01 13:13:01 ERROR 	ssl.trustmanager.algorithm = PKIX[0m
[0m2021.07.01 13:13:01 ERROR 	ssl.truststore.location = null[0m
[0m2021.07.01 13:13:01 ERROR 	ssl.truststore.password = null[0m
[0m2021.07.01 13:13:01 ERROR 	ssl.truststore.type = JKS[0m
[0m2021.07.01 13:13:01 ERROR 	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer[0m
[0m2021.07.01 13:13:01 ERROR [0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO AppInfoParser: Kafka version: 2.6.0[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO AppInfoParser: Kafka commitId: 62abe01bee039651[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO AppInfoParser: Kafka startTimeMs: 1625123581655[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Subscribed to topic(s): spark_ml_poc[0m
[0m2021.07.01 13:13:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO Metadata: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Cluster ID: -TctB5CtQT2vWcpApmpk2w[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Discovered group coordinator DESKTOP-G76NQH1.localdomain:9092 (id: 2147483647 rack: null)[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:01 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] (Re-)joining group[0m
[0m2021.07.01 13:13:01 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:02 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:02 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] (Re-)joining group[0m
[0m2021.07.01 13:13:01 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:02 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Finished assignment for group at generation 5: {consumer-spark_ml_testing-1-fb34afb2-1fc2-4c95-9f5e-736391d37fd3=Assignment(partitions=[spark_ml_poc-0])}[0m
[0m2021.07.01 13:13:01 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:02 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Successfully joined group with generation 5[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:02 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Notifying assignor about the new Assignment(partitions=[spark_ml_poc-0])[0m
[0m2021.07.01 13:13:01 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:02 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Adding newly assigned partitions: spark_ml_poc-0[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:02 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Found no committed offset for partition spark_ml_poc-0[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:02 INFO SubscriptionState: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Resetting offset for partition spark_ml_poc-0 to offset 0.[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:02 INFO RecurringTimer: Started timer for JobGenerator at time 1625123610000[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:02 INFO JobGenerator: Started JobGenerator at 1625123610000 ms[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:02 INFO JobScheduler: Started JobScheduler[0m
[0m2021.07.01 13:13:01 ERROR 21/07/01 13:13:02 INFO StreamingContext: StreamingContext started[0m
[0m2021.07.01 13:13:19 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.01 13:13:19 INFO  Listening for transport dt_socket at address: 48637[0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook[0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO ReceiverTracker: ReceiverTracker stopped[0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO JobGenerator: Stopping JobGenerator immediately[0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO RecurringTimer: Stopped timer for JobGenerator after time -1[0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Revoke previously assigned partitions spark_ml_poc-0[0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO AbstractCoordinator: [Consumer clientId=consumer-spark_ml_testing-1, groupId=spark_ml_testing] Member consumer-spark_ml_testing-1-fb34afb2-1fc2-4c95-9f5e-736391d37fd3 sending LeaveGroup request to coordinator DESKTOP-G76NQH1.localdomain:9092 (id: 2147483647 rack: null) due to the consumer is being closed[0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO JobGenerator: Stopped JobGenerator[0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO JobScheduler: Stopped JobScheduler[0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO StreamingContext: StreamingContext stopped successfully[0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO SparkContext: Invoking stop() from shutdown hook[0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO SparkUI: Stopped Spark web UI at http://172.19.152.53:4040[0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped![0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO MemoryStore: MemoryStore cleared[0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO BlockManager: BlockManager stopped[0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO BlockManagerMaster: BlockManagerMaster stopped[0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped![0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO SparkContext: Successfully stopped SparkContext[0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO ShutdownHookManager: Shutdown hook called[0m
[0m2021.07.01 13:13:19 ERROR 21/07/01 13:13:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-6720858d-a8fa-4dfd-8a59-c737581771e7[0m
[0m2021.07.01 13:13:21 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 13:13:22 INFO  time: compiled ht3 in 1.21s[0m
[0m2021.07.01 13:13:22 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 7m 31.937s)[0m
[0m2021.07.01 13:13:22 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 13:13:22 INFO  time: compiled ht3-test in 0.12s[0m
[0m2021.07.01 13:13:23 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 13:13:22 INFO  Listening for transport dt_socket at address: 39915[0m
[0m2021.07.01 13:13:23 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 13:13:23 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.01 13:13:22 INFO  Trying to attach to remote debuggee VM localhost:39915 .[0m
[0m2021.07.01 13:13:22 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 13:13:24 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 13:13:24 ERROR 21/07/01 13:13:24 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 13:13:24 ERROR 21/07/01 13:13:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 13:13:25 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 13:13:25 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 13:13:25 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 13:13:25 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 13:13:25 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 13:13:25 ERROR 21/07/01 13:13:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 13:13:27 ERROR 21/07/01 13:13:27 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.[0m
[0m2021.07.01 13:13:27 ERROR 21/07/01 13:13:27 WARN KafkaUtils: overriding enable.auto.commit to false for executor[0m
[0m2021.07.01 13:13:27 ERROR 21/07/01 13:13:27 WARN KafkaUtils: overriding auto.offset.reset to none for executor[0m
[0m2021.07.01 13:13:27 ERROR 21/07/01 13:13:27 WARN KafkaUtils: overriding executor group.id to spark-executor-spark_ml_testing[0m
[0m2021.07.01 13:13:27 ERROR 21/07/01 13:13:27 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135[0m
[0m2021.07.01 13:13:34 INFO  root[0m
[0m2021.07.01 13:13:34 INFO   |-- SepalLength: long (nullable = true)[0m
[0m2021.07.01 13:13:34 INFO   |-- _corrupt_record: string (nullable = true)[0m
[0m2021.07.01 13:13:34 INFO  [0m
[0m2021.07.01 13:13:34 INFO  +-----------+---------------+[0m
[0m2021.07.01 13:13:34 INFO  |SepalLength|_corrupt_record|[0m
[0m2021.07.01 13:13:34 INFO  +-----------+---------------+[0m
[0m2021.07.01 13:13:34 INFO  |          1|           null|[0m
[0m2021.07.01 13:13:34 INFO  |       null|             cc|[0m
[0m2021.07.01 13:13:34 INFO  |          1|           null|[0m
[0m2021.07.01 13:13:34 INFO  |          1|           null|[0m
[0m2021.07.01 13:13:34 INFO  |          1|           null|[0m
[0m2021.07.01 13:13:34 INFO  |          1|           null|[0m
[0m2021.07.01 13:13:34 INFO  |          1|           null|[0m
[0m2021.07.01 13:13:34 INFO  |          1|           null|[0m
[0m2021.07.01 13:13:34 INFO  |          1|           null|[0m
[0m2021.07.01 13:13:34 INFO  +-----------+---------------+[0m
[0m2021.07.01 13:13:34 INFO  [0m
Jul 01, 2021 1:51:17 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1206
[0m2021.07.01 13:54:42 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.01 13:54:42 INFO  Listening for transport dt_socket at address: 38935[0m
[0m2021.07.01 20:57:57 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 20:57:59 INFO  time: compiled ht3 in 2.19s[0m
[0m2021.07.01 20:57:59 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 20:57:59 INFO  time: compiled ht3 in 0.41s[0m
[0m2021.07.01 20:57:59 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 20:57:59 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 7h 52m 9.217s)[0m
[0m2021.07.01 20:57:59 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 20:57:59 INFO  time: compiled ht3 in 0.29s[0m
[0m2021.07.01 20:57:59 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 20:58:00 INFO  time: compiled ht3 in 0.22s[0m
Jul 01, 2021 8:58:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
java.util.concurrent.CompletionException: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:29)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:26)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:237)
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:229)
	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417)
	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
	... 4 more

[0m2021.07.01 20:58:17 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 20:58:17 INFO  time: compiled ht3 in 0.31s[0m
[0m2021.07.01 20:58:19 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 7h 52m 28.593s)[0m
[0m2021.07.01 20:58:19 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 20:58:19 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 20:58:19 INFO  time: compiled ht3 in 0.34s[0m
[0m2021.07.01 20:58:19 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 20:58:19 INFO  time: compiled ht3 in 0.18s[0m
Jul 01, 2021 8:58:20 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
java.util.concurrent.CompletionException: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:29)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:26)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:237)
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:229)
	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417)
	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
	... 4 more

[0m2021.07.01 20:58:31 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 20:58:31 INFO  time: compiled ht3 in 0.27s[0m
Jul 01, 2021 8:58:45 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1492
[0m2021.07.01 20:59:09 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 20:59:09 INFO  time: compiled ht3 in 0.3s[0m
[0m2021.07.01 20:59:09 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 20:59:09 INFO  time: compiled ht3 in 0.23s[0m
[0m2021.07.01 20:59:27 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.01 20:59:29 INFO  time: compiled ht3 in 1.98s[0m
Jul 01, 2021 8:59:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1554
[0m2021.07.01 21:00:22 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 7h 54m 30.949s)[0m
[0m2021.07.01 21:00:22 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 21:00:22 INFO  time: compiled ht3-test in 0.24s[0m
[0m2021.07.01 21:00:22 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 21:00:22 INFO  Listening for transport dt_socket at address: 34771[0m
[0m2021.07.01 21:00:22 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 21:00:22 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 21:00:22 INFO  Trying to attach to remote debuggee VM localhost:34771 .[0m
[0m2021.07.01 21:00:22 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 21:00:24 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 21:00:24 ERROR 21/07/01 21:00:24 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 21:00:24 ERROR 21/07/01 21:00:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 21:00:25 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 21:00:25 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 21:00:25 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 21:00:25 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 21:00:25 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 21:00:25 ERROR 21/07/01 21:00:25 INFO SparkContext: Running Spark version 3.1.1[0m
[0m2021.07.01 21:00:25 ERROR 21/07/01 21:00:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 21:00:25 ERROR 21/07/01 21:00:25 INFO ResourceUtils: ==============================================================[0m
[0m2021.07.01 21:00:25 ERROR 21/07/01 21:00:25 INFO ResourceUtils: No custom resources configured for spark.driver.[0m
[0m2021.07.01 21:00:25 ERROR 21/07/01 21:00:25 INFO ResourceUtils: ==============================================================[0m
[0m2021.07.01 21:00:25 ERROR 21/07/01 21:00:25 INFO SparkContext: Submitted application: Data Sources Practice[0m
[0m2021.07.01 21:00:25 ERROR 21/07/01 21:00:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)[0m
[0m2021.07.01 21:00:25 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:00:25 ERROR 21/07/01 21:00:25 INFO ResourceProfile: Limiting resource is cpu[0m
[0m2021.07.01 21:00:25 ERROR 21/07/01 21:00:25 INFO ResourceProfileManager: Added ResourceProfile id: 0[0m
[0m2021.07.01 21:00:25 ERROR 21/07/01 21:00:25 INFO SecurityManager: Changing view acls to: hwait[0m
[0m2021.07.01 21:00:25 ERROR 21/07/01 21:00:25 INFO SecurityManager: Changing modify acls to: hwait[0m
[0m2021.07.01 21:00:25 ERROR 21/07/01 21:00:25 INFO SecurityManager: Changing view acls groups to: [0m
[0m2021.07.01 21:00:25 ERROR 21/07/01 21:00:25 INFO SecurityManager: Changing modify acls groups to: [0m
[0m2021.07.01 21:00:25 ERROR 21/07/01 21:00:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hwait); groups with view permissions: Set(); users  with modify permissions: Set(hwait); groups with modify permissions: Set()[0m
[0m2021.07.01 21:00:25 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:00:25 ERROR 21/07/01 21:00:26 INFO Utils: Successfully started service 'sparkDriver' on port 46061.[0m
[0m2021.07.01 21:00:25 ERROR 21/07/01 21:00:26 INFO SparkEnv: Registering MapOutputTracker[0m
[0m2021.07.01 21:00:26 ERROR 21/07/01 21:00:26 INFO SparkEnv: Registering BlockManagerMaster[0m
[0m2021.07.01 21:00:26 ERROR 21/07/01 21:00:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information[0m
[0m2021.07.01 21:00:26 ERROR 21/07/01 21:00:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up[0m
[0m2021.07.01 21:00:26 ERROR 21/07/01 21:00:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat[0m
[0m2021.07.01 21:00:26 ERROR 21/07/01 21:00:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-18c9f658-e28a-4bae-8205-59318e126e4f[0m
[0m2021.07.01 21:00:26 ERROR 21/07/01 21:00:26 INFO MemoryStore: MemoryStore started with capacity 2.2 GiB[0m
[0m2021.07.01 21:00:26 ERROR 21/07/01 21:00:26 INFO SparkEnv: Registering OutputCommitCoordinator[0m
[0m2021.07.01 21:00:26 ERROR 21/07/01 21:00:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.[0m
[0m2021.07.01 21:00:26 ERROR 21/07/01 21:00:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.19.152.53:4040[0m
[0m2021.07.01 21:00:26 ERROR 21/07/01 21:00:26 INFO Executor: Starting executor ID driver on host 172.19.152.53[0m
[0m2021.07.01 21:00:26 ERROR 21/07/01 21:00:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38163.[0m
[0m2021.07.01 21:00:26 ERROR 21/07/01 21:00:26 INFO NettyBlockTransferService: Server created on 172.19.152.53:38163[0m
[0m2021.07.01 21:00:26 ERROR 21/07/01 21:00:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy[0m
[0m2021.07.01 21:00:26 ERROR 21/07/01 21:00:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.19.152.53, 38163, None)[0m
[0m2021.07.01 21:00:26 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:00:26 ERROR 21/07/01 21:00:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.152.53:38163 with 2.2 GiB RAM, BlockManagerId(driver, 172.19.152.53, 38163, None)[0m
[0m2021.07.01 21:00:26 ERROR 21/07/01 21:00:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.19.152.53, 38163, None)[0m
[0m2021.07.01 21:00:26 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:00:26 ERROR 21/07/01 21:00:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.19.152.53, 38163, None)[0m
[0m2021.07.01 21:00:26 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:00:27 ERROR 21/07/01 21:00:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hwait/dev/ht3/spark-warehouse').[0m
[0m2021.07.01 21:00:27 ERROR 21/07/01 21:00:27 INFO SharedState: Warehouse path is 'file:/home/hwait/dev/ht3/spark-warehouse'.[0m
[0m2021.07.01 21:00:27 ERROR Exception in thread "main" org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/hwait/dev/ht3/~/dev/data/sna[0m
[0m2021.07.01 21:00:27 ERROR 	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:803)[0m
[0m2021.07.01 21:00:27 ERROR 	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:800)[0m
[0m2021.07.01 21:00:27 ERROR 	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)[0m
[0m2021.07.01 21:00:27 ERROR 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)[0m
[0m2021.07.01 21:00:27 ERROR 	at scala.util.Success.$anonfun$map$1(Try.scala:255)[0m
[0m2021.07.01 21:00:27 ERROR 	at scala.util.Success.map(Try.scala:213)[0m
[0m2021.07.01 21:00:27 ERROR 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)[0m
[0m2021.07.01 21:00:27 ERROR 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)[0m
[0m2021.07.01 21:00:27 ERROR 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)[0m
[0m2021.07.01 21:00:27 ERROR 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)[0m
[0m2021.07.01 21:00:27 ERROR 	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)[0m
[0m2021.07.01 21:00:27 ERROR 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)[0m
[0m2021.07.01 21:00:27 ERROR 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)[0m
[0m2021.07.01 21:00:27 ERROR 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)[0m
[0m2021.07.01 21:00:27 ERROR 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)[0m
[0m2021.07.01 21:00:27 ERROR 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)[0m
[0m2021.07.01 21:00:27 ERROR 21/07/01 21:00:28 INFO SparkContext: Invoking stop() from shutdown hook[0m
[0m2021.07.01 21:00:27 ERROR 21/07/01 21:00:28 INFO SparkUI: Stopped Spark web UI at http://172.19.152.53:4040[0m
[0m2021.07.01 21:00:27 ERROR 21/07/01 21:00:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped![0m
[0m2021.07.01 21:00:27 ERROR 21/07/01 21:00:28 INFO MemoryStore: MemoryStore cleared[0m
[0m2021.07.01 21:00:27 ERROR 21/07/01 21:00:28 INFO BlockManager: BlockManager stopped[0m
[0m2021.07.01 21:00:27 ERROR 21/07/01 21:00:28 INFO BlockManagerMaster: BlockManagerMaster stopped[0m
[0m2021.07.01 21:00:27 ERROR 21/07/01 21:00:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped![0m
[0m2021.07.01 21:00:27 ERROR 21/07/01 21:00:28 INFO SparkContext: Successfully stopped SparkContext[0m
[0m2021.07.01 21:00:27 ERROR 21/07/01 21:00:28 INFO ShutdownHookManager: Shutdown hook called[0m
[0m2021.07.01 21:00:27 ERROR 21/07/01 21:00:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-c80b2b4d-850d-48be-b1af-aea0990b8b92[0m
[0m2021.07.01 21:00:29 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:00:29 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 21:01:04 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 7h 55m 13.347s)[0m
[0m2021.07.01 21:01:04 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 21:01:05 INFO  time: compiled ht3-test in 0.2s[0m
[0m2021.07.01 21:01:05 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 21:01:05 INFO  Listening for transport dt_socket at address: 38649[0m
[0m2021.07.01 21:01:05 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 21:01:05 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 21:01:05 INFO  Trying to attach to remote debuggee VM localhost:38649 .[0m
[0m2021.07.01 21:01:05 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 21:01:07 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 21:01:07 ERROR 21/07/01 21:01:07 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 21:01:07 ERROR 21/07/01 21:01:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 21:01:08 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 21:01:08 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 21:01:08 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 21:01:08 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 21:01:08 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 21:01:08 ERROR 21/07/01 21:01:08 INFO SparkContext: Running Spark version 3.1.1[0m
[0m2021.07.01 21:01:08 ERROR 21/07/01 21:01:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 21:01:08 ERROR 21/07/01 21:01:08 INFO ResourceUtils: ==============================================================[0m
[0m2021.07.01 21:01:08 ERROR 21/07/01 21:01:08 INFO ResourceUtils: No custom resources configured for spark.driver.[0m
[0m2021.07.01 21:01:08 ERROR 21/07/01 21:01:08 INFO ResourceUtils: ==============================================================[0m
[0m2021.07.01 21:01:08 ERROR 21/07/01 21:01:08 INFO SparkContext: Submitted application: Data Sources Practice[0m
[0m2021.07.01 21:01:08 ERROR 21/07/01 21:01:08 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)[0m
[0m2021.07.01 21:01:08 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:01:08 ERROR 21/07/01 21:01:08 INFO ResourceProfile: Limiting resource is cpu[0m
[0m2021.07.01 21:01:08 ERROR 21/07/01 21:01:08 INFO ResourceProfileManager: Added ResourceProfile id: 0[0m
[0m2021.07.01 21:01:08 ERROR 21/07/01 21:01:08 INFO SecurityManager: Changing view acls to: hwait[0m
[0m2021.07.01 21:01:08 ERROR 21/07/01 21:01:08 INFO SecurityManager: Changing modify acls to: hwait[0m
[0m2021.07.01 21:01:08 ERROR 21/07/01 21:01:08 INFO SecurityManager: Changing view acls groups to: [0m
[0m2021.07.01 21:01:08 ERROR 21/07/01 21:01:08 INFO SecurityManager: Changing modify acls groups to: [0m
[0m2021.07.01 21:01:08 ERROR 21/07/01 21:01:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hwait); groups with view permissions: Set(); users  with modify permissions: Set(hwait); groups with modify permissions: Set()[0m
[0m2021.07.01 21:01:08 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:01:08 ERROR 21/07/01 21:01:09 INFO Utils: Successfully started service 'sparkDriver' on port 34509.[0m
[0m2021.07.01 21:01:08 ERROR 21/07/01 21:01:09 INFO SparkEnv: Registering MapOutputTracker[0m
[0m2021.07.01 21:01:08 ERROR 21/07/01 21:01:09 INFO SparkEnv: Registering BlockManagerMaster[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e341d82f-3743-4500-b987-1eec85c09c84[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:09 INFO MemoryStore: MemoryStore started with capacity 2.2 GiB[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:09 INFO SparkEnv: Registering OutputCommitCoordinator[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:09 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.19.152.53:4040[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:09 INFO Executor: Starting executor ID driver on host 172.19.152.53[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38789.[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:09 INFO NettyBlockTransferService: Server created on 172.19.152.53:38789[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.19.152.53, 38789, None)[0m
[0m2021.07.01 21:01:09 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:09 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.152.53:38789 with 2.2 GiB RAM, BlockManagerId(driver, 172.19.152.53, 38789, None)[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.19.152.53, 38789, None)[0m
[0m2021.07.01 21:01:09 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.19.152.53, 38789, None)[0m
[0m2021.07.01 21:01:09 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hwait/dev/ht3/spark-warehouse').[0m
[0m2021.07.01 21:01:09 ERROR 21/07/01 21:01:10 INFO SharedState: Warehouse path is 'file:/home/hwait/dev/ht3/spark-warehouse'.[0m
[0m2021.07.01 21:01:10 ERROR Exception in thread "main" org.apache.spark.sql.AnalysisException: Path does not exist: file:/~/dev/data/sna[0m
[0m2021.07.01 21:01:10 ERROR 	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:803)[0m
[0m2021.07.01 21:01:10 ERROR 	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:800)[0m
[0m2021.07.01 21:01:10 ERROR 	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)[0m
[0m2021.07.01 21:01:10 ERROR 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)[0m
[0m2021.07.01 21:01:10 ERROR 	at scala.util.Success.$anonfun$map$1(Try.scala:255)[0m
[0m2021.07.01 21:01:10 ERROR 	at scala.util.Success.map(Try.scala:213)[0m
[0m2021.07.01 21:01:10 ERROR 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)[0m
[0m2021.07.01 21:01:10 ERROR 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)[0m
[0m2021.07.01 21:01:10 ERROR 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)[0m
[0m2021.07.01 21:01:10 ERROR 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)[0m
[0m2021.07.01 21:01:10 ERROR 	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)[0m
[0m2021.07.01 21:01:10 ERROR 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)[0m
[0m2021.07.01 21:01:10 ERROR 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)[0m
[0m2021.07.01 21:01:10 ERROR 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)[0m
[0m2021.07.01 21:01:10 ERROR 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)[0m
[0m2021.07.01 21:01:10 ERROR 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)[0m
[0m2021.07.01 21:01:10 ERROR 21/07/01 21:01:10 INFO SparkContext: Invoking stop() from shutdown hook[0m
[0m2021.07.01 21:01:10 ERROR 21/07/01 21:01:10 INFO SparkUI: Stopped Spark web UI at http://172.19.152.53:4040[0m
[0m2021.07.01 21:01:10 ERROR 21/07/01 21:01:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped![0m
[0m2021.07.01 21:01:10 ERROR 21/07/01 21:01:10 INFO MemoryStore: MemoryStore cleared[0m
[0m2021.07.01 21:01:10 ERROR 21/07/01 21:01:10 INFO BlockManager: BlockManager stopped[0m
[0m2021.07.01 21:01:10 ERROR 21/07/01 21:01:10 INFO BlockManagerMaster: BlockManagerMaster stopped[0m
[0m2021.07.01 21:01:10 ERROR 21/07/01 21:01:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped![0m
[0m2021.07.01 21:01:10 ERROR 21/07/01 21:01:10 INFO SparkContext: Successfully stopped SparkContext[0m
[0m2021.07.01 21:01:10 ERROR 21/07/01 21:01:10 INFO ShutdownHookManager: Shutdown hook called[0m
[0m2021.07.01 21:01:10 ERROR 21/07/01 21:01:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-969659b6-e816-4f86-be9b-a25e16d97a96[0m
[0m2021.07.01 21:01:11 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:01:11 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
Jul 01, 2021 9:01:41 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1579
[0m2021.07.01 21:01:54 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 7h 56m 3.566s)[0m
[0m2021.07.01 21:01:54 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 21:01:54 INFO  time: compiled ht3-test in 0.17s[0m
[0m2021.07.01 21:01:55 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 21:01:54 INFO  Listening for transport dt_socket at address: 36201[0m
[0m2021.07.01 21:01:55 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 21:01:55 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 21:01:54 INFO  Trying to attach to remote debuggee VM localhost:36201 .[0m
[0m2021.07.01 21:01:54 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 21:01:56 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 21:01:56 ERROR 21/07/01 21:01:56 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 21:01:56 ERROR 21/07/01 21:01:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 21:01:57 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 21:01:57 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 21:01:57 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 21:01:57 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 21:01:57 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:57 INFO SparkContext: Running Spark version 3.1.1[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO ResourceUtils: ==============================================================[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO ResourceUtils: No custom resources configured for spark.driver.[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO ResourceUtils: ==============================================================[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO SparkContext: Submitted application: Data Sources Practice[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)[0m
[0m2021.07.01 21:01:58 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO ResourceProfile: Limiting resource is cpu[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO ResourceProfileManager: Added ResourceProfile id: 0[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO SecurityManager: Changing view acls to: hwait[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO SecurityManager: Changing modify acls to: hwait[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO SecurityManager: Changing view acls groups to: [0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO SecurityManager: Changing modify acls groups to: [0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hwait); groups with view permissions: Set(); users  with modify permissions: Set(hwait); groups with modify permissions: Set()[0m
[0m2021.07.01 21:01:58 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO Utils: Successfully started service 'sparkDriver' on port 39051.[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO SparkEnv: Registering MapOutputTracker[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO SparkEnv: Registering BlockManagerMaster[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up[0m
[0m2021.07.01 21:01:57 ERROR 21/07/01 21:01:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat[0m
[0m2021.07.01 21:01:58 ERROR 21/07/01 21:01:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9e6c2055-9a1d-4520-8e56-cf04712d2751[0m
[0m2021.07.01 21:01:58 ERROR 21/07/01 21:01:58 INFO MemoryStore: MemoryStore started with capacity 2.2 GiB[0m
[0m2021.07.01 21:01:58 ERROR 21/07/01 21:01:58 INFO SparkEnv: Registering OutputCommitCoordinator[0m
[0m2021.07.01 21:01:58 ERROR 21/07/01 21:01:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.[0m
[0m2021.07.01 21:01:58 ERROR 21/07/01 21:01:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.19.152.53:4040[0m
[0m2021.07.01 21:01:58 ERROR 21/07/01 21:01:59 INFO Executor: Starting executor ID driver on host 172.19.152.53[0m
[0m2021.07.01 21:01:58 ERROR 21/07/01 21:01:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45613.[0m
[0m2021.07.01 21:01:58 ERROR 21/07/01 21:01:59 INFO NettyBlockTransferService: Server created on 172.19.152.53:45613[0m
[0m2021.07.01 21:01:58 ERROR 21/07/01 21:01:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy[0m
[0m2021.07.01 21:01:58 ERROR 21/07/01 21:01:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.19.152.53, 45613, None)[0m
[0m2021.07.01 21:01:59 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:01:58 ERROR 21/07/01 21:01:59 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.152.53:45613 with 2.2 GiB RAM, BlockManagerId(driver, 172.19.152.53, 45613, None)[0m
[0m2021.07.01 21:01:58 ERROR 21/07/01 21:01:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.19.152.53, 45613, None)[0m
[0m2021.07.01 21:01:59 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:01:58 ERROR 21/07/01 21:01:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.19.152.53, 45613, None)[0m
[0m2021.07.01 21:01:59 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:01:58 ERROR 21/07/01 21:01:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hwait/dev/ht3/spark-warehouse').[0m
[0m2021.07.01 21:01:58 ERROR 21/07/01 21:01:59 INFO SharedState: Warehouse path is 'file:/home/hwait/dev/ht3/spark-warehouse'.[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:00 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 48 paths. The first several paths are: file:/home/hwait/dev/data/sna/date=2018-02-06, file:/home/hwait/dev/data/sna/date=2018-02-24, file:/home/hwait/dev/data/sna/date=2018-02-23, file:/home/hwait/dev/data/sna/date=2018-03-01, file:/home/hwait/dev/data/sna/date=2018-03-21, file:/home/hwait/dev/data/sna/date=2018-03-20, file:/home/hwait/dev/data/sna/date=2018-03-09, file:/home/hwait/dev/data/sna/date=2018-03-04, file:/home/hwait/dev/data/sna/date=2018-03-15, file:/home/hwait/dev/data/sna/date=2018-03-10.[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:00 INFO SparkContext: Starting job: parquet at DataProvider.scala:30[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:00 INFO DAGScheduler: Got job 0 (parquet at DataProvider.scala:30) with 48 output partitions[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:00 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at DataProvider.scala:30)[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:00 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 21:02:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:00 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 21:02:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at parquet at DataProvider.scala:30), which has no missing parents[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 83.2 KiB, free 2.2 GiB)[0m
[0m2021.07.01 21:02:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 2.2 GiB)[0m
[0m2021.07.01 21:02:00 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.152.53:45613 (size: 29.5 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:01 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:01 INFO DAGScheduler: Submitting 48 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at parquet at DataProvider.scala:30) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:01 INFO TaskSchedulerImpl: Adding task set 0.0 with 48 tasks resource profile 0[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:01 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3221 bytes result sent to driver[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.19.152.53, executor driver, partition 1, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:01 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 251 ms on 172.19.152.53 (executor driver) (1/48)[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (172.19.152.53, executor driver, partition 2, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:01 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)[0m
[0m2021.07.01 21:02:00 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 22 ms on 172.19.152.53 (executor driver) (2/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (172.19.152.53, executor driver, partition 3, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 20 ms on 172.19.152.53 (executor driver) (3/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (172.19.152.53, executor driver, partition 4, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 22 ms on 172.19.152.53 (executor driver) (4/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (172.19.152.53, executor driver, partition 5, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 20 ms on 172.19.152.53 (executor driver) (5/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (172.19.152.53, executor driver, partition 6, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 22 ms on 172.19.152.53 (executor driver) (6/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (172.19.152.53, executor driver, partition 7, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 25 ms on 172.19.152.53 (executor driver) (7/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (172.19.152.53, executor driver, partition 8, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 18 ms on 172.19.152.53 (executor driver) (8/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (172.19.152.53, executor driver, partition 9, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 20 ms on 172.19.152.53 (executor driver) (9/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (172.19.152.53, executor driver, partition 10, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 16 ms on 172.19.152.53 (executor driver) (10/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (172.19.152.53, executor driver, partition 11, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 20 ms on 172.19.152.53 (executor driver) (11/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (172.19.152.53, executor driver, partition 12, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 15 ms on 172.19.152.53 (executor driver) (12/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (172.19.152.53, executor driver, partition 13, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 14 ms on 172.19.152.53 (executor driver) (13/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (172.19.152.53, executor driver, partition 14, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 15 ms on 172.19.152.53 (executor driver) (14/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (172.19.152.53, executor driver, partition 15, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 17 ms on 172.19.152.53 (executor driver) (15/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (172.19.152.53, executor driver, partition 16, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 17 ms on 172.19.152.53 (executor driver) (16/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (172.19.152.53, executor driver, partition 17, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 16 ms on 172.19.152.53 (executor driver) (17/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (172.19.152.53, executor driver, partition 18, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 12 ms on 172.19.152.53 (executor driver) (18/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (172.19.152.53, executor driver, partition 19, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 12 ms on 172.19.152.53 (executor driver) (19/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (172.19.152.53, executor driver, partition 20, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 14 ms on 172.19.152.53 (executor driver) (20/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (172.19.152.53, executor driver, partition 21, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 12 ms on 172.19.152.53 (executor driver) (21/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (172.19.152.53, executor driver, partition 22, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 12 ms on 172.19.152.53 (executor driver) (22/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (172.19.152.53, executor driver, partition 23, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 12 ms on 172.19.152.53 (executor driver) (23/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (172.19.152.53, executor driver, partition 24, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 11 ms on 172.19.152.53 (executor driver) (24/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (172.19.152.53, executor driver, partition 25, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 11 ms on 172.19.152.53 (executor driver) (25/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (172.19.152.53, executor driver, partition 26, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 12 ms on 172.19.152.53 (executor driver) (26/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (172.19.152.53, executor driver, partition 27, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 11 ms on 172.19.152.53 (executor driver) (27/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (172.19.152.53, executor driver, partition 28, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 11 ms on 172.19.152.53 (executor driver) (28/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (172.19.152.53, executor driver, partition 29, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 12 ms on 172.19.152.53 (executor driver) (29/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (172.19.152.53, executor driver, partition 30, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 13 ms on 172.19.152.53 (executor driver) (30/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (172.19.152.53, executor driver, partition 31, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 15 ms on 172.19.152.53 (executor driver) (31/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (172.19.152.53, executor driver, partition 32, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 16 ms on 172.19.152.53 (executor driver) (32/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (172.19.152.53, executor driver, partition 33, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 13 ms on 172.19.152.53 (executor driver) (33/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (172.19.152.53, executor driver, partition 34, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 12 ms on 172.19.152.53 (executor driver) (34/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (172.19.152.53, executor driver, partition 35, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 11 ms on 172.19.152.53 (executor driver) (35/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (172.19.152.53, executor driver, partition 36, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 14 ms on 172.19.152.53 (executor driver) (36/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (172.19.152.53, executor driver, partition 37, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 15 ms on 172.19.152.53 (executor driver) (37/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (172.19.152.53, executor driver, partition 38, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 14 ms on 172.19.152.53 (executor driver) (38/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (172.19.152.53, executor driver, partition 39, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 12 ms on 172.19.152.53 (executor driver) (39/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (172.19.152.53, executor driver, partition 40, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 15 ms on 172.19.152.53 (executor driver) (40/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (172.19.152.53, executor driver, partition 41, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 11 ms on 172.19.152.53 (executor driver) (41/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 3178 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (172.19.152.53, executor driver, partition 42, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 23 ms on 172.19.152.53 (executor driver) (42/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (172.19.152.53, executor driver, partition 43, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 12 ms on 172.19.152.53 (executor driver) (43/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (172.19.152.53, executor driver, partition 44, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 11 ms on 172.19.152.53 (executor driver) (44/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (172.19.152.53, executor driver, partition 45, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:01 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 14 ms on 172.19.152.53 (executor driver) (45/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (172.19.152.53, executor driver, partition 46, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 12 ms on 172.19.152.53 (executor driver) (46/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (172.19.152.53, executor driver, partition 47, PROCESS_LOCAL, 4525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 13 ms on 172.19.152.53 (executor driver) (47/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 3135 bytes result sent to driver[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 15 ms on 172.19.152.53 (executor driver) (48/48)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO DAGScheduler: ResultStage 0 (parquet at DataProvider.scala:30) finished in 1.102 s[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO DAGScheduler: Job 0 finished: parquet at DataProvider.scala:30, took 1.168363 s[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO InMemoryFileIndex: It took 1688 ms to list leaf files for 1 paths.[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO SparkContext: Starting job: parquet at DataProvider.scala:30[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO DAGScheduler: Got job 1 (parquet at DataProvider.scala:30) with 1 output partitions[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at DataProvider.scala:30)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 21:02:02 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at parquet at DataProvider.scala:30), which has no missing parents[0m
[0m2021.07.01 21:02:02 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 83.4 KiB, free 2.2 GiB)[0m
[0m2021.07.01 21:02:02 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 29.7 KiB, free 2.2 GiB)[0m
[0m2021.07.01 21:02:02 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.152.53:45613 (size: 29.7 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at parquet at DataProvider.scala:30) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 48) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 4685 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:01 ERROR 21/07/01 21:02:02 INFO Executor: Running task 0.0 in stage 1.0 (TID 48)[0m
[0m2021.07.01 21:02:02 ERROR 21/07/01 21:02:02 INFO Executor: Finished task 0.0 in stage 1.0 (TID 48). 11587 bytes result sent to driver[0m
[0m2021.07.01 21:02:02 ERROR 21/07/01 21:02:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 48) in 305 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 21:02:02 ERROR 21/07/01 21:02:02 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 21:02:02 ERROR 21/07/01 21:02:02 INFO DAGScheduler: ResultStage 1 (parquet at DataProvider.scala:30) finished in 0.330 s[0m
[0m2021.07.01 21:02:02 ERROR 21/07/01 21:02:02 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 21:02:02 ERROR 21/07/01 21:02:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished[0m
[0m2021.07.01 21:02:02 ERROR 21/07/01 21:02:02 INFO DAGScheduler: Job 1 finished: parquet at DataProvider.scala:30, took 0.335737 s[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:04 INFO DataSourceStrategy: Pruning directories with: [0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:04 INFO FileSourceStrategy: Pushed Filters: [0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:04 INFO FileSourceStrategy: Post-Scan Filters: [0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:04 INFO FileSourceStrategy: Output Data Schema: struct<>[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:04 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.19.152.53:45613 in memory (size: 29.5 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.152.53:45613 in memory (size: 29.7 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO CodeGenerator: Code generated in 203.7693 ms[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO CodeGenerator: Code generated in 25.0514 ms[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 175.3 KiB, free 2.2 GiB)[0m
[0m2021.07.01 21:02:05 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.9 KiB, free 2.2 GiB)[0m
[0m2021.07.01 21:02:05 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.152.53:45613 (size: 27.9 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO SparkContext: Created broadcast 2 from count at DataProvider.scala:31[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO SparkContext: Starting job: count at DataProvider.scala:31[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO DAGScheduler: Registering RDD 8 (count at DataProvider.scala:31) as input to shuffle 0[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO DAGScheduler: Got job 2 (count at DataProvider.scala:31) with 1 output partitions[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO DAGScheduler: Final stage: ResultStage 3 (count at DataProvider.scala:31)[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)[0m
[0m2021.07.01 21:02:05 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[8] at count at DataProvider.scala:31), which has no missing parents[0m
[0m2021.07.01 21:02:05 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.4 KiB, free 2.2 GiB)[0m
[0m2021.07.01 21:02:05 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 2.2 GiB)[0m
[0m2021.07.01 21:02:05 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.19.152.53:45613 (size: 7.1 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO DAGScheduler: Submitting 28 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[8] at count at DataProvider.scala:31) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO TaskSchedulerImpl: Adding task set 2.0 with 28 tasks resource profile 0[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 49) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO Executor: Running task 0.0 in stage 2.0 (TID 49)[0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-04/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7456154, partition values: [17594][0m
[0m2021.07.01 21:02:04 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-04/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7451790, partition values: [17594][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-11/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7420267, partition values: [17601][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-11/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7419692, partition values: [17601][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-25/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7418835, partition values: [17587][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-04/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7416989, partition values: [17594][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-25/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7415194, partition values: [17587][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-04/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7394827, partition values: [17594][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-25/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7393469, partition values: [17587][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-11/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7368173, partition values: [17601][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-11/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7362038, partition values: [17601][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO Executor: Finished task 0.0 in stage 2.0 (TID 49). 2223 bytes result sent to driver[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 50) (172.19.152.53, executor driver, partition 1, PROCESS_LOCAL, 6525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO Executor: Running task 1.0 in stage 2.0 (TID 50)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 49) in 403 ms on 172.19.152.53 (executor driver) (1/28)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-25/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7354198, partition values: [17587][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-04/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7341290, partition values: [17594][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-11/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7337771, partition values: [17601][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-04/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7334046, partition values: [17594][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-25/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7329070, partition values: [17587][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-04/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7320564, partition values: [17566][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-04/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7320072, partition values: [17594][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-04/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7319772, partition values: [17566][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-25/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7310957, partition values: [17587][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-11/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7309271, partition values: [17601][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-11/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7308810, partition values: [17601][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-04/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7303278, partition values: [17566][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO Executor: Finished task 1.0 in stage 2.0 (TID 50). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 51) (172.19.152.53, executor driver, partition 2, PROCESS_LOCAL, 6525 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 50) in 118 ms on 172.19.152.53 (executor driver) (2/28)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO Executor: Running task 2.0 in stage 2.0 (TID 51)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-04/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7301122, partition values: [17566][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-25/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7283811, partition values: [17587][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-04/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7272836, partition values: [17566][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-04/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7239793, partition values: [17566][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-04/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7223535, partition values: [17566][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-18/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7207852, partition values: [17608][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-18/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7202779, partition values: [17608][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-18/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7179969, partition values: [17608][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-18/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7177129, partition values: [17608][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-18/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7167161, partition values: [17608][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-18/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7146213, partition values: [17608][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-10/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7140670, partition values: [17600][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO Executor: Finished task 2.0 in stage 2.0 (TID 51). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 52) (172.19.152.53, executor driver, partition 3, PROCESS_LOCAL, 6546 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 51) in 113 ms on 172.19.152.53 (executor driver) (3/28)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO Executor: Running task 3.0 in stage 2.0 (TID 52)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-18/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7125678, partition values: [17608][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-10/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7063846, partition values: [17600][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-09/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7045345, partition values: [17599][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-10/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7044976, partition values: [17600][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-10/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7038655, partition values: [17600][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-10/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7015787, partition values: [17600][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-10/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7014166, partition values: [17600][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-03/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7006653, partition values: [17593][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-10/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6988002, partition values: [17600][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-03/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6986846, partition values: [17593][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-05/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6979466, partition values: [17595][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-09/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6978784, partition values: [17599][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO Executor: Finished task 3.0 in stage 2.0 (TID 52). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 53) (172.19.152.53, executor driver, partition 4, PROCESS_LOCAL, 6588 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 52) in 102 ms on 172.19.152.53 (executor driver) (4/28)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO Executor: Running task 4.0 in stage 2.0 (TID 53)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-19/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6972514, partition values: [17609][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-03/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6972245, partition values: [17593][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-05/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6970868, partition values: [17567][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-03/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6949243, partition values: [17593][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-09/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6943858, partition values: [17599][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-24/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6942366, partition values: [17586][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-06/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6940365, partition values: [17568][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-05/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6940097, partition values: [17567][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-12/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6939902, partition values: [17574][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-12/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6939478, partition values: [17574][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-09/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6935669, partition values: [17599][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-06/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6933542, partition values: [17568][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO Executor: Finished task 4.0 in stage 2.0 (TID 53). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 54) (172.19.152.53, executor driver, partition 5, PROCESS_LOCAL, 6630 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 53) in 92 ms on 172.19.152.53 (executor driver) (5/28)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO Executor: Running task 5.0 in stage 2.0 (TID 54)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-09/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6933049, partition values: [17599][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-24/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6932369, partition values: [17586][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-09/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6931828, partition values: [17599][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-06/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6927511, partition values: [17568][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-02/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6924830, partition values: [17592][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-03/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6922102, partition values: [17593][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-12/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6918887, partition values: [17574][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-05/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6918156, partition values: [17567][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-19/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6918143, partition values: [17609][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-05/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6911939, partition values: [17595][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-03/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6910736, partition values: [17593][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-24/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6908366, partition values: [17586][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO Executor: Finished task 5.0 in stage 2.0 (TID 54). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 55) (172.19.152.53, executor driver, partition 6, PROCESS_LOCAL, 6588 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO Executor: Running task 6.0 in stage 2.0 (TID 55)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 54) in 80 ms on 172.19.152.53 (executor driver) (6/28)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-12/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6907851, partition values: [17574][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-05/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6907368, partition values: [17567][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-02/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6903766, partition values: [17592][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-06/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6902309, partition values: [17568][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-05/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6901867, partition values: [17567][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-08/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6900687, partition values: [17570][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-22/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6897864, partition values: [17584][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-19/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6897403, partition values: [17609][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-05/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6895782, partition values: [17567][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-05/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6895573, partition values: [17567][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-19/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6894595, partition values: [17609][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-19/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6891010, partition values: [17609][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO Executor: Finished task 6.0 in stage 2.0 (TID 55). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 56) (172.19.152.53, executor driver, partition 7, PROCESS_LOCAL, 6567 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 55) in 82 ms on 172.19.152.53 (executor driver) (7/28)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO Executor: Running task 7.0 in stage 2.0 (TID 56)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-05/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6889416, partition values: [17595][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-24/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6888532, partition values: [17586][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-20/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6885279, partition values: [17582][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-12/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6882520, partition values: [17574][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-06/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6880957, partition values: [17568][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-24/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6880772, partition values: [17586][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-05/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6879283, partition values: [17595][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-05/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6878834, partition values: [17595][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-07/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6876768, partition values: [17569][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-12/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6875178, partition values: [17574][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-06/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6875120, partition values: [17568][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-24/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6874570, partition values: [17586][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO Executor: Finished task 7.0 in stage 2.0 (TID 56). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 57) (172.19.152.53, executor driver, partition 8, PROCESS_LOCAL, 6651 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 56) in 90 ms on 172.19.152.53 (executor driver) (8/28)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO Executor: Running task 8.0 in stage 2.0 (TID 57)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-12/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6874223, partition values: [17602][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-14/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6871075, partition values: [17604][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-03/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6868911, partition values: [17593][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-05/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6860868, partition values: [17595][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-09/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6858520, partition values: [17599][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-02/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6856628, partition values: [17592][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-20/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6856385, partition values: [17582][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-02/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6856052, partition values: [17592][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-22/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6855704, partition values: [17584][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-20/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6853855, partition values: [17582][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-19/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6852664, partition values: [17609][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-06/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6852114, partition values: [17596][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO Executor: Finished task 8.0 in stage 2.0 (TID 57). 2180 bytes result sent to driver[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 58) (172.19.152.53, executor driver, partition 9, PROCESS_LOCAL, 6630 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 57) in 83 ms on 172.19.152.53 (executor driver) (9/28)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO Executor: Running task 9.0 in stage 2.0 (TID 58)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-06/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6849375, partition values: [17568][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-24/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6848835, partition values: [17586][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-02/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6848764, partition values: [17592][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-22/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6848603, partition values: [17584][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-12/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6847438, partition values: [17574][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-15/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6846527, partition values: [17577][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-17/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6844768, partition values: [17607][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-15/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6837655, partition values: [17577][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-12/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6837098, partition values: [17602][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-07/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6836106, partition values: [17569][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-15/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6834227, partition values: [17577][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-07/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6828564, partition values: [17569][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO Executor: Finished task 9.0 in stage 2.0 (TID 58). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 59) (172.19.152.53, executor driver, partition 10, PROCESS_LOCAL, 6609 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 58) in 65 ms on 172.19.152.53 (executor driver) (10/28)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO Executor: Running task 10.0 in stage 2.0 (TID 59)[0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-10/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6823720, partition values: [17572][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-08/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6820388, partition values: [17570][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-19/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6820099, partition values: [17609][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-15/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6819490, partition values: [17577][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-20/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6812204, partition values: [17582][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-12/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6812029, partition values: [17602][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-20/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6811381, partition values: [17582][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-02/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6810799, partition values: [17592][0m
[0m2021.07.01 21:02:05 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-08/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6808177, partition values: [17570][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-02/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6807660, partition values: [17592][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-15/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6805608, partition values: [17577][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-07/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6804572, partition values: [17597][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO Executor: Finished task 10.0 in stage 2.0 (TID 59). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 60) (172.19.152.53, executor driver, partition 11, PROCESS_LOCAL, 6609 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 59) in 78 ms on 172.19.152.53 (executor driver) (11/28)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO Executor: Running task 11.0 in stage 2.0 (TID 60)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-01/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6804116, partition values: [17591][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-20/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6803037, partition values: [17582][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-06/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6802535, partition values: [17596][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-06/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6801716, partition values: [17596][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-15/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6801242, partition values: [17577][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-22/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6800112, partition values: [17584][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-08/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6798023, partition values: [17570][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-22/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6795902, partition values: [17584][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-10/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6795157, partition values: [17572][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-22/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6794911, partition values: [17584][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-20/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6791322, partition values: [17582][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-12/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6790906, partition values: [17602][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO Executor: Finished task 11.0 in stage 2.0 (TID 60). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 61) (172.19.152.53, executor driver, partition 12, PROCESS_LOCAL, 6651 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 60) in 72 ms on 172.19.152.53 (executor driver) (12/28)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO Executor: Running task 12.0 in stage 2.0 (TID 61)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-08/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6790452, partition values: [17570][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-12/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6790183, partition values: [17602][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-06/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6789659, partition values: [17596][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-17/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6788642, partition values: [17607][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-12/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6787867, partition values: [17602][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-21/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6787299, partition values: [17583][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-07/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6787222, partition values: [17569][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-22/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6786363, partition values: [17584][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-17/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6785842, partition values: [17607][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-05/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6784882, partition values: [17595][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-14/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6784303, partition values: [17604][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-14/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6783191, partition values: [17576][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO Executor: Finished task 12.0 in stage 2.0 (TID 61). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Starting task 13.0 in stage 2.0 (TID 62) (172.19.152.53, executor driver, partition 13, PROCESS_LOCAL, 6630 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 61) in 79 ms on 172.19.152.53 (executor driver) (13/28)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO Executor: Running task 13.0 in stage 2.0 (TID 62)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-14/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6782969, partition values: [17604][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-15/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6782543, partition values: [17577][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-14/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6777283, partition values: [17576][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-07/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6777235, partition values: [17597][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-17/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6776594, partition values: [17607][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-07/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6776471, partition values: [17569][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-06/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6774586, partition values: [17596][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-03/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6773317, partition values: [17565][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-07/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6770727, partition values: [17569][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-14/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6770353, partition values: [17604][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-08/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6769134, partition values: [17570][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-14/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6767041, partition values: [17576][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO Executor: Finished task 13.0 in stage 2.0 (TID 62). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Starting task 14.0 in stage 2.0 (TID 63) (172.19.152.53, executor driver, partition 14, PROCESS_LOCAL, 6630 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO Executor: Running task 14.0 in stage 2.0 (TID 63)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Finished task 13.0 in stage 2.0 (TID 62) in 76 ms on 172.19.152.53 (executor driver) (14/28)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-01/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6766977, partition values: [17591][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-01/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6765130, partition values: [17591][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-01/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6764160, partition values: [17591][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-14/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6763365, partition values: [17604][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-03/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6762180, partition values: [17565][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-13/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6759894, partition values: [17603][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-21/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6759882, partition values: [17583][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-06/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6757416, partition values: [17596][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-12/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6757143, partition values: [17602][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-14/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6755271, partition values: [17604][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-07/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6752827, partition values: [17569][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-10/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6752521, partition values: [17572][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO Executor: Finished task 14.0 in stage 2.0 (TID 63). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Starting task 15.0 in stage 2.0 (TID 64) (172.19.152.53, executor driver, partition 15, PROCESS_LOCAL, 6609 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO Executor: Running task 15.0 in stage 2.0 (TID 64)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO TaskSetManager: Finished task 14.0 in stage 2.0 (TID 63) in 73 ms on 172.19.152.53 (executor driver) (15/28)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-14/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6750873, partition values: [17604][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-21/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6746025, partition values: [17583][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-10/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6745720, partition values: [17572][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-23/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6745610, partition values: [17585][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-14/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6745370, partition values: [17576][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-17/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6742757, partition values: [17607][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-08/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6742216, partition values: [17570][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-21/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6741271, partition values: [17583][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-13/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6740613, partition values: [17603][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-14/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6739077, partition values: [17576][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-13/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6737775, partition values: [17603][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-21/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6736002, partition values: [17583][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Finished task 15.0 in stage 2.0 (TID 64). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Starting task 16.0 in stage 2.0 (TID 65) (172.19.152.53, executor driver, partition 16, PROCESS_LOCAL, 6630 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Running task 16.0 in stage 2.0 (TID 65)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Finished task 15.0 in stage 2.0 (TID 64) in 71 ms on 172.19.152.53 (executor driver) (16/28)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-06/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6735999, partition values: [17596][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-17/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6735861, partition values: [17607][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-13/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6735748, partition values: [17603][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-08/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6733619, partition values: [17598][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-21/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6731199, partition values: [17583][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-13/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6728914, partition values: [17603][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-07/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6726030, partition values: [17597][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-16/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6723880, partition values: [17578][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-15/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6723171, partition values: [17605][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-14/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6721977, partition values: [17576][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-07/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6718134, partition values: [17597][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-13/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6717956, partition values: [17603][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Finished task 16.0 in stage 2.0 (TID 65). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Starting task 17.0 in stage 2.0 (TID 66) (172.19.152.53, executor driver, partition 17, PROCESS_LOCAL, 6609 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Finished task 16.0 in stage 2.0 (TID 65) in 72 ms on 172.19.152.53 (executor driver) (17/28)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Running task 17.0 in stage 2.0 (TID 66)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-14/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6714862, partition values: [17576][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-16/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6712406, partition values: [17578][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-23/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6711586, partition values: [17585][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-23/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6710909, partition values: [17585][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-10/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6707717, partition values: [17572][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-10/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6706966, partition values: [17572][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-09/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6706629, partition values: [17571][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-01/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6703505, partition values: [17591][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-13/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6701121, partition values: [17575][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-23/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6699525, partition values: [17585][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-09/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6699472, partition values: [17571][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-17/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6698346, partition values: [17607][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Finished task 17.0 in stage 2.0 (TID 66). 2180 bytes result sent to driver[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Starting task 18.0 in stage 2.0 (TID 67) (172.19.152.53, executor driver, partition 18, PROCESS_LOCAL, 6651 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Running task 18.0 in stage 2.0 (TID 67)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Finished task 17.0 in stage 2.0 (TID 66) in 81 ms on 172.19.152.53 (executor driver) (18/28)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-13/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6696669, partition values: [17575][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-01/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6695909, partition values: [17591][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-03/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6692407, partition values: [17565][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-07/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6692115, partition values: [17597][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-09/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6687853, partition values: [17571][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-07/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6687562, partition values: [17597][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-16/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6685879, partition values: [17578][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-10/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6682741, partition values: [17572][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-20/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6682178, partition values: [17610][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-13/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6679085, partition values: [17575][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-21/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6678664, partition values: [17583][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-26/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6676699, partition values: [17588][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Finished task 18.0 in stage 2.0 (TID 67). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Starting task 19.0 in stage 2.0 (TID 68) (172.19.152.53, executor driver, partition 19, PROCESS_LOCAL, 6609 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Finished task 18.0 in stage 2.0 (TID 67) in 68 ms on 172.19.152.53 (executor driver) (19/28)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Running task 19.0 in stage 2.0 (TID 68)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-23/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6673018, partition values: [17585][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-15/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6672395, partition values: [17605][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-03/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6670560, partition values: [17565][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-07/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6668923, partition values: [17597][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-13/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6667977, partition values: [17603][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-23/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6667320, partition values: [17585][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-20/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6666072, partition values: [17610][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-20/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6665806, partition values: [17610][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-01/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6664062, partition values: [17591][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-08/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6658253, partition values: [17598][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-15/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6656819, partition values: [17605][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-23/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6653944, partition values: [17585][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Finished task 19.0 in stage 2.0 (TID 68). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Starting task 20.0 in stage 2.0 (TID 69) (172.19.152.53, executor driver, partition 20, PROCESS_LOCAL, 6567 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Finished task 19.0 in stage 2.0 (TID 68) in 68 ms on 172.19.152.53 (executor driver) (20/28)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Running task 20.0 in stage 2.0 (TID 69)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-09/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6653532, partition values: [17571][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-15/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6653508, partition values: [17605][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-03/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6652148, partition values: [17565][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-20/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6649512, partition values: [17610][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-15/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6649145, partition values: [17605][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-16/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6648242, partition values: [17578][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-16/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6648215, partition values: [17578][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-09/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6648141, partition values: [17571][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-09/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6648003, partition values: [17571][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-20/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6637721, partition values: [17610][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-08/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6636527, partition values: [17598][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-15/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6636455, partition values: [17605][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Finished task 20.0 in stage 2.0 (TID 69). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Starting task 21.0 in stage 2.0 (TID 70) (172.19.152.53, executor driver, partition 21, PROCESS_LOCAL, 6588 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Finished task 20.0 in stage 2.0 (TID 69) in 69 ms on 172.19.152.53 (executor driver) (21/28)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Running task 21.0 in stage 2.0 (TID 70)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-20/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6635364, partition values: [17610][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-03/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6627974, partition values: [17565][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-08/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6626197, partition values: [17598][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-09/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6622287, partition values: [17571][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-20/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6620982, partition values: [17610][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-26/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6617371, partition values: [17588][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-26/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6617265, partition values: [17588][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-08/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6615677, partition values: [17598][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-13/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6615030, partition values: [17575][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-26/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6612710, partition values: [17588][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-16/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6612134, partition values: [17578][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-13/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6610814, partition values: [17575][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Finished task 21.0 in stage 2.0 (TID 70). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Starting task 22.0 in stage 2.0 (TID 71) (172.19.152.53, executor driver, partition 22, PROCESS_LOCAL, 6609 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Finished task 21.0 in stage 2.0 (TID 70) in 65 ms on 172.19.152.53 (executor driver) (22/28)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Running task 22.0 in stage 2.0 (TID 71)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-08/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6604473, partition values: [17598][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-03/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6603351, partition values: [17565][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-26/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6597400, partition values: [17588][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-08/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6596926, partition values: [17598][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-16/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6586765, partition values: [17578][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-15/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6578804, partition values: [17605][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-26/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6567848, partition values: [17588][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-26/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6560094, partition values: [17588][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-13/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6558040, partition values: [17575][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-27/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6541570, partition values: [17589][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-28/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6533569, partition values: [17590][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-27/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6530209, partition values: [17589][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Finished task 22.0 in stage 2.0 (TID 71). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Starting task 23.0 in stage 2.0 (TID 72) (172.19.152.53, executor driver, partition 23, PROCESS_LOCAL, 6546 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Finished task 22.0 in stage 2.0 (TID 71) in 60 ms on 172.19.152.53 (executor driver) (23/28)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Running task 23.0 in stage 2.0 (TID 72)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-27/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6527807, partition values: [17589][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-27/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6523776, partition values: [17589][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-13/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6523100, partition values: [17575][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-28/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6510770, partition values: [17590][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-27/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6508341, partition values: [17589][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-28/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6502224, partition values: [17590][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-16/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6500562, partition values: [17606][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-16/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6499813, partition values: [17606][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-28/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6494381, partition values: [17590][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-16/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6480037, partition values: [17606][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-16/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6479666, partition values: [17606][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-21/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6472674, partition values: [17611][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Finished task 23.0 in stage 2.0 (TID 72). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Starting task 24.0 in stage 2.0 (TID 73) (172.19.152.53, executor driver, partition 24, PROCESS_LOCAL, 6685 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Finished task 23.0 in stage 2.0 (TID 72) in 61 ms on 172.19.152.53 (executor driver) (24/28)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Running task 24.0 in stage 2.0 (TID 73)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-16/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6471548, partition values: [17606][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-28/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6463604, partition values: [17590][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-21/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6461000, partition values: [17611][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-27/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6454027, partition values: [17589][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-27/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6443947, partition values: [17589][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-28/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6428282, partition values: [17590][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-16/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6424883, partition values: [17606][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-21/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6424324, partition values: [17611][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-16/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6421216, partition values: [17606][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-02/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6414201, partition values: [17564][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-21/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6409231, partition values: [17611][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-28/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6405724, partition values: [17590][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-21/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6404453, partition values: [17611][0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Finished task 24.0 in stage 2.0 (TID 73). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Starting task 25.0 in stage 2.0 (TID 74) (172.19.152.53, executor driver, partition 25, PROCESS_LOCAL, 6643 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Finished task 24.0 in stage 2.0 (TID 73) in 76 ms on 172.19.152.53 (executor driver) (25/28)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO Executor: Running task 25.0 in stage 2.0 (TID 74)[0m
[0m2021.07.01 21:02:06 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-02/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6399672, partition values: [17564][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-02/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6397595, partition values: [17564][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-21/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6388265, partition values: [17611][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-21/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6361687, partition values: [17611][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-02/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6357107, partition values: [17564][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-02/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6352970, partition values: [17564][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-01/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6349917, partition values: [17563][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-02/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6328709, partition values: [17564][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-02/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6322068, partition values: [17564][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-01/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6282002, partition values: [17563][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-01/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6274704, partition values: [17563][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-01/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6248906, partition values: [17563][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-01/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6216083, partition values: [17563][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO Executor: Finished task 25.0 in stage 2.0 (TID 74). 2180 bytes result sent to driver[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Starting task 26.0 in stage 2.0 (TID 75) (172.19.152.53, executor driver, partition 26, PROCESS_LOCAL, 6643 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Finished task 25.0 in stage 2.0 (TID 74) in 87 ms on 172.19.152.53 (executor driver) (26/28)[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO Executor: Running task 26.0 in stage 2.0 (TID 75)[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-01/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6208668, partition values: [17563][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-01/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6196347, partition values: [17563][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-19/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6136934, partition values: [17581][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-19/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6115531, partition values: [17581][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-19/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6109541, partition values: [17581][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-19/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6069368, partition values: [17581][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-19/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6049265, partition values: [17581][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-19/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6020665, partition values: [17581][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-19/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-5995242, partition values: [17581][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-18/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-5377021, partition values: [17580][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-18/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-5374749, partition values: [17580][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-18/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-5372496, partition values: [17580][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-18/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-5346986, partition values: [17580][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO Executor: Finished task 26.0 in stage 2.0 (TID 75). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Starting task 27.0 in stage 2.0 (TID 76) (172.19.152.53, executor driver, partition 27, PROCESS_LOCAL, 6205 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Finished task 26.0 in stage 2.0 (TID 75) in 66 ms on 172.19.152.53 (executor driver) (27/28)[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO Executor: Running task 27.0 in stage 2.0 (TID 76)[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-18/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-5342085, partition values: [17580][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-18/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-5332496, partition values: [17580][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-18/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-5319023, partition values: [17580][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-17/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-3605420, partition values: [17579][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-17/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-3603903, partition values: [17579][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-17/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-3595887, partition values: [17579][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-17/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-3590788, partition values: [17579][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-17/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-3582062, partition values: [17579][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-17/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-3559447, partition values: [17579][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-17/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-3537910, partition values: [17579][0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO Executor: Finished task 27.0 in stage 2.0 (TID 76). 2137 bytes result sent to driver[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Finished task 27.0 in stage 2.0 (TID 76) in 53 ms on 172.19.152.53 (executor driver) (28/28)[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO DAGScheduler: ShuffleMapStage 2 (count at DataProvider.scala:31) finished in 2.513 s[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO DAGScheduler: looking for newly runnable stages[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO DAGScheduler: running: Set()[0m
[0m2021.07.01 21:02:07 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO DAGScheduler: waiting: Set(ResultStage 3)[0m
[0m2021.07.01 21:02:07 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO DAGScheduler: failed: Set()[0m
[0m2021.07.01 21:02:07 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at count at DataProvider.scala:31), which has no missing parents[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 10.1 KiB, free 2.2 GiB)[0m
[0m2021.07.01 21:02:07 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 2.2 GiB)[0m
[0m2021.07.01 21:02:07 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.19.152.53:45613 (size: 5.0 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at count at DataProvider.scala:31) (first 15 tasks are for partitions Vector(0))[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 77) (172.19.152.53, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO Executor: Running task 0.0 in stage 3.0 (TID 77)[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO ShuffleBlockFetcherIterator: Getting 28 (1680.0 B) non-empty blocks including 28 (1680.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks[0m
[0m2021.07.01 21:02:07 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO Executor: Finished task 0.0 in stage 3.0 (TID 77). 2648 bytes result sent to driver[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 77) in 67 ms on 172.19.152.53 (executor driver) (1/1)[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO DAGScheduler: ResultStage 3 (count at DataProvider.scala:31) finished in 0.077 s[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished[0m
[0m2021.07.01 21:02:07 ERROR 21/07/01 21:02:07 INFO DAGScheduler: Job 2 finished: count at DataProvider.scala:31, took 2.632856 s[0m
[0m2021.07.01 21:02:07 INFO  Data shape=(1.8286575E7, 169)[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO DataSourceStrategy: Pruning directories with: [0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO FileSourceStrategy: Pushed Filters: [0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO FileSourceStrategy: Post-Scan Filters: [0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO FileSourceStrategy: Output Data Schema: struct<audit_timestamp: bigint, metadata_ownerId: int, metadata_createdAt: bigint, feedback: array<string> ... 2 more fields>[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO CodeGenerator: Code generated in 15.5818 ms[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO CodeGenerator: Code generated in 50.7209 ms[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 176.0 KiB, free 2.2 GiB)[0m
[0m2021.07.01 21:02:07 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 28.3 KiB, free 2.2 GiB)[0m
[0m2021.07.01 21:02:07 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.19.152.53:45613 (size: 28.3 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO SparkContext: Created broadcast 5 from show at DataProvider.scala:47[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO SparkContext: Starting job: show at DataProvider.scala:47[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO DAGScheduler: Got job 3 (show at DataProvider.scala:47) with 28 output partitions[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO DAGScheduler: Final stage: ResultStage 4 (show at DataProvider.scala:47)[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO DAGScheduler: Parents of final stage: List()[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO DAGScheduler: Missing parents: List()[0m
[0m2021.07.01 21:02:08 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[16] at show at DataProvider.scala:47), which has no missing parents[0m
[0m2021.07.01 21:02:08 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 24.1 KiB, free 2.2 GiB)[0m
[0m2021.07.01 21:02:08 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 2.2 GiB)[0m
[0m2021.07.01 21:02:08 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.19.152.53:45613 (size: 10.3 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1383[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO DAGScheduler: Submitting 28 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at show at DataProvider.scala:47) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO TaskSchedulerImpl: Adding task set 4.0 with 28 tasks resource profile 0[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 78) (172.19.152.53, executor driver, partition 0, PROCESS_LOCAL, 6376 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO Executor: Running task 0.0 in stage 4.0 (TID 78)[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO CodeGenerator: Code generated in 20.0199 ms[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:08 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-04/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7456154, partition values: [17594][0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 61434 records.[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:09 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:09 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:09 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 61434[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:09 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.19.152.53:45613 in memory (size: 7.1 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:09 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.19.152.53:45613 in memory (size: 5.0 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 21:02:08 ERROR 21/07/01 21:02:09 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.19.152.53:45613 in memory (size: 27.9 KiB, free: 2.2 GiB)[0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:09 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-04/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7451790, partition values: [17594][0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 61363 records.[0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:09 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:09 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:09 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 61363[0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:10 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-11/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7420267, partition values: [17601][0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 60503 records.[0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:10 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:10 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:10 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 60503[0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:10 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-11/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7419692, partition values: [17601][0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 60420 records.[0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:10 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:10 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:10 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 60420[0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:10 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-25/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7418835, partition values: [17587][0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 60356 records.[0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:10 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:10 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:09 ERROR 21/07/01 21:02:10 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 60356[0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-04/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7416989, partition values: [17594][0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 61005 records.[0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 61005[0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-25/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7415194, partition values: [17587][0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 60243 records.[0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 60243[0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-04/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7394827, partition values: [17594][0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 60941 records.[0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 60941[0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-25/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7393469, partition values: [17587][0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 60089 records.[0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:11 ERROR 21/07/01 21:02:11 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 60089[0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-11/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7368173, partition values: [17601][0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 59990 records.[0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 59990[0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-11/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7362038, partition values: [17601][0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 59932 records.[0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 59932[0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO Executor: Finished task 0.0 in stage 4.0 (TID 78). 9905 bytes result sent to driver[0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 79) (172.19.152.53, executor driver, partition 1, PROCESS_LOCAL, 6536 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO Executor: Running task 1.0 in stage 4.0 (TID 79)[0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 78) in 3851 ms on 172.19.152.53 (executor driver) (1/28)[0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-25/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7354198, partition values: [17587][0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 59759 records.[0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:12 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 59759[0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:13 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-04/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7341290, partition values: [17594][0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 60439 records.[0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:13 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:13 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:12 ERROR 21/07/01 21:02:13 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 60439[0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:13 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-11/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7337771, partition values: [17601][0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 59696 records.[0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:13 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:13 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:13 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 59696[0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:13 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-04/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7334046, partition values: [17594][0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 60400 records.[0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:13 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:13 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:13 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 60400[0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:13 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-25/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7329070, partition values: [17587][0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 59460 records.[0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:13 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:13 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:13 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 59460[0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:14 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-04/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7320564, partition values: [17566][0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 59595 records.[0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:14 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:14 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:13 ERROR 21/07/01 21:02:14 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 59595[0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:14 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-04/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7320072, partition values: [17594][0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 60233 records.[0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:14 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:14 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:14 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 60233[0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:14 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-04/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7319772, partition values: [17566][0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 59552 records.[0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:14 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:14 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:14 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 59552[0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:14 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-25/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7310957, partition values: [17587][0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 59406 records.[0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:15 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:15 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:15 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 59406[0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:15 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-11/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7309271, partition values: [17601][0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 59547 records.[0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:15 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:15 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:14 ERROR 21/07/01 21:02:15 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 59547[0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:15 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-11/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7308810, partition values: [17601][0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 60029 records.[0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:15 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:15 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:15 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 60029[0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:15 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-04/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7303278, partition values: [17566][0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 59483 records.[0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:15 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:15 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:15 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 59483[0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:16 INFO Executor: Finished task 1.0 in stage 4.0 (TID 79). 9862 bytes result sent to driver[0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:16 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 80) (172.19.152.53, executor driver, partition 2, PROCESS_LOCAL, 6536 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:16 INFO Executor: Running task 2.0 in stage 4.0 (TID 80)[0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:16 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 79) in 3398 ms on 172.19.152.53 (executor driver) (2/28)[0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:16 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-04/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7301122, partition values: [17566][0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 59386 records.[0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:16 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:16 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:16 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 59386[0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:16 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-25/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7283811, partition values: [17587][0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 59114 records.[0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:16 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:16 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:15 ERROR 21/07/01 21:02:16 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 59114[0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:16 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-04/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7272836, partition values: [17566][0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 59213 records.[0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:16 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:16 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:16 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 59213[0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:16 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-04/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7239793, partition values: [17566][0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 58911 records.[0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:16 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:16 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:16 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 58911[0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:17 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-04/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7223535, partition values: [17566][0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 58751 records.[0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:17 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:17 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:17 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 58751[0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:17 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-18/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7207852, partition values: [17608][0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 59125 records.[0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:17 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:17 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:16 ERROR 21/07/01 21:02:17 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 59125[0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:17 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-18/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7202779, partition values: [17608][0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 59114 records.[0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:17 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:17 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:17 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 59114[0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:18 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-18/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7179969, partition values: [17608][0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 58950 records.[0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:18 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:18 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 58950[0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:18 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-18/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7177129, partition values: [17608][0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 58916 records.[0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:18 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:18 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:18 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 58916[0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:18 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-18/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7167161, partition values: [17608][0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 58764 records.[0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:18 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:18 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:17 ERROR 21/07/01 21:02:18 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 58764[0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:18 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-18/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7146213, partition values: [17608][0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 58651 records.[0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:18 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:18 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:18 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 58651[0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-10/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7140670, partition values: [17600][0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 58656 records.[0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 58656[0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO Executor: Finished task 2.0 in stage 4.0 (TID 80). 9976 bytes result sent to driver[0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 81) (172.19.152.53, executor driver, partition 3, PROCESS_LOCAL, 6557 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO Executor: Running task 3.0 in stage 4.0 (TID 81)[0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 80) in 3291 ms on 172.19.152.53 (executor driver) (3/28)[0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-18/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7125678, partition values: [17608][0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 58369 records.[0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 58369[0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-10/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7063846, partition values: [17600][0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 57970 records.[0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:18 ERROR 21/07/01 21:02:19 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 57970[0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:19 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-09/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7045345, partition values: [17599][0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 57799 records.[0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:19 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:19 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:19 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 57799[0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:20 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-10/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7044976, partition values: [17600][0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 57732 records.[0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:20 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:20 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:20 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 57732[0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:20 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-10/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7038655, partition values: [17600][0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 57783 records.[0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:20 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:20 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:20 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 57783[0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:20 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-10/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7015787, partition values: [17600][0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 57587 records.[0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:20 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:20 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:19 ERROR 21/07/01 21:02:20 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 57587[0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-10/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7014166, partition values: [17600][0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 57514 records.[0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 57514[0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-03/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-7006653, partition values: [17593][0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 57426 records.[0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 57426[0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-10/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6988002, partition values: [17600][0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 57304 records.[0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 57304[0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-03/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6986846, partition values: [17593][0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 57231 records.[0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:21 ERROR 21/07/01 21:02:21 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 57231[0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-05/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6979466, partition values: [17595][0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 57225 records.[0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 57225[0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-09/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6978784, partition values: [17599][0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 57188 records.[0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 57188[0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO Executor: Finished task 3.0 in stage 4.0 (TID 81). 9912 bytes result sent to driver[0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 82) (172.19.152.53, executor driver, partition 4, PROCESS_LOCAL, 6599 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO Executor: Running task 4.0 in stage 4.0 (TID 82)[0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 81) in 3196 ms on 172.19.152.53 (executor driver) (4/28)[0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-19/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6972514, partition values: [17609][0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 57150 records.[0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 57150[0m
Jul 01, 2021 9:02:22 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1593
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-03/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6972245, partition values: [17593][0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 57109 records.[0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:22 ERROR 21/07/01 21:02:22 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 57109[0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-05/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6970868, partition values: [17567][0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56468 records.[0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56468[0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-03/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6949243, partition values: [17593][0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56844 records.[0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 56844[0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-09/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6943858, partition values: [17599][0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56911 records.[0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56911[0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-24/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6942366, partition values: [17586][0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56690 records.[0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:23 ERROR 21/07/01 21:02:23 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56690[0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-06/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6940365, partition values: [17568][0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56115 records.[0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 56115[0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-05/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6940097, partition values: [17567][0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56209 records.[0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56209[0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-12/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6939902, partition values: [17574][0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56321 records.[0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56321[0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-12/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6939478, partition values: [17574][0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56289 records.[0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:24 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56289[0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:25 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-09/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6935669, partition values: [17599][0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56893 records.[0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:25 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:25 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:24 ERROR 21/07/01 21:02:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56893[0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-06/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6933542, partition values: [17568][0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56133 records.[0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 56133[0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO Executor: Finished task 4.0 in stage 4.0 (TID 82). 9912 bytes result sent to driver[0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 83) (172.19.152.53, executor driver, partition 5, PROCESS_LOCAL, 6641 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO Executor: Running task 5.0 in stage 4.0 (TID 83)[0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 82) in 3091 ms on 172.19.152.53 (executor driver) (5/28)[0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-09/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6933049, partition values: [17599][0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56864 records.[0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56864[0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-24/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6932369, partition values: [17586][0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56557 records.[0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:25 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 56557[0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:26 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-09/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6931828, partition values: [17599][0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56767 records.[0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:26 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:26 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:25 ERROR 21/07/01 21:02:26 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56767[0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:26 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-06/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6927511, partition values: [17568][0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56016 records.[0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:26 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:26 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:26 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56016[0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:26 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-02/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6924830, partition values: [17592][0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56848 records.[0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:26 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:26 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:26 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56848[0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:27 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-03/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6922102, partition values: [17593][0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56686 records.[0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:27 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:27 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:27 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 56686[0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:27 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-12/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6918887, partition values: [17574][0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56141 records.[0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:27 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:27 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:26 ERROR 21/07/01 21:02:27 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56141[0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:27 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-05/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6918156, partition values: [17567][0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56021 records.[0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:27 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:27 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:27 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 56021[0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:27 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-19/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6918143, partition values: [17609][0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56592 records.[0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:27 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:27 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:27 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 56592[0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:28 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-05/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6911939, partition values: [17595][0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56622 records.[0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:28 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:28 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:28 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 56622[0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:28 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-03/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6910736, partition values: [17593][0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56524 records.[0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:28 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:28 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:28 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 56524[0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:28 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-24/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6908366, partition values: [17586][0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56371 records.[0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:28 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:28 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:27 ERROR 21/07/01 21:02:28 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56371[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:28 INFO Executor: Finished task 5.0 in stage 4.0 (TID 83). 9928 bytes result sent to driver[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:28 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 84) (172.19.152.53, executor driver, partition 6, PROCESS_LOCAL, 6599 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:28 INFO Executor: Running task 6.0 in stage 4.0 (TID 84)[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:28 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 83) in 3201 ms on 172.19.152.53 (executor driver) (6/28)[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:28 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-12/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6907851, partition values: [17574][0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56038 records.[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:28 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:28 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:28 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56038[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-05/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6907368, partition values: [17567][0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55962 records.[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 55962[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-02/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6903766, partition values: [17592][0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56664 records.[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 56664[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-06/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6902309, partition values: [17568][0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55875 records.[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55875[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-05/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6901867, partition values: [17567][0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55919 records.[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:28 ERROR 21/07/01 21:02:29 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55919[0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-08/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6900687, partition values: [17570][0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55842 records.[0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55842[0m
Jul 01, 2021 9:02:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1602
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-22/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6897864, partition values: [17584][0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55701 records.[0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55701[0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-19/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6897403, partition values: [17609][0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56417 records.[0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56417[0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-05/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6895782, partition values: [17567][0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55823 records.[0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:30 ERROR 21/07/01 21:02:30 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55823[0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:31 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-05/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6895573, partition values: [17567][0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55855 records.[0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:31 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:31 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:31 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55855[0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:31 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-19/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6894595, partition values: [17609][0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56463 records.[0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:31 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:31 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:31 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56463[0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:31 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-19/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6891010, partition values: [17609][0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56357 records.[0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:31 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:31 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:31 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56357[0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:32 INFO Executor: Finished task 6.0 in stage 4.0 (TID 84). 9976 bytes result sent to driver[0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:32 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 85) (172.19.152.53, executor driver, partition 7, PROCESS_LOCAL, 6578 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:32 INFO Executor: Running task 7.0 in stage 4.0 (TID 85)[0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:32 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 84) in 3166 ms on 172.19.152.53 (executor driver) (7/28)[0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:32 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-05/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6889416, partition values: [17595][0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56397 records.[0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:32 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:32 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:31 ERROR 21/07/01 21:02:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56397[0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:32 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-24/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6888532, partition values: [17586][0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56251 records.[0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:32 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:32 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 56251[0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:32 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-20/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6885279, partition values: [17582][0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55714 records.[0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:32 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:32 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 55714[0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:32 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-12/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6882520, partition values: [17574][0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55790 records.[0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:32 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:32 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 55790[0m
Jul 01, 2021 9:02:32 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1612
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:33 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-06/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6880957, partition values: [17568][0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55713 records.[0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:33 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:33 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:32 ERROR 21/07/01 21:02:33 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55713[0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:33 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-24/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6880772, partition values: [17586][0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56136 records.[0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:33 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:33 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:33 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 56136[0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:33 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-05/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6879283, partition values: [17595][0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56296 records.[0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:33 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:33 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:33 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56296[0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:33 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-05/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6878834, partition values: [17595][0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56353 records.[0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:33 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:33 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:33 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 56353[0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:34 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-07/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6876768, partition values: [17569][0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55655 records.[0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:34 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:34 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:33 ERROR 21/07/01 21:02:34 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 55655[0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:34 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-12/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6875178, partition values: [17574][0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55753 records.[0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:34 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:34 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55753[0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:34 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-06/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6875120, partition values: [17568][0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55610 records.[0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:34 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:34 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55610[0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:35 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-24/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6874570, partition values: [17586][0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56047 records.[0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:35 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:35 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:35 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56047[0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:35 INFO Executor: Finished task 7.0 in stage 4.0 (TID 85). 9912 bytes result sent to driver[0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:35 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 86) (172.19.152.53, executor driver, partition 8, PROCESS_LOCAL, 6662 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:35 INFO Executor: Running task 8.0 in stage 4.0 (TID 86)[0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:35 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 85) in 3273 ms on 172.19.152.53 (executor driver) (8/28)[0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:35 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-12/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6874223, partition values: [17602][0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56218 records.[0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:35 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:35 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:34 ERROR 21/07/01 21:02:35 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56218[0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:35 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-14/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6871075, partition values: [17604][0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56233 records.[0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:35 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:35 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:35 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56233[0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:35 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-03/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6868911, partition values: [17593][0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56165 records.[0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:35 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:35 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:35 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56165[0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:36 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-05/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6860868, partition values: [17595][0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56244 records.[0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:36 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:36 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:36 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56244[0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:36 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-09/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6858520, partition values: [17599][0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56185 records.[0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:36 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:36 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:36 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56185[0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:36 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-02/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6856628, partition values: [17592][0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56211 records.[0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:36 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:36 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:35 ERROR 21/07/01 21:02:36 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56211[0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:36 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-20/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6856385, partition values: [17582][0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55410 records.[0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:36 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:36 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:36 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55410[0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:37 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-02/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6856052, partition values: [17592][0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56174 records.[0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:37 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:37 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 56174[0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:37 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-22/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6855704, partition values: [17584][0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55366 records.[0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:37 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:37 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55366[0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:37 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-20/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6853855, partition values: [17582][0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55397 records.[0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:37 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:37 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:36 ERROR 21/07/01 21:02:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55397[0m
Jul 01, 2021 9:02:37 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1622
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:37 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-19/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6852664, partition values: [17609][0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56086 records.[0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:37 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:37 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 56086[0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-06/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6852114, partition values: [17596][0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56044 records.[0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56044[0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO Executor: Finished task 8.0 in stage 4.0 (TID 86). 9976 bytes result sent to driver[0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 87) (172.19.152.53, executor driver, partition 9, PROCESS_LOCAL, 6641 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO Executor: Running task 9.0 in stage 4.0 (TID 87)[0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 86) in 3027 ms on 172.19.152.53 (executor driver) (9/28)[0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-06/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6849375, partition values: [17568][0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55351 records.[0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 55351[0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-24/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6848835, partition values: [17586][0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55818 records.[0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:37 ERROR 21/07/01 21:02:38 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55818[0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:38 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-02/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6848764, partition values: [17592][0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56117 records.[0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:38 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:38 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:38 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 56117[0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:39 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-22/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6848603, partition values: [17584][0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55219 records.[0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:39 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:39 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:39 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55219[0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:39 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-12/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6847438, partition values: [17574][0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55520 records.[0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:39 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:39 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:39 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55520[0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:39 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-15/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6846527, partition values: [17577][0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55599 records.[0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:39 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:39 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:38 ERROR 21/07/01 21:02:39 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55599[0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:39 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-17/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6844768, partition values: [17607][0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 56034 records.[0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:39 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:39 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:39 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 56034[0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-15/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6837655, partition values: [17577][0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55492 records.[0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55492[0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-12/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6837098, partition values: [17602][0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55911 records.[0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55911[0m
Jul 01, 2021 9:02:40 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1629
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-07/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6836106, partition values: [17569][0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55260 records.[0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55260[0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-15/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6834227, partition values: [17577][0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55557 records.[0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:39 ERROR 21/07/01 21:02:40 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55557[0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-07/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6828564, partition values: [17569][0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55217 records.[0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55217[0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO Executor: Finished task 9.0 in stage 4.0 (TID 87). 9912 bytes result sent to driver[0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 88) (172.19.152.53, executor driver, partition 10, PROCESS_LOCAL, 6620 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO Executor: Running task 10.0 in stage 4.0 (TID 88)[0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 87) in 3034 ms on 172.19.152.53 (executor driver) (10/28)[0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-10/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6823720, partition values: [17572][0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55220 records.[0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55220[0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-08/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6820388, partition values: [17570][0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55049 records.[0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55049[0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-19/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6820099, partition values: [17609][0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55808 records.[0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:41 ERROR 21/07/01 21:02:41 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 55808[0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:42 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-15/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6819490, partition values: [17577][0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55359 records.[0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:42 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:42 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:42 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55359[0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:42 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-20/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6812204, partition values: [17582][0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55013 records.[0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:42 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:42 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:42 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55013[0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:42 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-12/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6812029, partition values: [17602][0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55682 records.[0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:42 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:42 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:42 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 55682[0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:43 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-20/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6811381, partition values: [17582][0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55041 records.[0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:43 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:43 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:42 ERROR 21/07/01 21:02:43 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55041[0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:43 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-02/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6810799, partition values: [17592][0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55808 records.[0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:43 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:43 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:43 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55808[0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:43 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-08/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6808177, partition values: [17570][0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55013 records.[0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:43 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:43 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:43 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55013[0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:43 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-02/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6807660, partition values: [17592][0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55766 records.[0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:43 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:43 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:43 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55766[0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:44 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-15/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6805608, partition values: [17577][0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55285 records.[0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:44 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:44 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:43 ERROR 21/07/01 21:02:44 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55285[0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:44 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-07/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6804572, partition values: [17597][0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55681 records.[0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:44 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:44 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:44 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55681[0m
Jul 01, 2021 9:02:44 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1635
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:44 INFO Executor: Finished task 10.0 in stage 4.0 (TID 88). 9928 bytes result sent to driver[0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:44 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 89) (172.19.152.53, executor driver, partition 11, PROCESS_LOCAL, 6620 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:44 INFO Executor: Running task 11.0 in stage 4.0 (TID 89)[0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:44 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 88) in 3341 ms on 172.19.152.53 (executor driver) (11/28)[0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:44 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-01/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6804116, partition values: [17591][0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55737 records.[0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:44 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:44 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:44 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55737[0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:45 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-20/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6803037, partition values: [17582][0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54938 records.[0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:45 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:45 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:45 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54938[0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:45 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-06/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6802535, partition values: [17596][0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55705 records.[0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:45 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:45 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:44 ERROR 21/07/01 21:02:45 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55705[0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:45 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-06/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6801716, partition values: [17596][0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55684 records.[0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:45 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:45 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:45 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55684[0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:45 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-15/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6801242, partition values: [17577][0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55264 records.[0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:45 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:45 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:45 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55264[0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:46 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-22/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6800112, partition values: [17584][0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54858 records.[0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:46 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:46 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:46 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 54858[0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:46 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-08/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6798023, partition values: [17570][0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54927 records.[0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:46 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:46 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:45 ERROR 21/07/01 21:02:46 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54927[0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:46 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-22/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6795902, partition values: [17584][0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55343 records.[0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:46 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:46 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:46 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55343[0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:46 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-10/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6795157, partition values: [17572][0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54934 records.[0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:46 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:46 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:46 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54934[0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:47 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-22/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6794911, partition values: [17584][0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55368 records.[0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:47 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:47 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:47 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55368[0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:47 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-20/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6791322, partition values: [17582][0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54817 records.[0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:47 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:47 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:46 ERROR 21/07/01 21:02:47 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54817[0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:47 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-12/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6790906, partition values: [17602][0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55468 records.[0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:47 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:47 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:47 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55468[0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO Executor: Finished task 11.0 in stage 4.0 (TID 89). 9928 bytes result sent to driver[0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 90) (172.19.152.53, executor driver, partition 12, PROCESS_LOCAL, 6662 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO Executor: Running task 12.0 in stage 4.0 (TID 90)[0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 89) in 3441 ms on 172.19.152.53 (executor driver) (12/28)[0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-08/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6790452, partition values: [17570][0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54879 records.[0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54879[0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-12/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6790183, partition values: [17602][0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55479 records.[0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55479[0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-06/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6789659, partition values: [17596][0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55493 records.[0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:47 ERROR 21/07/01 21:02:48 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55493[0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:48 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-17/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6788642, partition values: [17607][0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55571 records.[0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:48 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:48 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:48 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55571[0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:49 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-12/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6787867, partition values: [17602][0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55490 records.[0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:49 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:49 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:49 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55490[0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:49 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-21/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6787299, partition values: [17583][0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54803 records.[0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:49 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:49 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:49 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54803[0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:49 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-07/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6787222, partition values: [17569][0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54763 records.[0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:49 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:49 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:48 ERROR 21/07/01 21:02:49 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54763[0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-22/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6786363, partition values: [17584][0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54742 records.[0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54742[0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-17/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6785842, partition values: [17607][0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55513 records.[0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55513[0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-05/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6784882, partition values: [17595][0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55519 records.[0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55519[0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-14/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6784303, partition values: [17604][0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55477 records.[0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:50 ERROR 21/07/01 21:02:50 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55477[0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-14/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6783191, partition values: [17576][0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55152 records.[0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55152[0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO Executor: Finished task 12.0 in stage 4.0 (TID 90). 9944 bytes result sent to driver[0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 91) (172.19.152.53, executor driver, partition 13, PROCESS_LOCAL, 6641 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO Executor: Running task 13.0 in stage 4.0 (TID 91)[0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 90) in 3213 ms on 172.19.152.53 (executor driver) (13/28)[0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-14/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6782969, partition values: [17604][0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55488 records.[0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55488[0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-15/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6782543, partition values: [17577][0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55041 records.[0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55041[0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-14/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6777283, partition values: [17576][0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55103 records.[0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:51 ERROR 21/07/01 21:02:51 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55103[0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-07/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6777235, partition values: [17597][0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55420 records.[0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55420[0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-17/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6776594, partition values: [17607][0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55480 records.[0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55480[0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-07/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6776471, partition values: [17569][0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54775 records.[0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54775[0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-06/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6774586, partition values: [17596][0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55431 records.[0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:52 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55431[0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:53 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-03/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6773317, partition values: [17565][0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54872 records.[0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:53 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:53 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:52 ERROR 21/07/01 21:02:53 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 54872[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:53 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-07/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6770727, partition values: [17569][0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54713 records.[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:53 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:53 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:53 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54713[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:53 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-14/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6770353, partition values: [17604][0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55339 records.[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:53 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:53 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:53 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55339[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:53 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-08/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6769134, partition values: [17570][0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54643 records.[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:53 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:53 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:53 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54643[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:54 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-14/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6767041, partition values: [17576][0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55024 records.[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:54 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:54 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:54 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55024[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:54 INFO Executor: Finished task 13.0 in stage 4.0 (TID 91). 9878 bytes result sent to driver[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:54 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 92) (172.19.152.53, executor driver, partition 14, PROCESS_LOCAL, 6641 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:54 INFO Executor: Running task 14.0 in stage 4.0 (TID 92)[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:54 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 91) in 2957 ms on 172.19.152.53 (executor driver) (14/28)[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:54 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-01/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6766977, partition values: [17591][0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55504 records.[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:54 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:54 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:53 ERROR 21/07/01 21:02:54 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55504[0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:54 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-01/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6765130, partition values: [17591][0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55547 records.[0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:54 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:54 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:54 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 55547[0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:54 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-01/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6764160, partition values: [17591][0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55464 records.[0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:54 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:54 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:54 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55464[0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:55 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-14/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6763365, partition values: [17604][0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55321 records.[0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:55 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:55 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:55 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 55321[0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:55 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-03/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6762180, partition values: [17565][0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54746 records.[0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:55 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:55 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:54 ERROR 21/07/01 21:02:55 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54746[0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:55 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-13/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6759894, partition values: [17603][0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55199 records.[0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:55 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:55 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:55 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55199[0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:55 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-21/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6759882, partition values: [17583][0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54480 records.[0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:55 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:55 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:55 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54480[0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:56 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-06/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6757416, partition values: [17596][0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55326 records.[0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:56 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:56 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:56 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55326[0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:56 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-12/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6757143, partition values: [17602][0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55228 records.[0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:56 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:56 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:56 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55228[0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:56 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-14/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6755271, partition values: [17604][0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55217 records.[0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:56 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:56 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:55 ERROR 21/07/01 21:02:56 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55217[0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:56 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-07/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6752827, partition values: [17569][0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54625 records.[0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:56 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:56 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:56 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54625[0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-10/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6752521, partition values: [17572][0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54584 records.[0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54584[0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO Executor: Finished task 14.0 in stage 4.0 (TID 92). 9955 bytes result sent to driver[0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 93) (172.19.152.53, executor driver, partition 15, PROCESS_LOCAL, 6620 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO Executor: Running task 15.0 in stage 4.0 (TID 93)[0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 92) in 3132 ms on 172.19.152.53 (executor driver) (15/28)[0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-14/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6750873, partition values: [17604][0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55220 records.[0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 55220[0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-21/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6746025, partition values: [17583][0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54407 records.[0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:56 ERROR 21/07/01 21:02:57 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54407[0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-10/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6745720, partition values: [17572][0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54442 records.[0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 54442[0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-23/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6745610, partition values: [17585][0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54927 records.[0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54927[0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-14/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6745370, partition values: [17576][0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54791 records.[0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54791[0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-17/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6742757, partition values: [17607][0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55142 records.[0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:58 ERROR 21/07/01 21:02:58 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55142[0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-08/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6742216, partition values: [17570][0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54425 records.[0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54425[0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-21/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6741271, partition values: [17583][0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54370 records.[0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 54370[0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-13/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6740613, partition values: [17603][0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55090 records.[0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55090[0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-14/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6739077, partition values: [17576][0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54749 records.[0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:02:59 ERROR 21/07/01 21:02:59 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54749[0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-13/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6737775, partition values: [17603][0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55047 records.[0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55047[0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-21/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6736002, partition values: [17583][0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54355 records.[0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54355[0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO Executor: Finished task 15.0 in stage 4.0 (TID 93). 9896 bytes result sent to driver[0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 94) (172.19.152.53, executor driver, partition 16, PROCESS_LOCAL, 6641 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO Executor: Running task 16.0 in stage 4.0 (TID 94)[0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 93) in 3364 ms on 172.19.152.53 (executor driver) (16/28)[0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-06/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6735999, partition values: [17596][0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55137 records.[0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:00 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55137[0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:01 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-17/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6735861, partition values: [17607][0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55123 records.[0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:01 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:01 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:00 ERROR 21/07/01 21:03:01 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55123[0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:01 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-13/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6735748, partition values: [17603][0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54948 records.[0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:01 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:01 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:01 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54948[0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:01 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-08/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6733619, partition values: [17598][0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55059 records.[0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:01 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:01 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:01 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 55059[0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:01 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-21/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6731199, partition values: [17583][0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54301 records.[0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:01 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:01 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:01 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 54301[0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:02 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-13/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6728914, partition values: [17603][0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55026 records.[0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:02 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:02 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:01 ERROR 21/07/01 21:03:02 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 55026[0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:02 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-07/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6726030, partition values: [17597][0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 55011 records.[0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:02 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:02 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:02 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 55011[0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:02 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-16/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6723880, partition values: [17578][0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54315 records.[0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:02 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:02 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:02 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 54315[0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:02 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-15/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6723171, partition values: [17605][0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54892 records.[0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:02 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:02 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:02 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54892[0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:03 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-14/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6721977, partition values: [17576][0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54621 records.[0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:03 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:03 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:02 ERROR 21/07/01 21:03:03 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54621[0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-07/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6718134, partition values: [17597][0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54923 records.[0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54923[0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-13/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6717956, partition values: [17603][0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54850 records.[0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54850[0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO Executor: Finished task 16.0 in stage 4.0 (TID 94). 9878 bytes result sent to driver[0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 95) (172.19.152.53, executor driver, partition 17, PROCESS_LOCAL, 6620 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO Executor: Running task 17.0 in stage 4.0 (TID 95)[0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 94) in 3112 ms on 172.19.152.53 (executor driver) (17/28)[0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-14/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6714862, partition values: [17576][0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54532 records.[0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:03 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54532[0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:04 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-16/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6712406, partition values: [17578][0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54195 records.[0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:04 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:04 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:03 ERROR 21/07/01 21:03:04 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 54195[0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:04 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-23/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6711586, partition values: [17585][0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54619 records.[0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:04 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:04 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:04 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54619[0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:04 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-23/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6710909, partition values: [17585][0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54634 records.[0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:04 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:04 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:04 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54634[0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-10/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6707717, partition values: [17572][0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54208 records.[0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:05 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:05 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54208[0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-10/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6706966, partition values: [17572][0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54195 records.[0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:05 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:05 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:04 ERROR 21/07/01 21:03:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54195[0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-09/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6706629, partition values: [17571][0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54117 records.[0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:05 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:05 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:05 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54117[0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:05 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-01/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6703505, partition values: [17591][0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54981 records.[0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:05 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:05 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:05 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54981[0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-13/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6701121, partition values: [17575][0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54403 records.[0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:06 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:06 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:06 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54403[0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-23/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6699525, partition values: [17585][0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54507 records.[0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:06 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:06 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:06 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54507[0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-09/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6699472, partition values: [17571][0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54056 records.[0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:06 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:06 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:05 ERROR 21/07/01 21:03:06 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54056[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-17/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6698346, partition values: [17607][0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54790 records.[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:06 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:06 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:06 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54790[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:06 INFO Executor: Finished task 17.0 in stage 4.0 (TID 95). 9992 bytes result sent to driver[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:06 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 96) (172.19.152.53, executor driver, partition 18, PROCESS_LOCAL, 6662 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:06 INFO Executor: Running task 18.0 in stage 4.0 (TID 96)[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:06 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 95) in 3058 ms on 172.19.152.53 (executor driver) (18/28)[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:06 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-13/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6696669, partition values: [17575][0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54296 records.[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:06 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:06 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:06 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54296[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-01/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6695909, partition values: [17591][0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54886 records.[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:07 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:07 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:07 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54886[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-03/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6692407, partition values: [17565][0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54711 records.[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:07 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:07 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:07 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54711[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-07/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6692115, partition values: [17597][0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54712 records.[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:07 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:07 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:06 ERROR 21/07/01 21:03:07 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54712[0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:07 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-09/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6687853, partition values: [17571][0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54039 records.[0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:07 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:07 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:07 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54039[0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:08 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-07/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6687562, partition values: [17597][0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54629 records.[0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:08 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:08 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:08 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54629[0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:08 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-16/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6685879, partition values: [17578][0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54424 records.[0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:08 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:08 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:08 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54424[0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:08 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-10/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6682741, partition values: [17572][0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54021 records.[0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:08 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:08 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:07 ERROR 21/07/01 21:03:08 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54021[0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:08 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-20/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6682178, partition values: [17610][0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54542 records.[0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:08 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:08 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:08 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54542[0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-13/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6679085, partition values: [17575][0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54184 records.[0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54184[0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-21/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6678664, partition values: [17583][0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53888 records.[0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 53888[0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-26/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6676699, partition values: [17588][0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54326 records.[0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54326[0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO Executor: Finished task 18.0 in stage 4.0 (TID 96). 9971 bytes result sent to driver[0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 97) (172.19.152.53, executor driver, partition 19, PROCESS_LOCAL, 6620 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO Executor: Running task 19.0 in stage 4.0 (TID 97)[0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 96) in 2953 ms on 172.19.152.53 (executor driver) (19/28)[0m
[0m2021.07.01 21:03:08 ERROR 21/07/01 21:03:09 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-23/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6673018, partition values: [17585][0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54364 records.[0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:09 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:09 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:09 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54364[0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:10 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-15/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6672395, partition values: [17605][0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54482 records.[0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:10 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:10 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:10 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54482[0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:10 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-03/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6670560, partition values: [17565][0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54529 records.[0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:10 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:10 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:10 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54529[0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:10 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-07/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6668923, partition values: [17597][0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54450 records.[0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:10 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:10 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:09 ERROR 21/07/01 21:03:10 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54450[0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:10 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-13/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6667977, partition values: [17603][0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54430 records.[0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:10 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:10 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:10 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54430[0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:11 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-23/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6667320, partition values: [17585][0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54338 records.[0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:11 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:11 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:11 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54338[0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:11 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-20/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6666072, partition values: [17610][0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54489 records.[0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:11 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:11 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:11 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54489[0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:11 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-20/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6665806, partition values: [17610][0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54457 records.[0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:11 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:11 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:10 ERROR 21/07/01 21:03:11 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54457[0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:11 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-01/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6664062, partition values: [17591][0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54640 records.[0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:11 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:11 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:11 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54640[0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:12 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-08/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6658253, partition values: [17598][0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54351 records.[0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:12 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:12 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:12 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54351[0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:12 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-15/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6656819, partition values: [17605][0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54327 records.[0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:12 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:12 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:12 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54327[0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:12 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-23/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6653944, partition values: [17585][0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54138 records.[0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:12 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:12 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:11 ERROR 21/07/01 21:03:12 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54138[0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO Executor: Finished task 19.0 in stage 4.0 (TID 97). 9944 bytes result sent to driver[0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 98) (172.19.152.53, executor driver, partition 20, PROCESS_LOCAL, 6578 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO Executor: Running task 20.0 in stage 4.0 (TID 98)[0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 97) in 3102 ms on 172.19.152.53 (executor driver) (20/28)[0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-09/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6653532, partition values: [17571][0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53682 records.[0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 53682[0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-15/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6653508, partition values: [17605][0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54330 records.[0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54330[0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-03/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6652148, partition values: [17565][0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53814 records.[0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53814[0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-20/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6649512, partition values: [17610][0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54298 records.[0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:13 ERROR 21/07/01 21:03:13 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54298[0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-15/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6649145, partition values: [17605][0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54284 records.[0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 54284[0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-16/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6648242, partition values: [17578][0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53635 records.[0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 53635[0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-16/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6648215, partition values: [17578][0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54120 records.[0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54120[0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-09/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6648141, partition values: [17571][0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53696 records.[0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:14 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53696[0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:15 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-09/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6648003, partition values: [17571][0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53657 records.[0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:15 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:15 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:14 ERROR 21/07/01 21:03:15 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53657[0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:15 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-20/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6637721, partition values: [17610][0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54155 records.[0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:15 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:15 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:15 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54155[0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:15 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-08/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6636527, partition values: [17598][0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54190 records.[0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:15 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:15 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:15 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54190[0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:15 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-15/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6636455, partition values: [17605][0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54167 records.[0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:15 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:15 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:15 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54167[0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:16 INFO Executor: Finished task 20.0 in stage 4.0 (TID 98). 9878 bytes result sent to driver[0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:16 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 99) (172.19.152.53, executor driver, partition 21, PROCESS_LOCAL, 6599 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:16 INFO Executor: Running task 21.0 in stage 4.0 (TID 99)[0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:16 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 98) in 3085 ms on 172.19.152.53 (executor driver) (21/28)[0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:16 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-20/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6635364, partition values: [17610][0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54161 records.[0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:16 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:16 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:15 ERROR 21/07/01 21:03:16 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54161[0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:16 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-03/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6627974, partition values: [17565][0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54159 records.[0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:16 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:16 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:16 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54159[0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:16 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-08/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6626197, partition values: [17598][0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54088 records.[0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:16 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:16 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:16 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 54088[0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:16 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-09/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6622287, partition values: [17571][0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53421 records.[0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:16 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:16 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:16 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53421[0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:17 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-20/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6620982, partition values: [17610][0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54067 records.[0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:17 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:17 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:16 ERROR 21/07/01 21:03:17 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 54067[0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:17 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-26/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6617371, partition values: [17588][0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53885 records.[0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:17 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:17 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:17 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 53885[0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:17 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-26/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6617265, partition values: [17588][0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53804 records.[0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:17 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:17 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:17 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 53804[0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:17 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-08/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6615677, partition values: [17598][0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53973 records.[0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:17 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:17 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:17 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 53973[0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:18 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-13/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6615030, partition values: [17575][0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 54190 records.[0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:18 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:18 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 54190[0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:18 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-26/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6612710, partition values: [17588][0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53856 records.[0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:18 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:18 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:17 ERROR 21/07/01 21:03:18 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53856[0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:18 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-16/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6612134, partition values: [17578][0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53836 records.[0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:18 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:18 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 53836[0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:18 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-13/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6610814, partition values: [17575][0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53618 records.[0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:18 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:18 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:18 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53618[0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:19 INFO Executor: Finished task 21.0 in stage 4.0 (TID 99). 9878 bytes result sent to driver[0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:19 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 100) (172.19.152.53, executor driver, partition 22, PROCESS_LOCAL, 6620 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:19 INFO Executor: Running task 22.0 in stage 4.0 (TID 100)[0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:19 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 99) in 2951 ms on 172.19.152.53 (executor driver) (22/28)[0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:19 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-08/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6604473, partition values: [17598][0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53950 records.[0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:19 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:19 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:19 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 53950[0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:19 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-03/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6603351, partition values: [17565][0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53916 records.[0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:19 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:19 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:18 ERROR 21/07/01 21:03:19 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53916[0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:19 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-26/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6597400, partition values: [17588][0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53711 records.[0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:19 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:19 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:19 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 53711[0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:19 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-08/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6596926, partition values: [17598][0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53816 records.[0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:19 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:19 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:19 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53816[0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:20 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-16/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6586765, partition values: [17578][0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53623 records.[0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:20 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:20 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 53623[0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:20 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-15/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6578804, partition values: [17605][0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53618 records.[0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:20 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:20 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:19 ERROR 21/07/01 21:03:20 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53618[0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:20 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-26/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6567848, partition values: [17588][0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53458 records.[0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:20 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:20 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 53458[0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:20 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-26/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6560094, partition values: [17588][0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53419 records.[0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:20 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:20 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 53419[0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:21 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-13/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6558040, partition values: [17575][0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53644 records.[0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:21 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:21 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:21 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53644[0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:21 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-27/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6541570, partition values: [17589][0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53307 records.[0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:21 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:21 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:21 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53307[0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:21 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-28/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6533569, partition values: [17590][0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53326 records.[0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:21 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:21 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:20 ERROR 21/07/01 21:03:21 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53326[0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:21 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-27/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6530209, partition values: [17589][0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53207 records.[0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:21 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:21 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:21 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53207[0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO Executor: Finished task 22.0 in stage 4.0 (TID 100). 9878 bytes result sent to driver[0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 101) (172.19.152.53, executor driver, partition 23, PROCESS_LOCAL, 6557 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO Executor: Running task 23.0 in stage 4.0 (TID 101)[0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 100) in 3006 ms on 172.19.152.53 (executor driver) (23/28)[0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-27/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6527807, partition values: [17589][0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53216 records.[0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53216[0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-27/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6523776, partition values: [17589][0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53175 records.[0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53175[0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-13/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6523100, partition values: [17575][0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53348 records.[0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53348[0m
[0m2021.07.01 21:03:21 ERROR 21/07/01 21:03:22 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-28/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6510770, partition values: [17590][0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53119 records.[0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:22 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:22 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:22 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 53119[0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:23 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-27/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6508341, partition values: [17589][0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53008 records.[0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:23 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:23 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:23 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53008[0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:23 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-28/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6502224, partition values: [17590][0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53019 records.[0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:23 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:23 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:23 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 53019[0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:23 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-16/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6500562, partition values: [17606][0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53067 records.[0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:23 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:23 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:22 ERROR 21/07/01 21:03:23 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 53067[0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:23 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-16/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6499813, partition values: [17606][0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 53023 records.[0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:23 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:23 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:23 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 53023[0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:24 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-28/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6494381, partition values: [17590][0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52970 records.[0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:24 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:24 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:24 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 52970[0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:24 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-16/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6480037, partition values: [17606][0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52943 records.[0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:24 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:24 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:24 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 52943[0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:24 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-16/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6479666, partition values: [17606][0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52880 records.[0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:24 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:24 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:23 ERROR 21/07/01 21:03:24 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 52880[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:24 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-21/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6472674, partition values: [17611][0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52778 records.[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:24 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:24 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:24 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 52778[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO Executor: Finished task 23.0 in stage 4.0 (TID 101). 9896 bytes result sent to driver[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 102) (172.19.152.53, executor driver, partition 24, PROCESS_LOCAL, 6696 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO Executor: Running task 24.0 in stage 4.0 (TID 102)[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 101) in 3031 ms on 172.19.152.53 (executor driver) (24/28)[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-16/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6471548, partition values: [17606][0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52829 records.[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 52829[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-28/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6463604, partition values: [17590][0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52612 records.[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 52612[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-21/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6461000, partition values: [17611][0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52719 records.[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 52719[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-27/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6454027, partition values: [17589][0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52554 records.[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:24 ERROR 21/07/01 21:03:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 52554[0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-27/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6443947, partition values: [17589][0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52506 records.[0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 52506[0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-28/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6428282, partition values: [17590][0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52372 records.[0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 52372[0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-16/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6424883, partition values: [17606][0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52511 records.[0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 52511[0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-21/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6424324, partition values: [17611][0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52393 records.[0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:26 ERROR 21/07/01 21:03:26 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 52393[0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-16/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6421216, partition values: [17606][0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52455 records.[0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 52455[0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-02/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6414201, partition values: [17564][0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52348 records.[0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 52348[0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-21/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6409231, partition values: [17611][0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52253 records.[0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 52253[0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-28/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6405724, partition values: [17590][0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52169 records.[0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:27 ERROR 21/07/01 21:03:27 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 52169[0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-21/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6404453, partition values: [17611][0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52164 records.[0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 52164[0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO Executor: Finished task 24.0 in stage 4.0 (TID 102). 9896 bytes result sent to driver[0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 103) (172.19.152.53, executor driver, partition 25, PROCESS_LOCAL, 6654 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO Executor: Running task 25.0 in stage 4.0 (TID 103)[0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 102) in 3305 ms on 172.19.152.53 (executor driver) (25/28)[0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-02/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6399672, partition values: [17564][0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52234 records.[0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 52234[0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-02/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6397595, partition values: [17564][0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52157 records.[0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 52157[0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-21/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6388265, partition values: [17611][0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52122 records.[0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:28 ERROR 21/07/01 21:03:28 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 52122[0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:29 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-03-21/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6361687, partition values: [17611][0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 51899 records.[0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:29 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:29 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:29 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 51899[0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:29 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-02/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6357107, partition values: [17564][0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 51827 records.[0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:29 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:29 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:29 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 51827[0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:29 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-02/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6352970, partition values: [17564][0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 51731 records.[0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:29 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:29 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:29 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 51731[0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:29 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-01/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6349917, partition values: [17563][0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 52276 records.[0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:30 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:30 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:29 ERROR 21/07/01 21:03:30 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 52276[0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:30 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-02/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6328709, partition values: [17564][0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 51562 records.[0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:30 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:30 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:30 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 51562[0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:30 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-02/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6322068, partition values: [17564][0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 51467 records.[0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:30 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:30 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:30 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 51467[0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:30 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-01/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6282002, partition values: [17563][0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 51720 records.[0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:30 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:30 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:30 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 51720[0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:31 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-01/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6274704, partition values: [17563][0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 51610 records.[0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:31 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:31 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:30 ERROR 21/07/01 21:03:31 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 51610[0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-01/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6248906, partition values: [17563][0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 51385 records.[0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 51385[0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-01/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6216083, partition values: [17563][0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 51163 records.[0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 51163[0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO Executor: Finished task 25.0 in stage 4.0 (TID 103). 9928 bytes result sent to driver[0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 104) (172.19.152.53, executor driver, partition 26, PROCESS_LOCAL, 6654 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO Executor: Running task 26.0 in stage 4.0 (TID 104)[0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 103) in 3495 ms on 172.19.152.53 (executor driver) (26/28)[0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-01/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6208668, partition values: [17563][0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 51056 records.[0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:31 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 51056[0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:32 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-01/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6196347, partition values: [17563][0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 50920 records.[0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:32 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:32 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:31 ERROR 21/07/01 21:03:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 50920[0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:32 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-19/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6136934, partition values: [17581][0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 49418 records.[0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:32 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:32 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 49418[0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:32 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-19/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6115531, partition values: [17581][0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 49255 records.[0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:32 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:32 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 49255[0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:32 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-19/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6109541, partition values: [17581][0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 49109 records.[0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:32 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:32 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 49109[0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:33 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-19/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6069368, partition values: [17581][0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 48847 records.[0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:33 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:33 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:33 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 48847[0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:33 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-19/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6049265, partition values: [17581][0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 48706 records.[0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:33 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:33 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:32 ERROR 21/07/01 21:03:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 48706[0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:33 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-19/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-6020665, partition values: [17581][0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 48380 records.[0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:33 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:33 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:33 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 48380[0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:33 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-19/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-5995242, partition values: [17581][0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 48643 records.[0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:33 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:33 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 48643[0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:34 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-18/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-5377021, partition values: [17580][0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 43623 records.[0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:34 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:34 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 43623[0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:34 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-18/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-5374749, partition values: [17580][0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 43541 records.[0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:34 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:34 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 43541[0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:34 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-18/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-5372496, partition values: [17580][0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 43571 records.[0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:34 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:34 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:33 ERROR 21/07/01 21:03:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 43571[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:34 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-18/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-5346986, partition values: [17580][0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 43376 records.[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:34 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:34 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 43376[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:34 INFO Executor: Finished task 26.0 in stage 4.0 (TID 104). 9862 bytes result sent to driver[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:34 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 105) (172.19.152.53, executor driver, partition 27, PROCESS_LOCAL, 6216 bytes) taskResourceAssignments Map()[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:34 INFO Executor: Running task 27.0 in stage 4.0 (TID 105)[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:34 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 104) in 2996 ms on 172.19.152.53 (executor driver) (27/28)[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:34 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-18/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-5342085, partition values: [17580][0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 43331 records.[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:34 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:34 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 43331[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-18/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-5332496, partition values: [17580][0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 43283 records.[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 43283[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-18/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-5319023, partition values: [17580][0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 43106 records.[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 43106[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-17/part-00004-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-3605420, partition values: [17579][0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 28525 records.[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 28525[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-17/part-00002-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-3603903, partition values: [17579][0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 28520 records.[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:34 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 28520[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:35 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-17/part-00003-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-3595887, partition values: [17579][0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 28442 records.[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:35 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 28442[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:35 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-17/part-00000-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-3590788, partition values: [17579][0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 28362 records.[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:35 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:35 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 28362[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-17/part-00001-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-3582062, partition values: [17579][0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 28346 records.[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 28346[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-17/part-00006-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-3559447, partition values: [17579][0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 28130 records.[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 28130[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO FileScanRDD: Reading File path: file:/home/hwait/dev/data/sna/date=2018-02-17/part-00005-a3ee4273-5f2a-4454-9da3-2917f9345a30.c000.gz.parquet, range: 0-3537910, partition values: [17579][0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 27986 records.[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO InternalParquetRecordReader: at row 0. reading next block[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO CodecPool: Got brand-new decompressor [.gz][0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 27986[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO Executor: Finished task 27.0 in stage 4.0 (TID 105). 9960 bytes result sent to driver[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 105) in 1525 ms on 172.19.152.53 (executor driver) (28/28)[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool [0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO DAGScheduler: ResultStage 4 (show at DataProvider.scala:47) finished in 87.527 s[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO DAGScheduler: Job 3 finished: show at DataProvider.scala:47, took 87.534130 s[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO CodeGenerator: Code generated in 26.1107 ms[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO CodeGenerator: Code generated in 20.2308 ms[0m
[0m2021.07.01 21:03:35 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 21:03:35 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 21:03:35 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 21:03:35 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 21:03:35 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 21:03:35 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 21:03:35 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 21:03:35 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 21:03:35 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 21:03:35 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:03:35 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 21:03:35 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 21:03:35 INFO  only showing top 30 rows[0m
[0m2021.07.01 21:03:35 INFO  [0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO ProducerConfig: ProducerConfig values: [0m
[0m2021.07.01 21:03:35 ERROR 	acks = 1[0m
[0m2021.07.01 21:03:35 ERROR 	batch.size = 16384[0m
[0m2021.07.01 21:03:35 ERROR 	bootstrap.servers = [localhost:9092][0m
[0m2021.07.01 21:03:35 ERROR 	buffer.memory = 33554432[0m
[0m2021.07.01 21:03:35 ERROR 	client.dns.lookup = use_all_dns_ips[0m
[0m2021.07.01 21:03:35 ERROR 	client.id = producer-1[0m
[0m2021.07.01 21:03:35 ERROR 	compression.type = none[0m
[0m2021.07.01 21:03:35 ERROR 	connections.max.idle.ms = 540000[0m
[0m2021.07.01 21:03:35 ERROR 	delivery.timeout.ms = 120000[0m
[0m2021.07.01 21:03:35 ERROR 	enable.idempotence = false[0m
[0m2021.07.01 21:03:35 ERROR 	interceptor.classes = [][0m
[0m2021.07.01 21:03:35 ERROR 	internal.auto.downgrade.txn.commit = false[0m
[0m2021.07.01 21:03:35 ERROR 	key.serializer = class org.apache.kafka.common.serialization.StringSerializer[0m
[0m2021.07.01 21:03:35 ERROR 	linger.ms = 0[0m
[0m2021.07.01 21:03:35 ERROR 	max.block.ms = 60000[0m
[0m2021.07.01 21:03:35 ERROR 	max.in.flight.requests.per.connection = 5[0m
[0m2021.07.01 21:03:35 ERROR 	max.request.size = 1048576[0m
[0m2021.07.01 21:03:35 ERROR 	metadata.max.age.ms = 300000[0m
[0m2021.07.01 21:03:35 ERROR 	metadata.max.idle.ms = 300000[0m
[0m2021.07.01 21:03:35 ERROR 	metric.reporters = [][0m
[0m2021.07.01 21:03:35 ERROR 	metrics.num.samples = 2[0m
[0m2021.07.01 21:03:35 ERROR 	metrics.recording.level = INFO[0m
[0m2021.07.01 21:03:35 ERROR 	metrics.sample.window.ms = 30000[0m
[0m2021.07.01 21:03:35 ERROR 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner[0m
[0m2021.07.01 21:03:35 ERROR 	receive.buffer.bytes = 32768[0m
[0m2021.07.01 21:03:35 ERROR 	reconnect.backoff.max.ms = 1000[0m
[0m2021.07.01 21:03:35 ERROR 	reconnect.backoff.ms = 50[0m
[0m2021.07.01 21:03:35 ERROR 	request.timeout.ms = 30000[0m
[0m2021.07.01 21:03:35 ERROR 	retries = 2147483647[0m
[0m2021.07.01 21:03:35 ERROR 	retry.backoff.ms = 100[0m
[0m2021.07.01 21:03:35 ERROR 	sasl.client.callback.handler.class = null[0m
[0m2021.07.01 21:03:35 ERROR 	sasl.jaas.config = null[0m
[0m2021.07.01 21:03:35 ERROR 	sasl.kerberos.kinit.cmd = /usr/bin/kinit[0m
[0m2021.07.01 21:03:35 ERROR 	sasl.kerberos.min.time.before.relogin = 60000[0m
[0m2021.07.01 21:03:35 ERROR 	sasl.kerberos.service.name = null[0m
[0m2021.07.01 21:03:35 ERROR 	sasl.kerberos.ticket.renew.jitter = 0.05[0m
[0m2021.07.01 21:03:35 ERROR 	sasl.kerberos.ticket.renew.window.factor = 0.8[0m
[0m2021.07.01 21:03:35 ERROR 	sasl.login.callback.handler.class = null[0m
[0m2021.07.01 21:03:35 ERROR 	sasl.login.class = null[0m
[0m2021.07.01 21:03:35 ERROR 	sasl.login.refresh.buffer.seconds = 300[0m
[0m2021.07.01 21:03:35 ERROR 	sasl.login.refresh.min.period.seconds = 60[0m
[0m2021.07.01 21:03:35 ERROR 	sasl.login.refresh.window.factor = 0.8[0m
[0m2021.07.01 21:03:35 ERROR 	sasl.login.refresh.window.jitter = 0.05[0m
[0m2021.07.01 21:03:35 ERROR 	sasl.mechanism = GSSAPI[0m
[0m2021.07.01 21:03:35 ERROR 	security.protocol = PLAINTEXT[0m
[0m2021.07.01 21:03:35 ERROR 	security.providers = null[0m
[0m2021.07.01 21:03:35 ERROR 	send.buffer.bytes = 131072[0m
[0m2021.07.01 21:03:35 ERROR 	ssl.cipher.suites = null[0m
[0m2021.07.01 21:03:35 ERROR 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3][0m
[0m2021.07.01 21:03:35 ERROR 	ssl.endpoint.identification.algorithm = https[0m
[0m2021.07.01 21:03:35 ERROR 	ssl.engine.factory.class = null[0m
[0m2021.07.01 21:03:35 ERROR 	ssl.key.password = null[0m
[0m2021.07.01 21:03:35 ERROR 	ssl.keymanager.algorithm = SunX509[0m
[0m2021.07.01 21:03:35 ERROR 	ssl.keystore.location = null[0m
[0m2021.07.01 21:03:35 ERROR 	ssl.keystore.password = null[0m
[0m2021.07.01 21:03:35 ERROR 	ssl.keystore.type = JKS[0m
[0m2021.07.01 21:03:35 ERROR 	ssl.protocol = TLSv1.3[0m
[0m2021.07.01 21:03:35 ERROR 	ssl.provider = null[0m
[0m2021.07.01 21:03:35 ERROR 	ssl.secure.random.implementation = null[0m
[0m2021.07.01 21:03:35 ERROR 	ssl.trustmanager.algorithm = PKIX[0m
[0m2021.07.01 21:03:35 ERROR 	ssl.truststore.location = null[0m
[0m2021.07.01 21:03:35 ERROR 	ssl.truststore.password = null[0m
[0m2021.07.01 21:03:35 ERROR 	ssl.truststore.type = JKS[0m
[0m2021.07.01 21:03:35 ERROR 	transaction.timeout.ms = 60000[0m
[0m2021.07.01 21:03:35 ERROR 	transactional.id = null[0m
[0m2021.07.01 21:03:35 ERROR 	value.serializer = class org.apache.kafka.common.serialization.StringSerializer[0m
[0m2021.07.01 21:03:35 ERROR [0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO AppInfoParser: Kafka version: 2.6.0[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO AppInfoParser: Kafka commitId: 62abe01bee039651[0m
[0m2021.07.01 21:03:35 ERROR 21/07/01 21:03:36 INFO AppInfoParser: Kafka startTimeMs: 1625151816594[0m
[0m2021.07.01 21:03:37 ERROR 21/07/01 21:03:37 INFO Metadata: [Producer clientId=producer-1] Cluster ID: -TctB5CtQT2vWcpApmpk2w[0m
[0m2021.07.01 21:03:37 INFO  Pushed Setosa Record: {"SepalLength":1}[0m
[0m2021.07.01 21:03:37 ERROR 21/07/01 21:03:37 INFO KafkaProducer: [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.[0m
[0m2021.07.01 21:03:37 ERROR 21/07/01 21:03:37 INFO SparkContext: Invoking stop() from shutdown hook[0m
[0m2021.07.01 21:03:37 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 21:03:37 ERROR 21/07/01 21:03:37 INFO SparkUI: Stopped Spark web UI at http://172.19.152.53:4040[0m
[0m2021.07.01 21:03:37 ERROR 21/07/01 21:03:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped![0m
[0m2021.07.01 21:03:37 ERROR 21/07/01 21:03:37 INFO MemoryStore: MemoryStore cleared[0m
[0m2021.07.01 21:03:37 ERROR 21/07/01 21:03:37 INFO BlockManager: BlockManager stopped[0m
[0m2021.07.01 21:03:37 ERROR 21/07/01 21:03:37 INFO BlockManagerMaster: BlockManagerMaster stopped[0m
[0m2021.07.01 21:03:37 ERROR 21/07/01 21:03:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped![0m
[0m2021.07.01 21:03:37 ERROR 21/07/01 21:03:37 INFO SparkContext: Successfully stopped SparkContext[0m
[0m2021.07.01 21:03:37 ERROR 21/07/01 21:03:37 INFO ShutdownHookManager: Shutdown hook called[0m
[0m2021.07.01 21:03:37 ERROR 21/07/01 21:03:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-7c27948e-4731-4fbd-aa87-67f60ab43c1d[0m
[0m2021.07.01 21:03:37 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 21:06:01 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 8h 10.452s)[0m
[0m2021.07.01 21:06:01 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 21:06:01 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 21:06:02 INFO  time: compiled ht3 in 1.08s[0m
[0m2021.07.01 21:06:02 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 21:06:02 INFO  time: compiled ht3-test in 0.14s[0m
[0m2021.07.01 21:06:03 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 21:06:02 INFO  Listening for transport dt_socket at address: 47039[0m
[0m2021.07.01 21:06:03 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 21:06:03 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 21:06:02 INFO  Trying to attach to remote debuggee VM localhost:47039 .[0m
[0m2021.07.01 21:06:02 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 21:06:04 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 21:06:04 ERROR 21/07/01 21:06:04 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 21:06:04 ERROR 21/07/01 21:06:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 21:06:05 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 21:06:05 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 21:06:05 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 21:06:05 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 21:06:05 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 21:06:05 ERROR 21/07/01 21:06:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 21:06:15 INFO  Data shape=(1.8286575E7, 169)[0m
[0m2021.07.01 21:07:39 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 21:07:39 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 21:07:39 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 21:07:39 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 21:07:39 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 21:07:39 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 21:07:39 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 21:07:39 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 21:07:39 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 21:07:39 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 21:07:39 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 21:07:39 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 21:07:39 INFO  only showing top 30 rows[0m
[0m2021.07.01 21:07:39 INFO  [0m
[0m2021.07.01 21:07:39 INFO  Pushed Setosa Record: {"SepalLength":1}[0m
[0m2021.07.01 21:07:40 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
Jul 01, 2021 10:15:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1755
[0m2021.07.01 22:15:55 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 22:15:55 INFO  time: compiled ht3-test in 0.21s[0m
[0m2021.07.01 22:15:55 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 22:15:55 INFO  time: compiled ht3-test in 0.12s[0m
[0m2021.07.01 22:16:51 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:16:51 INFO  time: compiled ht3 in 0.18s[0m
[0m2021.07.01 22:17:32 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:17:32 INFO  time: compiled ht3 in 0.21s[0m
[0m2021.07.01 22:17:54 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:17:54 INFO  time: compiled ht3 in 0.18s[0m
[0m2021.07.01 22:19:05 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:19:06 INFO  time: compiled ht3 in 1.17s[0m
[0m2021.07.01 22:19:18 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 22:19:18 INFO  time: compiled ht3-test in 0.57s[0m
[0m2021.07.01 22:19:24 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:19:24 INFO  time: compiled ht3 in 0.98s[0m
[0m2021.07.01 22:19:29 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 9h 13m 38.357s)[0m
[0m2021.07.01 22:19:29 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 22:19:29 INFO  time: compiled ht3-test in 0.19s[0m
[0m2021.07.01 22:19:29 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 22:19:29 INFO  Listening for transport dt_socket at address: 59815[0m
[0m2021.07.01 22:19:29 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 22:19:30 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:19:29 INFO  Trying to attach to remote debuggee VM localhost:59815 .[0m
[0m2021.07.01 22:19:29 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 22:19:31 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 22:19:31 ERROR 21/07/01 22:19:31 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 22:19:31 ERROR 21/07/01 22:19:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 22:19:32 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 22:19:32 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 22:19:32 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 22:19:32 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 22:19:32 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 22:19:32 ERROR 21/07/01 22:19:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 22:19:41 INFO  Data shape=(1.8286575E7, 169)[0m
[0m2021.07.01 22:20:30 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:20:30 INFO  Listening for transport dt_socket at address: 60927[0m
[0m2021.07.01 22:20:30 ERROR Exception in thread "main" org.apache.spark.SparkException: Job 3 cancelled because SparkContext was shut down[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1084)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1082)[0m
[0m2021.07.01 22:20:30 ERROR 	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1082)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2458)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2364)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2075)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1419)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.SparkContext.stop(SparkContext.scala:2075)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.SparkContext.$anonfun$new$37(SparkContext.scala:661)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)[0m
[0m2021.07.01 22:20:30 ERROR 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)[0m
[0m2021.07.01 22:20:30 ERROR 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m2021.07.01 22:20:30 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)[0m
[0m2021.07.01 22:20:30 ERROR 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)[0m
[0m2021.07.01 22:20:30 ERROR 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)[0m
[0m2021.07.01 22:20:30 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.01 22:20:30 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.01 22:20:30 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2297)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1120)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1102)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1524)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1512)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:183)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.sql.Dataset.take(Dataset.scala:2929)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.sql.Dataset.showString(Dataset.scala:338)[0m
[0m2021.07.01 22:20:30 ERROR 	at org.apache.spark.sql.Dataset.show(Dataset.scala:827)[0m
[0m2021.07.01 22:20:30 ERROR 	at ru.otus.bigdataml.ht3.DataProvider$.main(DataProvider.scala:50)[0m
[0m2021.07.01 22:20:30 ERROR 	at ru.otus.bigdataml.ht3.DataProvider.main(DataProvider.scala)[0m
[0m2021.07.01 22:20:42 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:20:42 INFO  time: compiled ht3 in 0.98s[0m
[0m2021.07.01 22:20:45 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 9h 14m 53.819s)[0m
[0m2021.07.01 22:20:45 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 22:20:45 INFO  time: compiled ht3-test in 0.23s[0m
[0m2021.07.01 22:20:45 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 22:20:45 INFO  Listening for transport dt_socket at address: 59023[0m
[0m2021.07.01 22:20:45 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 22:20:45 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:20:45 INFO  Trying to attach to remote debuggee VM localhost:59023 .[0m
[0m2021.07.01 22:20:45 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 22:20:46 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 22:20:46 ERROR 21/07/01 22:20:46 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 22:20:46 ERROR 21/07/01 22:20:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 22:20:47 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 22:20:47 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 22:20:47 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 22:20:47 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 22:20:47 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 22:20:47 ERROR 21/07/01 22:20:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 22:20:57 INFO  Data shape=(1.8286575E7, 169)[0m
[0m2021.07.01 22:21:48 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:21:48 INFO  Listening for transport dt_socket at address: 48835[0m
[0m2021.07.01 22:21:48 ERROR Exception in thread "main" org.apache.spark.SparkException: Job 3 cancelled because SparkContext was shut down[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1084)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1082)[0m
[0m2021.07.01 22:21:48 ERROR 	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1082)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2458)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2364)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2075)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1419)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.SparkContext.stop(SparkContext.scala:2075)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.SparkContext.$anonfun$new$37(SparkContext.scala:661)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)[0m
[0m2021.07.01 22:21:48 ERROR 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)[0m
[0m2021.07.01 22:21:48 ERROR 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m2021.07.01 22:21:48 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)[0m
[0m2021.07.01 22:21:48 ERROR 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)[0m
[0m2021.07.01 22:21:48 ERROR 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)[0m
[0m2021.07.01 22:21:48 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.01 22:21:48 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.01 22:21:48 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2297)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1120)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1102)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1524)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1512)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:183)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.sql.Dataset.take(Dataset.scala:2929)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.sql.Dataset.showString(Dataset.scala:338)[0m
[0m2021.07.01 22:21:48 ERROR 	at org.apache.spark.sql.Dataset.show(Dataset.scala:827)[0m
[0m2021.07.01 22:21:48 ERROR 	at ru.otus.bigdataml.ht3.DataProvider$.main(DataProvider.scala:51)[0m
[0m2021.07.01 22:21:48 ERROR 	at ru.otus.bigdataml.ht3.DataProvider.main(DataProvider.scala)[0m
[0m2021.07.01 22:23:46 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:23:47 INFO  time: compiled ht3 in 1.06s[0m
[0m2021.07.01 22:23:49 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 9h 17m 58.203s)[0m
[0m2021.07.01 22:23:49 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 22:23:49 INFO  time: compiled ht3-test in 0.19s[0m
[0m2021.07.01 22:23:49 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 22:23:49 INFO  Listening for transport dt_socket at address: 55401[0m
[0m2021.07.01 22:23:49 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 22:23:49 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:23:49 INFO  Trying to attach to remote debuggee VM localhost:55401 .[0m
[0m2021.07.01 22:23:49 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 22:23:51 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 22:23:51 ERROR 21/07/01 22:23:51 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 22:23:51 ERROR 21/07/01 22:23:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 22:23:52 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 22:23:52 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 22:23:52 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 22:23:52 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 22:23:52 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 22:23:52 ERROR 21/07/01 22:23:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 22:24:01 INFO  Data shape=(1.8286575E7, 169)[0m
[0m2021.07.01 22:25:27 INFO  +----------------+----------+-------------------+-------------------+---------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:25:27 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 22:25:27 INFO  +----------------+----------+-------------------+-------------------+---------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:25:27 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 22:25:27 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:25:27 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:25:27 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:25:27 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 22:25:27 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:25:27 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:25:27 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:25:27 INFO  +----------------+----------+-------------------+-------------------+---------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:25:27 INFO  only showing top 30 rows[0m
[0m2021.07.01 22:25:27 INFO  [0m
[0m2021.07.01 22:25:27 INFO  done[0m
[0m2021.07.01 22:31:01 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:31:03 INFO  time: compiled ht3 in 1.25s[0m
[0m2021.07.01 22:31:04 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 9h 25m 13.174s)[0m
[0m2021.07.01 22:31:04 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 22:31:04 INFO  time: compiled ht3-test in 0.21s[0m
[0m2021.07.01 22:31:04 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 22:31:04 INFO  Listening for transport dt_socket at address: 36973[0m
[0m2021.07.01 22:31:04 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 22:31:04 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:31:04 INFO  Trying to attach to remote debuggee VM localhost:36973 .[0m
[0m2021.07.01 22:31:04 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 22:31:05 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 22:31:05 ERROR 21/07/01 22:31:05 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 22:31:05 ERROR 21/07/01 22:31:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 22:31:06 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 22:31:06 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 22:31:06 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 22:31:06 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 22:31:06 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 22:31:06 ERROR 21/07/01 22:31:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 22:31:13 ERROR Exception in thread "main" 21/07/01 22:31:13 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.01 22:31:13 ERROR org.apache.spark.sql.AnalysisException: cannot resolve '(`date` = ((2018 - 2) - 1))' due to data type mismatch: differing types in '(`date` = ((2018 - 2) - 1))' (date and int).; line 1 pos 0;[0m
[0m2021.07.01 22:31:13 ERROR 'Filter (date#168 = ((2018 - 2) - 1))[0m
[0m2021.07.01 22:31:13 ERROR +- Relation[instanceId_userId#0,instanceId_objectType#1,instanceId_objectId#2,audit_pos#3L,audit_clientType#4,audit_timestamp#5L,audit_timePassed#6L,audit_experiment#7,audit_resourceType#8L,metadata_ownerId#9,metadata_ownerType#10,metadata_createdAt#11L,metadata_authorId#12,metadata_applicationId#13L,metadata_numCompanions#14,metadata_numPhotos#15,metadata_numPolls#16,metadata_numSymbols#17,metadata_numTokens#18,metadata_numVideos#19,metadata_platform#20,metadata_totalVideoLength#21,metadata_options#22,relationsMask#23L,... 145 more fields] parquet[0m
[0m2021.07.01 22:31:13 ERROR [0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:161)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:152)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:341)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)[0m
[0m2021.07.01 22:31:13 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:341)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:104)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:116)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:116)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:127)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:137)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:137)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:104)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:152)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:93)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:183)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:93)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:90)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:155)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:176)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:210)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:216)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3720)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1596)[0m
[0m2021.07.01 22:31:13 ERROR 	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1610)[0m
[0m2021.07.01 22:31:13 ERROR 	at ru.otus.bigdataml.ht3.DataProvider$.main(DataProvider.scala:34)[0m
[0m2021.07.01 22:31:13 ERROR 	at ru.otus.bigdataml.ht3.DataProvider.main(DataProvider.scala)[0m
[0m2021.07.01 22:31:14 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
Jul 01, 2021 10:31:14 PM org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer fireError
SEVERE: java.net.SocketException: Broken pipe (Write failed)
org.eclipse.lsp4j.jsonrpc.JsonRpcException: java.net.SocketException: Broken pipe (Write failed)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:72)
	at scala.meta.internal.metals.debug.SocketEndpoint.consume(SocketEndpoint.scala:22)
	at scala.meta.internal.metals.debug.MessageIdAdapter.consume(MessageIdAdapter.scala:43)
	at scala.meta.internal.metals.debug.ServerAdapter.send(ServerAdapter.scala:30)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleClientMessage$1(DebugProxy.scala:89)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToClient$1(DebugProxy.scala:54)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketException: Broken pipe (Write failed)
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)
	at java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:69)
	... 21 more

Jul 01, 2021 10:31:16 PM org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer fireError
SEVERE: java.net.SocketException: Broken pipe (Write failed)
org.eclipse.lsp4j.jsonrpc.JsonRpcException: java.net.SocketException: Broken pipe (Write failed)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:72)
	at scala.meta.internal.metals.debug.SocketEndpoint.consume(SocketEndpoint.scala:22)
	at scala.meta.internal.metals.debug.MessageIdAdapter.consume(MessageIdAdapter.scala:43)
	at scala.meta.internal.metals.debug.ServerAdapter.send(ServerAdapter.scala:30)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleClientMessage$1(DebugProxy.scala:89)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToClient$1(DebugProxy.scala:54)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketException: Broken pipe (Write failed)
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)
	at java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:69)
	... 21 more

[0m2021.07.01 22:31:17 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:31:19 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 9h 25m 28.73s)[0m
[0m2021.07.01 22:31:19 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 22:31:19 INFO  time: compiled ht3-test in 0.22s[0m
[0m2021.07.01 22:31:20 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 22:31:19 INFO  Listening for transport dt_socket at address: 46261[0m
[0m2021.07.01 22:31:20 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 22:31:20 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:31:19 INFO  Trying to attach to remote debuggee VM localhost:46261 .[0m
[0m2021.07.01 22:31:19 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 22:31:21 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 22:31:21 ERROR 21/07/01 22:31:21 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 22:31:21 ERROR 21/07/01 22:31:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 22:31:22 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 22:31:22 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 22:31:22 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 22:31:22 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 22:31:22 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 22:31:22 ERROR 21/07/01 22:31:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 22:31:29 ERROR Exception in thread "main" 21/07/01 22:31:29 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.01 22:31:29 ERROR org.apache.spark.sql.AnalysisException: cannot resolve '(`date` = ((2018 - 2) - 1))' due to data type mismatch: differing types in '(`date` = ((2018 - 2) - 1))' (date and int).; line 1 pos 0;[0m
[0m2021.07.01 22:31:29 ERROR 'Filter (date#168 = ((2018 - 2) - 1))[0m
[0m2021.07.01 22:31:29 ERROR +- Relation[instanceId_userId#0,instanceId_objectType#1,instanceId_objectId#2,audit_pos#3L,audit_clientType#4,audit_timestamp#5L,audit_timePassed#6L,audit_experiment#7,audit_resourceType#8L,metadata_ownerId#9,metadata_ownerType#10,metadata_createdAt#11L,metadata_authorId#12,metadata_applicationId#13L,metadata_numCompanions#14,metadata_numPhotos#15,metadata_numPolls#16,metadata_numSymbols#17,metadata_numTokens#18,metadata_numVideos#19,metadata_platform#20,metadata_totalVideoLength#21,metadata_options#22,relationsMask#23L,... 145 more fields] parquet[0m
[0m2021.07.01 22:31:29 ERROR [0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:161)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:152)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:341)[0m
[0m2021.07.01 22:31:29 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:341)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:104)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:116)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:116)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:127)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:137)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:137)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:104)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:152)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:93)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:183)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:93)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:90)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:155)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:176)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:210)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:216)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3720)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1596)[0m
[0m2021.07.01 22:31:29 ERROR 	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1610)[0m
[0m2021.07.01 22:31:29 ERROR 	at ru.otus.bigdataml.ht3.DataProvider$.main(DataProvider.scala:34)[0m
[0m2021.07.01 22:31:29 ERROR 	at ru.otus.bigdataml.ht3.DataProvider.main(DataProvider.scala)[0m
Jul 01, 2021 10:31:30 PM org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer fireError
SEVERE: java.net.SocketException: Broken pipe (Write failed)
org.eclipse.lsp4j.jsonrpc.JsonRpcException: java.net.SocketException: Broken pipe (Write failed)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:72)
	at scala.meta.internal.metals.debug.SocketEndpoint.consume(SocketEndpoint.scala:22)
	at scala.meta.internal.metals.debug.MessageIdAdapter.consume(MessageIdAdapter.scala:43)
	at scala.meta.internal.metals.debug.ServerAdapter.send(ServerAdapter.scala:30)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleClientMessage$1(DebugProxy.scala:89)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToClient$1(DebugProxy.scala:54)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketException: Broken pipe (Write failed)
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)
	at java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:69)
	... 21 more

Jul 01, 2021 10:31:31 PM org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer fireError
SEVERE: java.net.SocketException: Broken pipe (Write failed)
org.eclipse.lsp4j.jsonrpc.JsonRpcException: java.net.SocketException: Broken pipe (Write failed)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:72)
	at scala.meta.internal.metals.debug.SocketEndpoint.consume(SocketEndpoint.scala:22)
	at scala.meta.internal.metals.debug.MessageIdAdapter.consume(MessageIdAdapter.scala:43)
	at scala.meta.internal.metals.debug.ServerAdapter.send(ServerAdapter.scala:30)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleClientMessage$1(DebugProxy.scala:89)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToClient$1(DebugProxy.scala:54)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketException: Broken pipe (Write failed)
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)
	at java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:69)
	... 21 more

[0m2021.07.01 22:31:32 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:31:58 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:31:59 INFO  time: compiled ht3 in 1.04s[0m
Jul 01, 2021 10:32:01 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2525
[0m2021.07.01 22:32:01 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 9h 26m 10.695s)[0m
[0m2021.07.01 22:32:01 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 22:32:01 INFO  time: compiled ht3-test in 0.18s[0m
[0m2021.07.01 22:32:02 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 22:32:01 INFO  Listening for transport dt_socket at address: 51325[0m
[0m2021.07.01 22:32:02 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 22:32:02 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:32:01 INFO  Trying to attach to remote debuggee VM localhost:51325 .[0m
[0m2021.07.01 22:32:01 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 22:32:03 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 22:32:03 ERROR 21/07/01 22:32:03 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 22:32:03 ERROR 21/07/01 22:32:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 22:32:04 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 22:32:04 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 22:32:04 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 22:32:04 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 22:32:04 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 22:32:04 ERROR 21/07/01 22:32:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 22:32:12 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 22:32:15 INFO  +----------------+----------+-------------------+-------------------+---------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:32:15 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 22:32:15 INFO  +----------------+----------+-------------------+-------------------+---------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:32:15 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 22:32:15 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:32:15 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:32:15 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 22:32:15 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:32:15 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:32:15 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:32:15 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:32:15 INFO  +----------------+----------+-------------------+-------------------+---------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:32:15 INFO  only showing top 30 rows[0m
[0m2021.07.01 22:32:15 INFO  [0m
[0m2021.07.01 22:32:15 INFO  done[0m
[0m2021.07.01 22:32:16 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:32:51 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 9h 27m 0.214s)[0m
[0m2021.07.01 22:32:51 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:32:51 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:32:51 INFO  time: compiled ht3 in 0.95s[0m
[0m2021.07.01 22:32:52 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 22:32:52 INFO  time: compiled ht3-test in 0.34s[0m
[0m2021.07.01 22:32:52 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 22:32:52 INFO  Listening for transport dt_socket at address: 53593[0m
[0m2021.07.01 22:32:52 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 22:32:52 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:32:52 INFO  Trying to attach to remote debuggee VM localhost:53593 .[0m
[0m2021.07.01 22:32:52 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 22:32:54 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 22:32:54 ERROR 21/07/01 22:32:54 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 22:32:54 ERROR 21/07/01 22:32:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 22:32:55 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 22:32:55 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 22:32:55 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 22:32:55 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 22:32:55 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 22:32:55 ERROR 21/07/01 22:32:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 22:33:03 INFO  Data shape=(363326.0, 169)[0m
[0m2021.07.01 22:33:06 INFO  +----------------+----------+-------------------+-------------------+---------------+---------+-----------+-----------+-------------------+------+[0m
[0m2021.07.01 22:33:06 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|timeDelta|createdHour|auditedHour|feedback           |target|[0m
[0m2021.07.01 22:33:06 INFO  +----------------+----------+-------------------+-------------------+---------------+---------+-----------+-----------+-------------------+------+[0m
[0m2021.07.01 22:33:06 INFO  |79660           |2018-02-02|2018-01-29 23:36:02|2018-02-02 03:00:02|1517518802108  |3        |23         |3          |[Ignored]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |78936           |2018-02-02|2018-02-02 00:50:45|2018-02-02 03:00:02|1517518802197  |0        |0          |3          |[Liked]            |1     |[0m
[0m2021.07.01 22:33:06 INFO  |4034            |2018-02-02|2018-02-01 14:26:04|2018-02-02 03:00:02|1517518802530  |0        |14         |3          |[Ignored]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |57554           |2018-02-02|2018-02-01 20:51:16|2018-02-02 03:00:02|1517518802800  |0        |20         |3          |[Ignored]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |43210           |2018-02-02|2018-02-01 19:51:10|2018-02-02 03:00:02|1517518802891  |0        |19         |3          |[Clicked]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |6780            |2018-02-02|2018-01-31 19:56:17|2018-02-02 03:00:03|1517518803212  |1        |19         |3          |[Ignored]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |46135           |2018-02-02|2018-02-01 18:21:27|2018-02-02 03:00:04|1517518804486  |0        |18         |3          |[Disliked]         |0     |[0m
[0m2021.07.01 22:33:06 INFO  |49007           |2018-02-02|2018-02-02 01:27:44|2018-02-02 03:00:05|1517518805048  |0        |1          |3          |[Disliked]         |0     |[0m
[0m2021.07.01 22:33:06 INFO  |32533           |2018-02-02|2018-02-02 02:30:01|2018-02-02 03:00:06|1517518806490  |0        |2          |3          |[Ignored]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |14285           |2018-02-02|2018-01-31 11:45:54|2018-02-02 03:00:07|1517518807673  |1        |11         |3          |[Liked]            |1     |[0m
[0m2021.07.01 22:33:06 INFO  |28813           |2018-02-02|2018-01-31 15:01:04|2018-02-02 03:00:07|1517518807248  |1        |15         |3          |[Ignored]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |28193           |2018-02-02|2018-01-30 13:10:24|2018-02-02 03:00:07|1517518807554  |2        |13         |3          |[Ignored]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |18838           |2018-02-02|2018-01-31 15:02:46|2018-02-02 03:00:07|1517518807554  |1        |15         |3          |[Ignored]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |2165            |2018-02-02|2018-01-30 21:02:28|2018-02-02 03:00:07|1517518807526  |2        |21         |3          |[Clicked]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |22449           |2018-02-02|2018-02-01 22:35:58|2018-02-02 03:00:08|1517518808241  |0        |22         |3          |[Liked]            |1     |[0m
[0m2021.07.01 22:33:06 INFO  |17527           |2018-02-02|2018-01-30 03:00:08|2018-02-02 03:00:08|1517518808790  |3        |3          |3          |[Ignored]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |53876           |2018-02-02|2018-02-02 00:20:13|2018-02-02 03:00:08|1517518808391  |0        |0          |3          |[Ignored]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |23052           |2018-02-02|2018-02-01 10:17:17|2018-02-02 03:00:09|1517518809116  |0        |10         |3          |[Liked]            |1     |[0m
[0m2021.07.01 22:33:06 INFO  |37463           |2018-02-02|2018-02-01 07:36:16|2018-02-02 03:00:09|1517518809988  |0        |7          |3          |[Ignored]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |10949           |2018-02-02|2018-01-31 20:32:40|2018-02-02 03:00:10|1517518810139  |1        |20         |3          |[Clicked]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |16035           |2018-02-02|2018-02-02 01:01:24|2018-02-02 03:00:10|1517518810876  |0        |1          |3          |[Ignored]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |23821           |2018-02-02|2018-02-01 21:00:47|2018-02-02 03:00:10|1517518810999  |0        |21         |3          |[Liked]            |1     |[0m
[0m2021.07.01 22:33:06 INFO  |59479           |2018-02-02|2018-01-30 16:29:13|2018-02-02 03:00:10|1517518810816  |2        |16         |3          |[Ignored]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |78985           |2018-02-02|2017-10-17 17:05:12|2018-02-02 03:00:11|1517518811030  |107      |17         |3          |[Liked]            |1     |[0m
[0m2021.07.01 22:33:06 INFO  |79619           |2018-02-02|2018-02-01 12:00:28|2018-02-02 03:00:11|1517518811556  |0        |12         |3          |[Ignored]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |78644           |2018-02-02|2018-02-01 11:20:07|2018-02-02 03:00:11|1517518811824  |0        |11         |3          |[Liked]            |1     |[0m
[0m2021.07.01 22:33:06 INFO  |68917           |2018-02-02|2018-02-02 01:00:18|2018-02-02 03:00:11|1517518811960  |0        |1          |3          |[Ignored]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |27991           |2018-02-02|2018-02-02 02:22:23|2018-02-02 03:00:11|1517518811447  |0        |2          |3          |[Ignored]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |81540           |2018-02-02|2018-02-02 02:50:05|2018-02-02 03:00:11|1517518811595  |0        |2          |3          |[Ignored]          |0     |[0m
[0m2021.07.01 22:33:06 INFO  |27857           |2018-02-02|2018-02-01 21:50:03|2018-02-02 03:00:11|1517518811102  |0        |21         |3          |[Clicked, ReShared]|0     |[0m
[0m2021.07.01 22:33:06 INFO  +----------------+----------+-------------------+-------------------+---------------+---------+-----------+-----------+-------------------+------+[0m
[0m2021.07.01 22:33:06 INFO  only showing top 30 rows[0m
[0m2021.07.01 22:33:06 INFO  [0m
[0m2021.07.01 22:33:06 INFO  done[0m
[0m2021.07.01 22:33:06 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala in Array[String]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala, 490, 490, 503)
Exception in thread "pool-10-thread-1" java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[0m2021.07.01 22:38:34 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:38:34 INFO  time: compiled ht3 in 0.99s[0m
[0m2021.07.01 22:38:37 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 9h 32m 46.299s)[0m
[0m2021.07.01 22:38:37 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 22:38:37 INFO  time: compiled ht3-test in 0.23s[0m
[0m2021.07.01 22:38:37 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 22:38:37 INFO  Listening for transport dt_socket at address: 44473[0m
[0m2021.07.01 22:38:37 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 22:38:38 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:38:37 INFO  Trying to attach to remote debuggee VM localhost:44473 .[0m
[0m2021.07.01 22:38:37 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 22:38:39 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 22:38:39 ERROR 21/07/01 22:38:39 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 22:38:39 ERROR 21/07/01 22:38:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 22:38:40 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 22:38:40 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 22:38:40 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 22:38:40 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 22:38:40 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 22:38:40 ERROR 21/07/01 22:38:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 22:38:48 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 22:38:51 INFO  +----------------+----------+-------------------+-------------------+---------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:38:51 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 22:38:51 INFO  +----------------+----------+-------------------+-------------------+---------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:38:51 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 22:38:51 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:38:51 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:38:51 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 22:38:51 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:38:51 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:38:51 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:38:51 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:38:51 INFO  +----------------+----------+-------------------+-------------------+---------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:38:51 INFO  only showing top 30 rows[0m
[0m2021.07.01 22:38:51 INFO  [0m
[0m2021.07.01 22:38:51 INFO  done[0m
[0m2021.07.01 22:38:51 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:39:09 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:39:09 INFO  time: compiled ht3 in 0.93s[0m
[0m2021.07.01 22:39:11 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 9h 33m 19.809s)[0m
[0m2021.07.01 22:39:11 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 22:39:11 INFO  time: compiled ht3-test in 95ms[0m
[0m2021.07.01 22:39:11 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 22:39:11 INFO  Listening for transport dt_socket at address: 35511[0m
[0m2021.07.01 22:39:11 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 22:39:11 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:39:11 INFO  Trying to attach to remote debuggee VM localhost:35511 .[0m
[0m2021.07.01 22:39:11 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 22:39:12 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 22:39:12 ERROR 21/07/01 22:39:12 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 22:39:12 ERROR 21/07/01 22:39:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 22:39:13 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 22:39:13 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 22:39:13 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 22:39:13 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 22:39:13 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 22:39:13 ERROR 21/07/01 22:39:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 22:39:22 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 22:39:25 INFO  +----------------+----------+-------------------+-------------------+---------------+----------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:39:25 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|audit_unixtime  |timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 22:39:25 INFO  +----------------+----------+-------------------+-------------------+---------------+----------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:39:25 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1.517432400675E9|1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |1.517432400333E9|0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 22:39:25 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |1.517432400687E9|0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |1.517432401051E9|0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |1.517432402347E9|0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:39:25 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |1.51743240294E9 |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |1.5174324025E9  |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |1.517432403869E9|0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |1.517432403478E9|0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:39:25 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |1.517432403097E9|0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |1.517432404163E9|0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |1.517432404153E9|16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |1.517432405094E9|0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |1.517432405168E9|0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |1.51743240675E9 |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |1.51743240675E9 |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1.517432406042E9|1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |1.517432406755E9|0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |1.51743240786E9 |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |1.517432408358E9|0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 22:39:25 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |1.517432408331E9|0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |1.517432408061E9|2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1.517432408884E9|1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:39:25 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |1.517432409283E9|0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:39:25 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1.517432410836E9|1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |1.51743241003E9 |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |1.517432411277E9|2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |1.517432411986E9|725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1.51743241103E9 |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:39:25 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1.51743241259E9 |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:39:25 INFO  +----------------+----------+-------------------+-------------------+---------------+----------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:39:25 INFO  only showing top 30 rows[0m
[0m2021.07.01 22:39:25 INFO  [0m
[0m2021.07.01 22:39:25 INFO  done[0m
[0m2021.07.01 22:39:25 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:43:31 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:43:32 INFO  time: compiled ht3 in 1.13s[0m
[0m2021.07.01 22:43:36 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 9h 37m 45.65s)[0m
[0m2021.07.01 22:43:36 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 22:43:36 INFO  time: compiled ht3-test in 0.22s[0m
[0m2021.07.01 22:43:37 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 22:43:36 INFO  Listening for transport dt_socket at address: 59317[0m
[0m2021.07.01 22:43:37 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 22:43:37 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:43:36 INFO  Trying to attach to remote debuggee VM localhost:59317 .[0m
[0m2021.07.01 22:43:36 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 22:43:38 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 22:43:38 ERROR 21/07/01 22:43:38 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 22:43:38 ERROR 21/07/01 22:43:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 22:43:39 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 22:43:39 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 22:43:39 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 22:43:39 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 22:43:39 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 22:43:39 ERROR 21/07/01 22:43:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 22:43:47 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 22:43:50 INFO  +----------------+----------+-------------------+-------------------+---------------+----------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:43:50 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|audit_unixtime  |timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 22:43:50 INFO  +----------------+----------+-------------------+-------------------+---------------+----------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:43:50 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1.517432400675E9|1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |1.517432400333E9|0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 22:43:50 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |1.517432400687E9|0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |1.517432401051E9|0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |1.517432402347E9|0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:43:50 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |1.51743240294E9 |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |1.5174324025E9  |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |1.517432403869E9|0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |1.517432403478E9|0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:43:50 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |1.517432403097E9|0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |1.517432404163E9|0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |1.517432404153E9|16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |1.517432405094E9|0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |1.517432405168E9|0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |1.51743240675E9 |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |1.51743240675E9 |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1.517432406042E9|1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |1.517432406755E9|0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |1.51743240786E9 |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |1.517432408358E9|0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 22:43:50 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |1.517432408331E9|0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |1.517432408061E9|2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1.517432408884E9|1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:43:50 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |1.517432409283E9|0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:43:50 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1.517432410836E9|1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |1.51743241003E9 |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |1.517432411277E9|2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |1.517432411986E9|725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1.51743241103E9 |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:43:50 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1.51743241259E9 |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:43:50 INFO  +----------------+----------+-------------------+-------------------+---------------+----------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:43:50 INFO  only showing top 30 rows[0m
[0m2021.07.01 22:43:50 INFO  [0m
[0m2021.07.01 22:43:50 INFO  done[0m
[0m2021.07.01 22:43:50 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:45:34 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:45:35 INFO  time: compiled ht3 in 1.1s[0m
[0m2021.07.01 22:45:36 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 9h 39m 45.474s)[0m
[0m2021.07.01 22:45:36 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 22:45:36 INFO  time: compiled ht3-test in 0.19s[0m
[0m2021.07.01 22:45:36 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 22:45:36 INFO  Listening for transport dt_socket at address: 51143[0m
[0m2021.07.01 22:45:36 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 22:45:37 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:45:36 INFO  Trying to attach to remote debuggee VM localhost:51143 .[0m
[0m2021.07.01 22:45:36 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 22:45:38 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 22:45:38 ERROR 21/07/01 22:45:38 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 22:45:38 ERROR 21/07/01 22:45:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 22:45:39 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 22:45:39 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 22:45:39 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 22:45:39 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 22:45:39 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 22:45:39 ERROR 21/07/01 22:45:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 22:45:47 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 22:45:51 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:45:51 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|audit_unixtime|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 22:45:51 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:45:51 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1.517432401E9 |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |1.5174324E9   |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 22:45:51 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |1.517432401E9 |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |1.517432401E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |1.517432402E9 |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:45:51 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |1.517432403E9 |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |1.517432403E9 |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |1.517432404E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |1.517432403E9 |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:45:51 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |1.517432403E9 |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |1.517432404E9 |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |1.517432404E9 |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |1.517432405E9 |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |1.517432405E9 |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1.517432406E9 |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |1.517432407E9 |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |1.517432408E9 |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |1.517432408E9 |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 22:45:51 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |1.517432408E9 |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |1.517432408E9 |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1.517432409E9 |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:45:51 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |1.517432409E9 |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:45:51 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1.517432411E9 |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |1.51743241E9  |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |1.517432411E9 |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |1.517432412E9 |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1.517432411E9 |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:45:51 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1.517432413E9 |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:45:51 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:45:51 INFO  only showing top 30 rows[0m
[0m2021.07.01 22:45:51 INFO  [0m
[0m2021.07.01 22:45:51 INFO  done[0m
[0m2021.07.01 22:45:51 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:47:42 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:47:42 INFO  time: compiled ht3 in 0.94s[0m
[0m2021.07.01 22:47:44 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 9h 41m 53.536s)[0m
[0m2021.07.01 22:47:44 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 22:47:44 INFO  time: compiled ht3-test in 0.2s[0m
[0m2021.07.01 22:47:45 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 22:47:44 INFO  Listening for transport dt_socket at address: 59167[0m
[0m2021.07.01 22:47:45 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 22:47:45 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:47:44 INFO  Trying to attach to remote debuggee VM localhost:59167 .[0m
[0m2021.07.01 22:47:44 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 22:47:46 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 22:47:46 ERROR 21/07/01 22:47:46 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 22:47:46 ERROR 21/07/01 22:47:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 22:47:47 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 22:47:47 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 22:47:47 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 22:47:47 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 22:47:47 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 22:47:47 ERROR 21/07/01 22:47:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 22:47:55 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 22:47:59 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:47:59 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|audit_unixtime|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 22:47:59 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:47:59 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1.517432401E9 |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |1.5174324E9   |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 22:47:59 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |1.517432401E9 |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |1.517432401E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |1.517432402E9 |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:47:59 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |1.517432403E9 |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |1.517432403E9 |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |1.517432404E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |1.517432403E9 |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:47:59 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |1.517432403E9 |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |1.517432404E9 |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |1.517432404E9 |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |1.517432405E9 |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |1.517432405E9 |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1.517432406E9 |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |1.517432407E9 |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |1.517432408E9 |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |1.517432408E9 |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 22:47:59 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |1.517432408E9 |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |1.517432408E9 |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1.517432409E9 |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:47:59 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |1.517432409E9 |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:47:59 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1.517432411E9 |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |1.51743241E9  |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |1.517432411E9 |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |1.517432412E9 |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1.517432411E9 |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:47:59 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1.517432413E9 |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:47:59 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:47:59 INFO  only showing top 30 rows[0m
[0m2021.07.01 22:47:59 INFO  [0m
[0m2021.07.01 22:48:00 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+--------------------+------+[0m
[0m2021.07.01 22:48:00 INFO  |metadata_ownerId|      date|        createdTime|        auditedTime|timeDelta|createdHour|auditedHour|            feedback|target|[0m
[0m2021.07.01 22:48:00 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+--------------------+------+[0m
[0m2021.07.01 22:48:00 INFO  |           49367|2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|        0|          0|          3|[Clicked, Unliked...|     1|[0m
[0m2021.07.01 22:48:00 INFO  |           76851|2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|        1|         10|          3|           [Ignored]|     0|[0m
[0m2021.07.01 22:48:00 INFO  |           19357|2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|        0|          2|          3|           [Ignored]|     0|[0m
[0m2021.07.01 22:48:00 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+--------------------+------+[0m
[0m2021.07.01 22:48:00 INFO  [0m
[0m2021.07.01 22:48:00 INFO  done[0m
[0m2021.07.01 22:48:00 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:53:04 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:53:04 INFO  time: compiled ht3 in 0.97s[0m
[0m2021.07.01 22:53:06 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 9h 47m 15.509s)[0m
[0m2021.07.01 22:53:06 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 22:53:06 INFO  time: compiled ht3-test in 0.13s[0m
[0m2021.07.01 22:53:06 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 22:53:06 INFO  Listening for transport dt_socket at address: 57447[0m
[0m2021.07.01 22:53:06 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 22:53:07 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:53:06 INFO  Trying to attach to remote debuggee VM localhost:57447 .[0m
[0m2021.07.01 22:53:06 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 22:53:08 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 22:53:08 ERROR 21/07/01 22:53:08 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 22:53:08 ERROR 21/07/01 22:53:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 22:53:09 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 22:53:09 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 22:53:09 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 22:53:09 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 22:53:09 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 22:53:09 ERROR 21/07/01 22:53:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 22:53:17 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 22:53:21 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:53:21 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|audit_unixtime|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 22:53:21 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:53:21 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1.517432401E9 |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |1.5174324E9   |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 22:53:21 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |1.517432401E9 |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |1.517432401E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |1.517432402E9 |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:53:21 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |1.517432403E9 |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |1.517432403E9 |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |1.517432404E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |1.517432403E9 |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:53:21 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |1.517432403E9 |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |1.517432404E9 |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |1.517432404E9 |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |1.517432405E9 |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |1.517432405E9 |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1.517432406E9 |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |1.517432407E9 |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |1.517432408E9 |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |1.517432408E9 |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 22:53:21 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |1.517432408E9 |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |1.517432408E9 |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1.517432409E9 |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:53:21 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |1.517432409E9 |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:53:21 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1.517432411E9 |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |1.51743241E9  |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |1.517432411E9 |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |1.517432412E9 |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1.517432411E9 |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:53:21 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1.517432413E9 |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:53:21 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:53:21 INFO  only showing top 30 rows[0m
[0m2021.07.01 22:53:21 INFO  [0m
[0m2021.07.01 22:53:22 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+--------------------+------+[0m
[0m2021.07.01 22:53:22 INFO  |metadata_ownerId|      date|        createdTime|        auditedTime|timeDelta|createdHour|auditedHour|            feedback|target|[0m
[0m2021.07.01 22:53:22 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+--------------------+------+[0m
[0m2021.07.01 22:53:22 INFO  |           49367|2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|        0|          0|          3|[Clicked, Unliked...|     1|[0m
[0m2021.07.01 22:53:22 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+--------------------+------+[0m
[0m2021.07.01 22:53:22 INFO  [0m
[0m2021.07.01 22:53:22 INFO  done[0m
[0m2021.07.01 22:53:23 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:53:46 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:53:46 INFO  time: compiled ht3 in 0.24s[0m
[0m2021.07.01 22:53:48 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 9h 47m 57.055s)[0m
[0m2021.07.01 22:53:48 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:53:48 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:53:48 INFO  time: compiled ht3 in 0.17s[0m
[0m2021.07.01 22:53:48 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:53:48 INFO  time: compiled ht3 in 0.1s[0m
Jul 01, 2021 10:53:48 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
java.util.concurrent.CompletionException: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:29)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:26)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:237)
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:229)
	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417)
	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
	... 4 more

[0m2021.07.01 22:54:23 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 22:54:23 INFO  time: compiled ht3 in 0.96s[0m
[0m2021.07.01 22:54:26 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 9h 48m 35.219s)[0m
[0m2021.07.01 22:54:26 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 22:54:26 INFO  time: compiled ht3-test in 0.23s[0m
[0m2021.07.01 22:54:26 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 22:54:26 INFO  Listening for transport dt_socket at address: 48007[0m
[0m2021.07.01 22:54:26 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 22:54:26 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 22:54:26 INFO  Trying to attach to remote debuggee VM localhost:48007 .[0m
[0m2021.07.01 22:54:26 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 22:54:28 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 22:54:28 ERROR 21/07/01 22:54:28 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 22:54:28 ERROR 21/07/01 22:54:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 22:54:29 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 22:54:29 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 22:54:29 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 22:54:29 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 22:54:29 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 22:54:29 ERROR 21/07/01 22:54:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 22:54:37 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 22:54:40 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:54:40 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|audit_unixtime|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 22:54:40 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:54:40 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1517432400    |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |1517432400    |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 22:54:40 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |1517432400    |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |1517432401    |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |1517432402    |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:54:40 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |1517432402    |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |1517432402    |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |1517432403    |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |1517432403    |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 22:54:40 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |1517432403    |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |1517432404    |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |1517432404    |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |1517432405    |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |1517432405    |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |1517432406    |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |1517432406    |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1517432406    |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |1517432406    |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |1517432407    |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |1517432408    |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 22:54:40 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |1517432408    |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |1517432408    |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1517432408    |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:54:40 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |1517432409    |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:54:40 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1517432410    |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |1517432410    |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |1517432411    |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |1517432411    |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1517432411    |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 22:54:40 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1517432412    |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 22:54:40 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 22:54:40 INFO  only showing top 30 rows[0m
[0m2021.07.01 22:54:40 INFO  [0m
[0m2021.07.01 22:54:40 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+--------------------+------+[0m
[0m2021.07.01 22:54:40 INFO  |metadata_ownerId|      date|        createdTime|        auditedTime|timeDelta|createdHour|auditedHour|            feedback|target|[0m
[0m2021.07.01 22:54:40 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+--------------------+------+[0m
[0m2021.07.01 22:54:40 INFO  |           49367|2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|        0|          0|          3|[Clicked, Unliked...|     1|[0m
[0m2021.07.01 22:54:40 INFO  |           76851|2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|        1|         10|          3|           [Ignored]|     0|[0m
[0m2021.07.01 22:54:40 INFO  |           19357|2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|        0|          2|          3|           [Ignored]|     0|[0m
[0m2021.07.01 22:54:40 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+--------------------+------+[0m
[0m2021.07.01 22:54:40 INFO  [0m
[0m2021.07.01 22:54:40 INFO  done[0m
[0m2021.07.01 22:54:41 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:01:55 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 23:01:55 INFO  time: compiled ht3-test in 0.21s[0m
[0m2021.07.01 23:01:57 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:01:57 INFO  time: compiled ht3 in 0.17s[0m
[0m2021.07.01 23:03:48 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:03:48 INFO  time: compiled ht3 in 0.18s[0m
[0m2021.07.01 23:03:52 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:03:52 INFO  time: compiled ht3 in 0.93s[0m
[0m2021.07.01 23:03:57 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 9h 58m 5.995s)[0m
[0m2021.07.01 23:03:57 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:03:57 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:03:57 INFO  time: compiled ht3 in 0.97s[0m
[0m2021.07.01 23:03:58 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 23:03:58 INFO  time: compiled ht3-test in 0.17s[0m
[0m2021.07.01 23:03:58 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 23:03:58 INFO  Listening for transport dt_socket at address: 48675[0m
[0m2021.07.01 23:03:58 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 23:03:59 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:03:58 INFO  Trying to attach to remote debuggee VM localhost:48675 .[0m
[0m2021.07.01 23:03:58 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 23:04:00 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 23:04:00 ERROR 21/07/01 23:04:00 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 23:04:00 ERROR 21/07/01 23:04:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 23:04:01 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 23:04:01 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 23:04:01 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 23:04:01 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 23:04:01 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 23:04:01 ERROR 21/07/01 23:04:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 23:04:09 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 23:04:13 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:04:13 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|audit_unixtime|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 23:04:13 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:04:13 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1.517432401E9 |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |1.5174324E9   |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 23:04:13 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |1.517432401E9 |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |1.517432401E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |1.517432402E9 |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:04:13 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |1.517432403E9 |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |1.517432403E9 |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |1.517432404E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |1.517432403E9 |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:04:13 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |1.517432403E9 |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |1.517432404E9 |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |1.517432404E9 |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |1.517432405E9 |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |1.517432405E9 |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1.517432406E9 |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |1.517432407E9 |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |1.517432408E9 |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |1.517432408E9 |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 23:04:13 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |1.517432408E9 |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |1.517432408E9 |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1.517432409E9 |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:04:13 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |1.517432409E9 |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:04:13 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1.517432411E9 |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |1.51743241E9  |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |1.517432411E9 |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |1.517432412E9 |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1.517432411E9 |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:04:13 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1.517432413E9 |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:04:13 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:04:13 INFO  only showing top 30 rows[0m
[0m2021.07.01 23:04:13 INFO  [0m
[0m2021.07.01 23:04:13 INFO  {},ISO resolved to 2018-02-01T03:00[0m
[0m2021.07.01 23:04:13 INFO  +----------------+----+-----------+-----------+---------+-----------+-----------+--------+------+[0m
[0m2021.07.01 23:04:13 INFO  |metadata_ownerId|date|createdTime|auditedTime|timeDelta|createdHour|auditedHour|feedback|target|[0m
[0m2021.07.01 23:04:13 INFO  +----------------+----+-----------+-----------+---------+-----------+-----------+--------+------+[0m
[0m2021.07.01 23:04:13 INFO  +----------------+----+-----------+-----------+---------+-----------+-----------+--------+------+[0m
[0m2021.07.01 23:04:13 INFO  [0m
[0m2021.07.01 23:04:13 INFO  done[0m
[0m2021.07.01 23:04:14 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:04:40 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 23:04:40 INFO  time: compiled ht3-test in 0.18s[0m
[0m2021.07.01 23:04:49 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:04:49 INFO  time: compiled ht3 in 0.21s[0m
[0m2021.07.01 23:05:29 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala:60:16: stale bloop error: overloaded method value where with alternatives:
  (conditionExpr: String)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] <and>
  (condition: org.apache.spark.sql.Column)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
 cannot be applied to (Boolean)
> dfRaw
>             .select("metadata_ownerId", "date", "createdTime", "auditedTime", "timeDelta", "createdHour", "auditedHour", "feedback", "target")
>             .orderBy("auditedTime")
>             .where[0m
[0m2021.07.01 23:05:29 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala:60:16: stale bloop error: overloaded method value where with alternatives:
  (conditionExpr: String)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] <and>
  (condition: org.apache.spark.sql.Column)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
 cannot be applied to (Boolean)
> dfRaw
>             .select("metadata_ownerId", "date", "createdTime", "auditedTime", "timeDelta", "createdHour", "auditedHour", "feedback", "target")
>             .orderBy("auditedTime")
>             .where[0m
[0m2021.07.01 23:05:29 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala:60:16: stale bloop error: overloaded method value where with alternatives:
  (conditionExpr: String)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] <and>
  (condition: org.apache.spark.sql.Column)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
 cannot be applied to (Boolean)
> dfRaw
>             .select("metadata_ownerId", "date", "createdTime", "auditedTime", "timeDelta", "createdHour", "auditedHour", "feedback", "target")
>             .orderBy("auditedTime")
>             .where[0m
[0m2021.07.01 23:05:29 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala:60:16: stale bloop error: overloaded method value where with alternatives:
  (conditionExpr: String)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] <and>
  (condition: org.apache.spark.sql.Column)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
 cannot be applied to (Boolean)
> dfRaw
>             .select("metadata_ownerId", "date", "createdTime", "auditedTime", "timeDelta", "createdHour", "auditedHour", "feedback", "target")
>             .orderBy("auditedTime")
>             .where[0m
[0m2021.07.01 23:11:25 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 23:11:25 INFO  time: compiled ht3-test in 0.18s[0m
[0m2021.07.01 23:11:31 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:11:31 INFO  time: compiled ht3 in 0.94s[0m
[0m2021.07.01 23:11:34 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 10h 5m 43.589s)[0m
[0m2021.07.01 23:11:34 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 23:11:34 INFO  time: compiled ht3-test in 0.17s[0m
[0m2021.07.01 23:11:35 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 23:11:34 INFO  Listening for transport dt_socket at address: 33365[0m
[0m2021.07.01 23:11:35 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 23:11:35 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:11:34 INFO  Trying to attach to remote debuggee VM localhost:33365 .[0m
[0m2021.07.01 23:11:34 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 23:11:36 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 23:11:36 ERROR 21/07/01 23:11:36 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 23:11:36 ERROR 21/07/01 23:11:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 23:11:37 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 23:11:37 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 23:11:37 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 23:11:37 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 23:11:37 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 23:11:37 ERROR 21/07/01 23:11:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 23:11:45 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 23:11:49 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:11:49 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|audit_unixtime|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 23:11:49 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:11:49 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1.517432401E9 |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |1.5174324E9   |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 23:11:49 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |1.517432401E9 |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |1.517432401E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |1.517432402E9 |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:11:49 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |1.517432403E9 |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |1.517432403E9 |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |1.517432404E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |1.517432403E9 |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:11:49 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |1.517432403E9 |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |1.517432404E9 |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |1.517432404E9 |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |1.517432405E9 |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |1.517432405E9 |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1.517432406E9 |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |1.517432407E9 |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |1.517432408E9 |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |1.517432408E9 |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 23:11:49 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |1.517432408E9 |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |1.517432408E9 |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1.517432409E9 |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:11:49 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |1.517432409E9 |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:11:49 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1.517432411E9 |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |1.51743241E9  |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |1.517432411E9 |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |1.517432412E9 |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1.517432411E9 |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:11:49 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1.517432413E9 |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:11:49 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:11:49 INFO  only showing top 30 rows[0m
[0m2021.07.01 23:11:49 INFO  [0m
[0m2021.07.01 23:11:49 INFO  2018-02-01 03:00:00[0m
[0m2021.07.01 23:11:49 ERROR Exception in thread "main" org.apache.spark.sql.catalyst.parser.ParseException: [0m
[0m2021.07.01 23:11:49 ERROR extraneous input '{' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 17)[0m
[0m2021.07.01 23:11:49 ERROR [0m
[0m2021.07.01 23:11:49 ERROR == SQL ==[0m
[0m2021.07.01 23:11:49 ERROR audit_timestamp=={datetime_format.}[0m
[0m2021.07.01 23:11:49 ERROR -----------------^^^[0m
[0m2021.07.01 23:11:49 ERROR unexpected error during source scanning
java.lang.RuntimeException: invalid symbol format
#
^
	at scala.sys.package$.error(package.scala:30)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.fail(Scala.scala:200)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.readChar(Scala.scala:213)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseValue(Scala.scala:231)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.parseDescriptor(Scala.scala:256)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser.entryPoint(Scala.scala:279)
	at scala.meta.internal.semanticdb.Scala$DescriptorParser$.apply(Scala.scala:287)
	at scala.meta.internal.semanticdb.Scala$ScalaSymbolOps.owner(Scala.scala:101)
	at scala.meta.internal.mtags.Symbol.toplevel(Symbol.scala:56)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.findSymbolDefinition(OnDemandSymbolIndex.scala:203)
	at scala.meta.internal.mtags.OnDemandSymbolIndex.definition(OnDemandSymbolIndex.scala:54)
	at scala.meta.internal.metals.DestinationProvider.fromSymbol(DefinitionProvider.scala:259)
	at scala.meta.internal.metals.DefinitionProvider.fromSymbol(DefinitionProvider.scala:97)
	at scala.meta.internal.metals.StacktraceAnalyzer.findLocationForSymbol$1(StacktraceAnalyzer.scala:71)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$2(StacktraceAnalyzer.scala:76)
	at scala.PartialFunction$Unlifted.applyOrElse(PartialFunction.scala:237)
	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)
	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)
	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)
	at scala.meta.internal.metals.StacktraceAnalyzer.$anonfun$fileLocationFromLine$1(StacktraceAnalyzer.scala:76)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.metals.StacktraceAnalyzer.fileLocationFromLine(StacktraceAnalyzer.scala:74)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1(DebugProxy.scala:117)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleServerMessage$1$adapted(DebugProxy.scala:92)
	at scala.meta.internal.metals.debug.ServerAdapter.$anonfun$onReceived$1(ServerAdapter.scala:25)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.ServerAdapter.onReceived(ServerAdapter.scala:18)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToServer$1(DebugProxy.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:829)
[0m
[0m2021.07.01 23:11:49 ERROR [0m
[0m2021.07.01 23:11:49 ERROR 	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:255)[0m
[0m2021.07.01 23:11:49 ERROR 	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:124)[0m
[0m2021.07.01 23:11:49 ERROR 	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:49)[0m
[0m2021.07.01 23:11:49 ERROR 	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseExpression(ParseDriver.scala:43)[0m
[0m2021.07.01 23:11:49 ERROR 	at org.apache.spark.sql.Dataset.where(Dataset.scala:1636)[0m
[0m2021.07.01 23:11:49 ERROR 	at ru.otus.bigdataml.ht3.DataProvider$.main(DataProvider.scala:63)[0m
[0m2021.07.01 23:11:49 ERROR 	at ru.otus.bigdataml.ht3.DataProvider.main(DataProvider.scala)[0m
[0m2021.07.01 23:11:49 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:12:14 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:12:15 INFO  time: compiled ht3 in 1.08s[0m
[0m2021.07.01 23:12:18 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 10h 6m 27.508s)[0m
[0m2021.07.01 23:12:18 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 23:12:18 INFO  time: compiled ht3-test in 0.19s[0m
[0m2021.07.01 23:12:19 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 23:12:18 INFO  Listening for transport dt_socket at address: 48905[0m
[0m2021.07.01 23:12:19 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 23:12:19 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:12:18 INFO  Trying to attach to remote debuggee VM localhost:48905 .[0m
[0m2021.07.01 23:12:18 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 23:12:20 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 23:12:20 ERROR 21/07/01 23:12:20 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 23:12:20 ERROR 21/07/01 23:12:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 23:12:21 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 23:12:21 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 23:12:21 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 23:12:21 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 23:12:21 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 23:12:21 ERROR 21/07/01 23:12:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 23:12:29 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 23:12:33 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:12:33 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|audit_unixtime|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 23:12:33 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:12:33 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1.517432401E9 |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |1.5174324E9   |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 23:12:33 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |1.517432401E9 |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |1.517432401E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |1.517432402E9 |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:12:33 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |1.517432403E9 |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |1.517432403E9 |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |1.517432404E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |1.517432403E9 |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:12:33 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |1.517432403E9 |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |1.517432404E9 |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |1.517432404E9 |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |1.517432405E9 |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |1.517432405E9 |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1.517432406E9 |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |1.517432407E9 |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |1.517432408E9 |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |1.517432408E9 |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 23:12:33 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |1.517432408E9 |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |1.517432408E9 |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1.517432409E9 |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:12:33 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |1.517432409E9 |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:12:33 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1.517432411E9 |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |1.51743241E9  |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |1.517432411E9 |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |1.517432412E9 |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1.517432411E9 |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:12:33 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1.517432413E9 |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:12:33 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:12:33 INFO  only showing top 30 rows[0m
[0m2021.07.01 23:12:33 INFO  [0m
[0m2021.07.01 23:12:33 INFO  2018-02-01 03:00:00[0m
[0m2021.07.01 23:12:33 INFO  +----------------+----+-----------+-----------+---------+-----------+-----------+--------+------+[0m
[0m2021.07.01 23:12:33 INFO  |metadata_ownerId|date|createdTime|auditedTime|timeDelta|createdHour|auditedHour|feedback|target|[0m
[0m2021.07.01 23:12:33 INFO  +----------------+----+-----------+-----------+---------+-----------+-----------+--------+------+[0m
[0m2021.07.01 23:12:33 INFO  +----------------+----+-----------+-----------+---------+-----------+-----------+--------+------+[0m
[0m2021.07.01 23:12:33 INFO  [0m
[0m2021.07.01 23:12:33 INFO  done[0m
[0m2021.07.01 23:12:33 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:13:03 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:13:03 INFO  time: compiled ht3 in 0.17s[0m
Exception in thread "pool-10-thread-2" java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[0m2021.07.01 23:19:52 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:19:52 INFO  time: compiled ht3 in 0.19s[0m
[0m2021.07.01 23:20:14 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:20:14 INFO  time: compiled ht3 in 0.17s[0m
[0m2021.07.01 23:20:19 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:20:20 INFO  time: compiled ht3 in 1.04s[0m
[0m2021.07.01 23:20:24 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 10h 14m 33.344s)[0m
[0m2021.07.01 23:20:24 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 23:20:24 INFO  time: compiled ht3-test in 0.5s[0m
[0m2021.07.01 23:20:25 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 23:20:24 INFO  Listening for transport dt_socket at address: 37059[0m
[0m2021.07.01 23:20:25 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 23:20:25 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:20:24 INFO  Trying to attach to remote debuggee VM localhost:37059 .[0m
[0m2021.07.01 23:20:24 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 23:20:26 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 23:20:26 ERROR 21/07/01 23:20:26 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 23:20:26 ERROR 21/07/01 23:20:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 23:20:27 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 23:20:27 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 23:20:27 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 23:20:27 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 23:20:27 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 23:20:27 ERROR 21/07/01 23:20:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 23:20:35 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 23:20:39 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:20:39 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|audit_unixtime|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 23:20:39 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:20:39 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1.517432401E9 |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |1.5174324E9   |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 23:20:39 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |1.517432401E9 |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |1.517432401E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |1.517432402E9 |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:20:39 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |1.517432403E9 |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |1.517432403E9 |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |1.517432404E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |1.517432403E9 |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:20:39 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |1.517432403E9 |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |1.517432404E9 |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |1.517432404E9 |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |1.517432405E9 |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |1.517432405E9 |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1.517432406E9 |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |1.517432407E9 |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |1.517432408E9 |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |1.517432408E9 |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 23:20:39 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |1.517432408E9 |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |1.517432408E9 |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1.517432409E9 |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:20:39 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |1.517432409E9 |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:20:39 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1.517432411E9 |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |1.51743241E9  |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |1.517432411E9 |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |1.517432412E9 |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1.517432411E9 |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:20:39 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1.517432413E9 |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:20:39 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:20:39 INFO  only showing top 30 rows[0m
[0m2021.07.01 23:20:39 INFO  [0m
[0m2021.07.01 23:20:39 INFO  2018-02-01 03:00:15[0m
[0m2021.07.01 23:20:39 INFO  +----------------+----+-----------+-----------+---------+-----------+-----------+--------+------+[0m
[0m2021.07.01 23:20:39 INFO  |metadata_ownerId|date|createdTime|auditedTime|timeDelta|createdHour|auditedHour|feedback|target|[0m
[0m2021.07.01 23:20:39 INFO  +----------------+----+-----------+-----------+---------+-----------+-----------+--------+------+[0m
[0m2021.07.01 23:20:39 INFO  +----------------+----+-----------+-----------+---------+-----------+-----------+--------+------+[0m
[0m2021.07.01 23:20:39 INFO  [0m
[0m2021.07.01 23:20:39 INFO  done[0m
[0m2021.07.01 23:20:40 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:22:54 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:22:54 INFO  time: compiled ht3 in 0.95s[0m
[0m2021.07.01 23:23:10 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:23:10 INFO  time: compiled ht3 in 0.98s[0m
[0m2021.07.01 23:23:13 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 10h 17m 22.156s)[0m
[0m2021.07.01 23:23:13 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 23:23:13 INFO  time: compiled ht3-test in 0.37s[0m
[0m2021.07.01 23:23:13 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 23:23:13 INFO  Listening for transport dt_socket at address: 46009[0m
[0m2021.07.01 23:23:13 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 23:23:14 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:23:13 INFO  Trying to attach to remote debuggee VM localhost:46009 .[0m
[0m2021.07.01 23:23:13 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 23:23:15 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 23:23:15 ERROR 21/07/01 23:23:15 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 23:23:15 ERROR 21/07/01 23:23:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 23:23:16 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 23:23:16 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 23:23:16 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 23:23:16 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 23:23:16 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 23:23:16 ERROR 21/07/01 23:23:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 23:23:24 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 23:23:29 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:23:29 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|audit_unixtime|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 23:23:29 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:23:29 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1.517432401E9 |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |1.5174324E9   |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 23:23:29 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |1.517432401E9 |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |1.517432401E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |1.517432402E9 |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:23:29 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |1.517432403E9 |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |1.517432403E9 |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |1.517432404E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |1.517432403E9 |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:23:29 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |1.517432403E9 |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |1.517432404E9 |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |1.517432404E9 |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |1.517432405E9 |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |1.517432405E9 |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1.517432406E9 |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |1.517432407E9 |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |1.517432408E9 |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |1.517432408E9 |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 23:23:29 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |1.517432408E9 |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |1.517432408E9 |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1.517432409E9 |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:23:29 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |1.517432409E9 |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:23:29 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1.517432411E9 |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |1.51743241E9  |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |1.517432411E9 |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |1.517432412E9 |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1.517432411E9 |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:23:29 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1.517432413E9 |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:23:29 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:23:29 INFO  only showing top 30 rows[0m
[0m2021.07.01 23:23:29 INFO  [0m
[0m2021.07.01 23:23:29 INFO  2018-02-01 03:00:01[0m
[0m2021.07.01 23:23:30 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+---------+------+[0m
[0m2021.07.01 23:23:30 INFO  |metadata_ownerId|      date|        createdTime|        auditedTime|timeDelta|createdHour|auditedHour| feedback|target|[0m
[0m2021.07.01 23:23:30 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+---------+------+[0m
[0m2021.07.01 23:23:30 INFO  |           77159|2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|        0|          1|          3|[Ignored]|     0|[0m
[0m2021.07.01 23:23:30 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+---------+------+[0m
[0m2021.07.01 23:23:30 INFO  [0m
[0m2021.07.01 23:23:30 INFO  done[0m
[0m2021.07.01 23:23:30 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:24:07 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:24:08 INFO  time: compiled ht3 in 1.31s[0m
[0m2021.07.01 23:24:08 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 10h 18m 17.802s)[0m
[0m2021.07.01 23:24:08 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 23:24:08 INFO  time: compiled ht3-test in 0.13s[0m
[0m2021.07.01 23:24:09 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 23:24:08 INFO  Listening for transport dt_socket at address: 56933[0m
[0m2021.07.01 23:24:09 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 23:24:09 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:24:08 INFO  Trying to attach to remote debuggee VM localhost:56933 .[0m
[0m2021.07.01 23:24:08 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 23:24:10 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 23:24:10 ERROR 21/07/01 23:24:10 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 23:24:10 ERROR 21/07/01 23:24:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 23:24:11 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 23:24:11 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 23:24:11 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 23:24:11 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 23:24:11 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 23:24:11 ERROR 21/07/01 23:24:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 23:24:19 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 23:24:23 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:24:23 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|audit_unixtime|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 23:24:23 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:24:23 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1.517432401E9 |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |1.5174324E9   |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 23:24:23 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |1.517432401E9 |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |1.517432401E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |1.517432402E9 |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:24:23 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |1.517432403E9 |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |1.517432403E9 |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |1.517432404E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |1.517432403E9 |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:24:23 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |1.517432403E9 |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |1.517432404E9 |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |1.517432404E9 |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |1.517432405E9 |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |1.517432405E9 |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1.517432406E9 |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |1.517432407E9 |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |1.517432408E9 |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |1.517432408E9 |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 23:24:23 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |1.517432408E9 |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |1.517432408E9 |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1.517432409E9 |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:24:23 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |1.517432409E9 |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:24:23 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1.517432411E9 |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |1.51743241E9  |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |1.517432411E9 |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |1.517432412E9 |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1.517432411E9 |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:24:23 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1.517432413E9 |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:24:23 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:24:23 INFO  only showing top 30 rows[0m
[0m2021.07.01 23:24:23 INFO  [0m
[0m2021.07.01 23:24:23 INFO  2018-02-01 03:00:02[0m
[0m2021.07.01 23:24:24 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:24:24 INFO  |metadata_ownerId|      date|        createdTime|        auditedTime|timeDelta|createdHour|auditedHour|  feedback|target|[0m
[0m2021.07.01 23:24:24 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:24:24 INFO  |           25161|2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|        0|          0|          3|[Disliked]|     0|[0m
[0m2021.07.01 23:24:24 INFO  |           80203|2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|        2|         20|          3| [Clicked]|     0|[0m
[0m2021.07.01 23:24:24 INFO  |           65675|2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|        8|         15|          3| [Ignored]|     0|[0m
[0m2021.07.01 23:24:24 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:24:24 INFO  [0m
[0m2021.07.01 23:24:24 INFO  done[0m
[0m2021.07.01 23:24:25 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:25:05 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:25:06 INFO  time: compiled ht3 in 1.05s[0m
[0m2021.07.01 23:25:09 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 10h 19m 18.061s)[0m
[0m2021.07.01 23:25:09 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 23:25:09 INFO  time: compiled ht3-test in 0.2s[0m
[0m2021.07.01 23:25:09 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 23:25:09 INFO  Listening for transport dt_socket at address: 33893[0m
[0m2021.07.01 23:25:09 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 23:25:09 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:25:09 INFO  Trying to attach to remote debuggee VM localhost:33893 .[0m
[0m2021.07.01 23:25:09 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 23:25:10 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 23:25:10 ERROR 21/07/01 23:25:10 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 23:25:10 ERROR 21/07/01 23:25:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 23:25:12 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 23:25:12 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 23:25:12 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 23:25:12 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 23:25:12 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 23:25:12 ERROR 21/07/01 23:25:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 23:25:20 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 23:25:24 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:25:24 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|audit_unixtime|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 23:25:24 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:25:24 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1.517432401E9 |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |1.5174324E9   |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 23:25:24 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |1.517432401E9 |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |1.517432401E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |1.517432402E9 |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:25:24 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |1.517432403E9 |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |1.517432403E9 |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |1.517432404E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |1.517432403E9 |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:25:24 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |1.517432403E9 |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |1.517432404E9 |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |1.517432404E9 |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |1.517432405E9 |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |1.517432405E9 |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1.517432406E9 |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |1.517432407E9 |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |1.517432408E9 |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |1.517432408E9 |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 23:25:24 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |1.517432408E9 |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |1.517432408E9 |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1.517432409E9 |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:25:24 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |1.517432409E9 |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:25:24 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1.517432411E9 |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |1.51743241E9  |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |1.517432411E9 |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |1.517432412E9 |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1.517432411E9 |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:25:24 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1.517432413E9 |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:25:24 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:25:24 INFO  only showing top 30 rows[0m
[0m2021.07.01 23:25:24 INFO  [0m
[0m2021.07.01 23:25:24 INFO  2018-02-01 03:00:02[0m
[0m2021.07.01 23:25:25 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:25:25 INFO  |metadata_ownerId|      date|        createdTime|        auditedTime|timeDelta|createdHour|auditedHour|  feedback|target|[0m
[0m2021.07.01 23:25:25 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:25:25 INFO  |           25161|2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|        0|          0|          3|[Disliked]|     0|[0m
[0m2021.07.01 23:25:25 INFO  |           80203|2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|        2|         20|          3| [Clicked]|     0|[0m
[0m2021.07.01 23:25:25 INFO  |           65675|2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|        8|         15|          3| [Ignored]|     0|[0m
[0m2021.07.01 23:25:25 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:25:25 INFO  [0m
[0m2021.07.01 23:25:25 INFO  done[0m
[0m2021.07.01 23:25:25 ERROR Exception in thread "main" org.apache.spark.sql.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame[0m
[0m2021.07.01 23:25:25 ERROR 	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[0m2021.07.01 23:25:25 ERROR 	at org.apache.spark.sql.Dataset.writeStream(Dataset.scala:3412)[0m
[0m2021.07.01 23:25:25 ERROR 	at ru.otus.bigdataml.ht3.DataProvider$.main(DataProvider.scala:78)[0m
[0m2021.07.01 23:25:25 ERROR 	at ru.otus.bigdataml.ht3.DataProvider.main(DataProvider.scala)[0m
[0m2021.07.01 23:25:26 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:32:52 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:32:53 INFO  time: compiled ht3 in 1.13s[0m
[0m2021.07.01 23:32:56 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 10h 27m 5.769s)[0m
[0m2021.07.01 23:32:56 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 23:32:56 INFO  time: compiled ht3-test in 0.2s[0m
[0m2021.07.01 23:32:57 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 23:32:56 INFO  Listening for transport dt_socket at address: 56729[0m
[0m2021.07.01 23:32:57 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 23:32:57 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:32:56 INFO  Trying to attach to remote debuggee VM localhost:56729 .[0m
[0m2021.07.01 23:32:56 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 23:32:58 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 23:32:58 ERROR 21/07/01 23:32:58 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 23:32:58 ERROR 21/07/01 23:32:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 23:32:59 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 23:32:59 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 23:32:59 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 23:32:59 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 23:32:59 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 23:32:59 ERROR 21/07/01 23:33:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 23:33:07 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 23:33:12 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:33:12 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|audit_unixtime|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 23:33:12 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:33:12 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1.517432401E9 |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |1.5174324E9   |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 23:33:12 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |1.517432401E9 |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |1.517432401E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |1.517432402E9 |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:33:12 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |1.517432403E9 |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |1.517432403E9 |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |1.517432404E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |1.517432403E9 |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:33:12 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |1.517432403E9 |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |1.517432404E9 |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |1.517432404E9 |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |1.517432405E9 |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |1.517432405E9 |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1.517432406E9 |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |1.517432407E9 |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |1.517432408E9 |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |1.517432408E9 |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 23:33:12 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |1.517432408E9 |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |1.517432408E9 |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1.517432409E9 |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:33:12 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |1.517432409E9 |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:33:12 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1.517432411E9 |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |1.51743241E9  |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |1.517432411E9 |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |1.517432412E9 |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1.517432411E9 |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:33:12 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1.517432413E9 |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:33:12 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:33:12 INFO  only showing top 30 rows[0m
[0m2021.07.01 23:33:12 INFO  [0m
[0m2021.07.01 23:33:12 INFO  2018-02-01 03:00:02[0m
[0m2021.07.01 23:33:13 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:33:13 INFO  |metadata_ownerId|      date|        createdTime|        auditedTime|timeDelta|createdHour|auditedHour|  feedback|target|[0m
[0m2021.07.01 23:33:13 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:33:13 INFO  |           25161|2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|        0|          0|          3|[Disliked]|     0|[0m
[0m2021.07.01 23:33:13 INFO  |           80203|2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|        2|         20|          3| [Clicked]|     0|[0m
[0m2021.07.01 23:33:13 INFO  |           65675|2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|        8|         15|          3| [Ignored]|     0|[0m
[0m2021.07.01 23:33:13 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:33:13 INFO  [0m
[0m2021.07.01 23:33:13 INFO  [value: string][0m
[0m2021.07.01 23:33:13 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:38:53 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:38:54 INFO  time: compiled ht3 in 1.25s[0m
[0m2021.07.01 23:38:54 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 10h 33m 4.009s)[0m
[0m2021.07.01 23:38:54 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:38:54 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:38:54 INFO  time: compiled ht3 in 0.11s[0m
[0m2021.07.01 23:38:54 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:38:54 INFO  time: compiled ht3 in 0.11s[0m
Jul 01, 2021 11:38:55 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
java.util.concurrent.CompletionException: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:29)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:26)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:237)
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:229)
	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417)
	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
	... 4 more

[0m2021.07.01 23:42:27 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:42:27 INFO  time: compiled ht3 in 0.19s[0m
[0m2021.07.01 23:42:41 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:42:41 INFO  time: compiled ht3 in 0.17s[0m
[0m2021.07.01 23:42:46 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:42:46 INFO  time: compiled ht3 in 0.2s[0m
[0m2021.07.01 23:43:40 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:43:42 INFO  time: compiled ht3 in 1.88s[0m
[0m2021.07.01 23:43:43 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 10h 37m 52.195s)[0m
[0m2021.07.01 23:43:43 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 23:43:44 INFO  time: compiled ht3-test in 1.24s[0m
[0m2021.07.01 23:43:44 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 23:43:44 INFO  Listening for transport dt_socket at address: 53421[0m
[0m2021.07.01 23:43:44 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 23:43:45 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:43:44 INFO  Trying to attach to remote debuggee VM localhost:53421 .[0m
[0m2021.07.01 23:43:44 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 23:43:46 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 23:43:46 ERROR 21/07/01 23:43:46 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 23:43:46 ERROR 21/07/01 23:43:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 23:43:47 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 23:43:47 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 23:43:47 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 23:43:47 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 23:43:47 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 23:43:47 ERROR 21/07/01 23:43:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 23:43:56 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 23:44:00 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:44:00 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|audit_unixtime|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 23:44:00 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:44:00 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1.517432401E9 |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |1.5174324E9   |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 23:44:00 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |1.517432401E9 |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |1.517432401E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |1.517432402E9 |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:44:00 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |1.517432403E9 |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |1.517432403E9 |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |1.517432404E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |1.517432403E9 |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:44:00 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |1.517432403E9 |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |1.517432404E9 |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |1.517432404E9 |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |1.517432405E9 |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |1.517432405E9 |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1.517432406E9 |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |1.517432407E9 |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |1.517432408E9 |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |1.517432408E9 |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 23:44:00 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |1.517432408E9 |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |1.517432408E9 |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1.517432409E9 |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:44:00 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |1.517432409E9 |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:44:00 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1.517432411E9 |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |1.51743241E9  |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |1.517432411E9 |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |1.517432412E9 |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1.517432411E9 |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:44:00 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1.517432413E9 |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:44:00 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:44:00 INFO  only showing top 30 rows[0m
[0m2021.07.01 23:44:00 INFO  [0m
[0m2021.07.01 23:44:00 INFO  2018-02-01 03:00:02[0m
[0m2021.07.01 23:44:01 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:44:01 INFO  |metadata_ownerId|      date|        createdTime|        auditedTime|timeDelta|createdHour|auditedHour|  feedback|target|[0m
[0m2021.07.01 23:44:01 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:44:01 INFO  |           25161|2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|        0|          0|          3|[Disliked]|     0|[0m
[0m2021.07.01 23:44:01 INFO  |           80203|2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|        2|         20|          3| [Clicked]|     0|[0m
[0m2021.07.01 23:44:01 INFO  |           65675|2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|        8|         15|          3| [Ignored]|     0|[0m
[0m2021.07.01 23:44:01 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:44:01 INFO  [0m
[0m2021.07.01 23:44:04 INFO  {"metadata_ownerId":25161,"date":"2018-02-01","createdTime":"2018-02-01 00:03:09","auditedTime":"2018-02-01 03:00:02","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Disliked"],"target":0}[0m
[0m2021.07.01 23:44:04 INFO  {"metadata_ownerId":80203,"date":"2018-02-01","createdTime":"2018-01-29 20:21:50","auditedTime":"2018-02-01 03:00:02","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Clicked"],"target":0}[0m
[0m2021.07.01 23:44:04 INFO  {"metadata_ownerId":65675,"date":"2018-02-01","createdTime":"2018-01-23 15:15:04","auditedTime":"2018-02-01 03:00:02","timeDelta":8,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}[0m
[0m2021.07.01 23:44:04 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:46:54 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:46:56 INFO  time: compiled ht3 in 1.08s[0m
[0m2021.07.01 23:46:57 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 10h 41m 6.348s)[0m
[0m2021.07.01 23:46:57 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 23:46:57 INFO  time: compiled ht3-test in 0.21s[0m
[0m2021.07.01 23:46:57 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 23:46:57 INFO  Listening for transport dt_socket at address: 59851[0m
[0m2021.07.01 23:46:57 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 23:46:58 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:46:57 INFO  Trying to attach to remote debuggee VM localhost:59851 .[0m
[0m2021.07.01 23:46:57 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 23:46:59 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 23:46:59 ERROR 21/07/01 23:46:59 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 23:46:59 ERROR 21/07/01 23:46:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 23:47:00 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 23:47:00 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 23:47:00 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 23:47:00 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 23:47:00 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 23:47:00 ERROR 21/07/01 23:47:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 23:47:08 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 23:47:12 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:47:12 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|audit_unixtime|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 23:47:12 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:47:12 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1.517432401E9 |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |1.5174324E9   |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 23:47:12 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |1.517432401E9 |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |1.517432401E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |1.517432402E9 |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:47:12 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |1.517432403E9 |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |1.517432403E9 |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |1.517432404E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |1.517432403E9 |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:47:12 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |1.517432403E9 |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |1.517432404E9 |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |1.517432404E9 |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |1.517432405E9 |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |1.517432405E9 |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1.517432406E9 |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |1.517432407E9 |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |1.517432408E9 |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |1.517432408E9 |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 23:47:12 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |1.517432408E9 |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |1.517432408E9 |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1.517432409E9 |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:47:12 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |1.517432409E9 |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:47:12 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1.517432411E9 |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |1.51743241E9  |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |1.517432411E9 |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |1.517432412E9 |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1.517432411E9 |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:47:12 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1.517432413E9 |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:47:12 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:47:12 INFO  only showing top 30 rows[0m
[0m2021.07.01 23:47:12 INFO  [0m
[0m2021.07.01 23:47:12 INFO  2018-02-01 03:00:02[0m
[0m2021.07.01 23:47:13 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:47:13 INFO  |metadata_ownerId|      date|        createdTime|        auditedTime|timeDelta|createdHour|auditedHour|  feedback|target|[0m
[0m2021.07.01 23:47:13 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:47:13 INFO  |           25161|2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|        0|          0|          3|[Disliked]|     0|[0m
[0m2021.07.01 23:47:13 INFO  |           80203|2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|        2|         20|          3| [Clicked]|     0|[0m
[0m2021.07.01 23:47:13 INFO  |           65675|2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|        8|         15|          3| [Ignored]|     0|[0m
[0m2021.07.01 23:47:13 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:47:13 INFO  [0m
[0m2021.07.01 23:47:15 INFO  {[{"metadata_ownerId":25161,"date":"2018-02-01","createdTime":"2018-02-01 00:03:09","auditedTime":"2018-02-01 03:00:02","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":80203,"date":"2018-02-01","createdTime":"2018-01-29 20:21:50","auditedTime":"2018-02-01 03:00:02","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":65675,"date":"2018-02-01","createdTime":"2018-01-23 15:15:04","auditedTime":"2018-02-01 03:00:02","timeDelta":8,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}]}[0m
[0m2021.07.01 23:47:15 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:49:37 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:49:38 INFO  time: compiled ht3 in 1.08s[0m
[0m2021.07.01 23:51:25 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.01 23:51:26 INFO  time: compiled ht3 in 1.11s[0m
[0m2021.07.01 23:51:28 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.2' (since 10h 45m 37.676s)[0m
[0m2021.07.01 23:51:28 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.01 23:51:28 INFO  time: compiled ht3-test in 0.21s[0m
[0m2021.07.01 23:51:29 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.01 23:51:28 INFO  Listening for transport dt_socket at address: 57427[0m
[0m2021.07.01 23:51:29 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.01 23:51:29 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.01 23:51:28 INFO  Trying to attach to remote debuggee VM localhost:57427 .[0m
[0m2021.07.01 23:51:28 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.01 23:51:30 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.01 23:51:30 ERROR 21/07/01 23:51:30 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.01 23:51:30 ERROR 21/07/01 23:51:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.01 23:51:31 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.01 23:51:31 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.01 23:51:31 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.01 23:51:31 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.01 23:51:31 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.01 23:51:31 ERROR 21/07/01 23:51:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.01 23:51:40 INFO  Data shape=(360130.0, 169)[0m
[0m2021.07.01 23:51:43 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:51:43 INFO  |metadata_ownerId|date      |createdTime        |auditedTime        |audit_timestamp|audit_unixtime|timeDelta|createdHour|auditedHour|feedback                 |target|[0m
[0m2021.07.01 23:51:43 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:51:43 INFO  |76851           |2018-02-01|2018-01-30 10:30:14|2018-02-01 03:00:00|1517432400675  |1.517432401E9 |1        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |49367           |2018-02-01|2018-02-01 00:25:14|2018-02-01 03:00:00|1517432400333  |1.5174324E9   |0        |0          |3          |[Clicked, Unliked, Liked]|1     |[0m
[0m2021.07.01 23:51:43 INFO  |19357           |2018-02-01|2018-02-01 02:36:21|2018-02-01 03:00:00|1517432400687  |1.517432401E9 |0        |2          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |77159           |2018-02-01|2018-02-01 01:50:18|2018-02-01 03:00:01|1517432401051  |1.517432401E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|1517432402347  |1.517432402E9 |0        |0          |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:51:43 INFO  |80203           |2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|1517432402940  |1.517432403E9 |2        |20         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |65675           |2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|1517432402500  |1.517432403E9 |8        |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |75062           |2018-02-01|2018-02-01 01:00:21|2018-02-01 03:00:03|1517432403869  |1.517432404E9 |0        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |68682           |2018-02-01|2018-01-31 15:06:46|2018-02-01 03:00:03|1517432403478  |1.517432403E9 |0        |15         |3          |[Disliked]               |0     |[0m
[0m2021.07.01 23:51:43 INFO  |49570           |2018-02-01|2018-01-31 13:00:48|2018-02-01 03:00:03|1517432403097  |1.517432403E9 |0        |13         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |40612           |2018-02-01|2018-02-01 02:50:09|2018-02-01 03:00:04|1517432404163  |1.517432404E9 |0        |2          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |27554           |2018-02-01|2018-01-15 14:22:34|2018-02-01 03:00:04|1517432404153  |1.517432404E9 |16       |14         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |64597           |2018-02-01|2018-01-31 06:00:05|2018-02-01 03:00:05|1517432405094  |1.517432405E9 |0        |6          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |33827           |2018-02-01|2018-01-31 03:04:19|2018-02-01 03:00:05|1517432405168  |1.517432405E9 |0        |3          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |22580           |2018-02-01|2018-01-31 20:01:09|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |0        |20         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |35049           |2018-02-01|2017-10-05 15:45:48|2018-02-01 03:00:06|1517432406750  |1.517432407E9 |118      |15         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |5862            |2018-02-01|2018-01-30 19:36:22|2018-02-01 03:00:06|1517432406042  |1.517432406E9 |1        |19         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |24079           |2018-02-01|2018-02-01 00:26:49|2018-02-01 03:00:06|1517432406755  |1.517432407E9 |0        |0          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |5049            |2018-02-01|2018-01-31 22:16:22|2018-02-01 03:00:07|1517432407860  |1.517432408E9 |0        |22         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |1846            |2018-02-01|2018-02-01 02:20:42|2018-02-01 03:00:08|1517432408358  |1.517432408E9 |0        |2          |3          |[Liked, Ignored]         |1     |[0m
[0m2021.07.01 23:51:43 INFO  |45299           |2018-02-01|2018-02-01 01:01:15|2018-02-01 03:00:08|1517432408331  |1.517432408E9 |0        |1          |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |80229           |2018-02-01|2018-01-29 10:16:19|2018-02-01 03:00:08|1517432408061  |1.517432408E9 |2        |10         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |13290           |2018-02-01|2018-01-31 02:02:07|2018-02-01 03:00:08|1517432408884  |1.517432409E9 |1        |2          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:51:43 INFO  |25161           |2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:09|1517432409283  |1.517432409E9 |0        |0          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:51:43 INFO  |39412           |2018-02-01|2018-01-30 16:47:35|2018-02-01 03:00:10|1517432410836  |1.517432411E9 |1        |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |20606           |2018-02-01|2018-01-31 13:10:42|2018-02-01 03:00:10|1517432410030  |1.51743241E9  |0        |13         |3          |[Clicked]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |7195            |2018-02-01|2018-01-29 17:22:03|2018-02-01 03:00:11|1517432411277  |1.517432411E9 |2        |17         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |52022           |2018-02-01|2016-02-06 16:11:39|2018-02-01 03:00:11|1517432411986  |1.517432412E9 |725      |16         |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |80581           |2018-02-01|2018-01-31 01:08:07|2018-02-01 03:00:11|1517432411030  |1.517432411E9 |1        |1          |3          |[Ignored]                |0     |[0m
[0m2021.07.01 23:51:43 INFO  |36559           |2018-02-01|2018-01-30 05:44:20|2018-02-01 03:00:12|1517432412590  |1.517432413E9 |1        |5          |3          |[Liked]                  |1     |[0m
[0m2021.07.01 23:51:43 INFO  +----------------+----------+-------------------+-------------------+---------------+--------------+---------+-----------+-----------+-------------------------+------+[0m
[0m2021.07.01 23:51:43 INFO  only showing top 30 rows[0m
[0m2021.07.01 23:51:43 INFO  [0m
[0m2021.07.01 23:51:43 INFO  2018-02-01 03:00:02[0m
[0m2021.07.01 23:51:44 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:51:44 INFO  |metadata_ownerId|      date|        createdTime|        auditedTime|timeDelta|createdHour|auditedHour|  feedback|target|[0m
[0m2021.07.01 23:51:44 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:51:44 INFO  |           25161|2018-02-01|2018-02-01 00:03:09|2018-02-01 03:00:02|        0|          0|          3|[Disliked]|     0|[0m
[0m2021.07.01 23:51:44 INFO  |           80203|2018-02-01|2018-01-29 20:21:50|2018-02-01 03:00:02|        2|         20|          3| [Clicked]|     0|[0m
[0m2021.07.01 23:51:44 INFO  |           65675|2018-02-01|2018-01-23 15:15:04|2018-02-01 03:00:02|        8|         15|          3| [Ignored]|     0|[0m
[0m2021.07.01 23:51:44 INFO  +----------------+----------+-------------------+-------------------+---------+-----------+-----------+----------+------+[0m
[0m2021.07.01 23:51:44 INFO  [0m
[0m2021.07.01 23:51:46 INFO  [{"metadata_ownerId":25161,"date":"2018-02-01","createdTime":"2018-02-01 00:03:09","auditedTime":"2018-02-01 03:00:02","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":80203,"date":"2018-02-01","createdTime":"2018-01-29 20:21:50","auditedTime":"2018-02-01 03:00:02","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":65675,"date":"2018-02-01","createdTime":"2018-01-23 15:15:04","auditedTime":"2018-02-01 03:00:02","timeDelta":8,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.01 23:51:48 INFO  Pushed Record for 2018-02-01 03:00:02: [{"metadata_ownerId":25161,"date":"2018-02-01","createdTime":"2018-02-01 00:03:09","auditedTime":"2018-02-01 03:00:02","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":80203,"date":"2018-02-01","createdTime":"2018-01-29 20:21:50","auditedTime":"2018-02-01 03:00:02","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":65675,"date":"2018-02-01","createdTime":"2018-01-23 15:15:04","auditedTime":"2018-02-01 03:00:02","timeDelta":8,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.01 23:51:48 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.02 00:05:57 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.02 00:05:57 INFO  time: compiled ht3-test in 0.22s[0m
[0m2021.07.02 00:05:57 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.02 00:05:57 INFO  time: compiled ht3-test in 0.12s[0m
Jul 02, 2021 12:07:14 AM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTraceNotification
[0m2021.07.02 00:07:47 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.02 00:07:47 INFO  time: compiled ht3-test in 0.23s[0m
Jul 02, 2021 12:08:15 AM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTraceNotification
Jul 02, 2021 12:08:23 AM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTraceNotification
[0m2021.07.02 00:08:24 INFO  shutting down Metals[0m
[0m2021.07.02 00:08:24 INFO  Shut down connection with build server.[0m
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.07.02 00:08:24 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.07.02 00:08:43 INFO  Started: Metals version 0.10.4 in workspace '/home/hwait/dev/ht3' for client Visual Studio Code 1.57.1.[0m
[0m2021.07.02 00:08:46 INFO  time: initialize in 2.83s[0m
[0m2021.07.02 00:08:46 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5313928371761411345/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.07.02 00:08:46 WARN  no build target for: /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala[0m
[0m2021.07.02 00:08:46 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.07.02 00:08:49 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8-19-4d9f966b...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.12.10.
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.6/jline-2.14.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.12/jansi-1.12.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.10/scala-compiler-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.10/scala-reflect-2.12.10.jar
[0m[32m[D][0m Configured SemanticDB in projects 'ht3', 'ht3-test'
[0m[32m[D][0m Loading previous analysis for 'ht3' from '/home/hwait/dev/ht3/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Loading previous analysis for 'ht3-test' from '/home/hwait/dev/ht3/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5313928371761411345/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5313928371761411345/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.02 00:08:50 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.02 00:08:50 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5796062489754733509/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5796062489754733509/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5796062489754733509/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.02 00:08:50 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.02 00:08:50 INFO  time: Connected to build server in 4.37s[0m
[0m2021.07.02 00:08:50 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
[0m2021.07.02 00:08:50 INFO  time: Imported build in 0.12s[0m
[0m2021.07.02 00:08:53 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
[0m2021.07.02 00:08:53 INFO  time: code lens generation in 6.99s[0m
[0m2021.07.02 00:08:53 INFO  time: code lens generation in 5.65s[0m
[0m2021.07.02 00:08:53 INFO  time: code lens generation in 3.87s[0m
[0m2021.07.02 00:08:54 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.07.02 00:08:54 INFO  time: indexed workspace in 4.33s[0m
[0m2021.07.02 00:08:56 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.02 00:09:03 INFO  time: compiled ht3 in 7.08s[0m
[0m2021.07.02 00:09:03 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.02 00:09:03 INFO  time: compiled ht3-test in 0.41s[0m
Jul 02, 2021 12:20:23 AM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTrace
Jul 02, 2021 12:20:23 AM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTrace
Exception in thread "pool-5-thread-1" java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Exception in thread "pool-5-thread-2" java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Exception in thread "pool-5-thread-3" java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[0m2021.07.02 00:30:01 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.02 00:30:01 INFO  time: compiled ht3-test in 0.43s[0m
[0m2021.07.02 00:32:59 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.02 00:32:59 INFO  time: compiled ht3 in 0.52s[0m
[0m2021.07.02 00:34:49 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.02 00:34:49 INFO  time: compiled ht3 in 0.37s[0m
Jul 02, 2021 12:57:34 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2320
[0m2021.07.02 00:58:34 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.02 00:58:34 INFO  time: compiled ht3 in 0.5s[0m
Exception in thread "pool-5-thread-4" java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Exception in thread "pool-5-thread-5" java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Jul 02, 2021 1:04:07 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2823
[0m2021.07.02 01:04:27 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.02 01:04:27 INFO  time: compiled ht3 in 0.31s[0m
[0m2021.07.02 01:09:56 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.02 01:09:56 INFO  time: compiled ht3 in 0.29s[0m
[0m2021.07.02 01:10:26 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.02 01:10:26 INFO  time: compiled ht3 in 0.52s[0m
[0m2021.07.02 01:10:32 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.02 01:10:32 INFO  time: compiled ht3 in 0.32s[0m
[0m2021.07.02 01:11:41 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.02 01:11:41 INFO  time: compiled ht3 in 0.33s[0m
Exception in thread "pool-5-thread-6" java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Exception in thread "pool-5-thread-7" java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Exception in thread "pool-5-thread-8" java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[0m2021.07.02 01:12:13 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.02 01:12:13 INFO  time: compiled ht3 in 0.3s[0m
[0m2021.07.02 01:12:25 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.02 01:12:26 INFO  time: compiled ht3 in 1.64s[0m
[0m2021.07.02 01:12:34 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.02 01:12:34 INFO  time: compiled ht3-test in 0.25s[0m
[0m2021.07.02 01:12:34 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.02 01:12:34 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.02 01:12:35 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.02 01:12:34 INFO  Listening for transport dt_socket at address: 41341[0m
[0m2021.07.02 01:12:34 INFO  Trying to attach to remote debuggee VM localhost:41341 .[0m
[0m2021.07.02 01:12:34 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.02 01:12:36 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.02 01:12:36 ERROR 21/07/02 01:12:36 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.02 01:12:36 ERROR 21/07/02 01:12:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.02 01:12:37 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.02 01:12:37 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.02 01:12:37 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.02 01:12:37 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.02 01:12:37 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.02 01:12:37 ERROR 21/07/02 01:12:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.02 01:12:45 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.02 01:12:52 INFO  Pushed Record for 2018-02-01 03:00:00: [{"metadata_ownerId":49367,"date":"2018-02-01","createdTime":"2018-02-01 00:25:14","auditedTime":"2018-02-01 03:00:00","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Clicked","Unliked","Liked"],"target":1},{"metadata_ownerId":76851,"date":"2018-02-01","createdTime":"2018-01-30 10:30:14","auditedTime":"2018-02-01 03:00:00","timeDelta":1,"createdHour":10,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":19357,"date":"2018-02-01","createdTime":"2018-02-01 02:36:21","auditedTime":"2018-02-01 03:00:00","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.02 01:12:57 INFO  Pushed Record for 2018-02-01 03:00:01: [{"metadata_ownerId":77159,"date":"2018-02-01","createdTime":"2018-02-01 01:50:18","auditedTime":"2018-02-01 03:00:01","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.02 01:13:01 INFO  Pushed Record for 2018-02-01 03:00:02: [{"metadata_ownerId":25161,"date":"2018-02-01","createdTime":"2018-02-01 00:03:09","auditedTime":"2018-02-01 03:00:02","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":80203,"date":"2018-02-01","createdTime":"2018-01-29 20:21:50","auditedTime":"2018-02-01 03:00:02","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":65675,"date":"2018-02-01","createdTime":"2018-01-23 15:15:04","auditedTime":"2018-02-01 03:00:02","timeDelta":8,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.02 01:13:02 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.02 01:14:41 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.02 01:14:43 INFO  time: compiled ht3 in 2.02s[0m
[0m2021.07.02 01:14:44 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 1h 5m 54.108s)[0m
[0m2021.07.02 01:14:44 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.02 01:14:44 INFO  time: compiled ht3-test in 0.21s[0m
[0m2021.07.02 01:14:45 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.02 01:14:44 INFO  Listening for transport dt_socket at address: 54251[0m
[0m2021.07.02 01:14:45 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.02 01:14:45 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.02 01:14:44 INFO  Trying to attach to remote debuggee VM localhost:54251 .[0m
[0m2021.07.02 01:14:44 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.02 01:14:46 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.02 01:14:46 ERROR 21/07/02 01:14:46 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.02 01:14:46 ERROR 21/07/02 01:14:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.02 01:14:47 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.02 01:14:47 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.02 01:14:47 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.02 01:14:47 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.02 01:14:47 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.02 01:14:47 ERROR 21/07/02 01:14:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.02 01:14:55 INFO  Day 2018-02-02 loaded 363326.0 rows.[0m
[0m2021.07.02 01:15:01 INFO  Pushed Record for 2018-02-02 23:59:55: [{"metadata_ownerId":80,"date":"2018-02-02","createdTime":"2018-02-02 23:29:47","auditedTime":"2018-02-02 23:59:55","timeDelta":0,"createdHour":23,"auditedHour":23,"feedback":["Clicked"],"target":0},{"metadata_ownerId":23403,"date":"2018-02-02","createdTime":"2018-02-02 09:33:03","auditedTime":"2018-02-02 23:59:55","timeDelta":0,"createdHour":9,"auditedHour":23,"feedback":["Ignored"],"target":0},{"metadata_ownerId":51979,"date":"2018-02-02","createdTime":"2018-02-01 10:13:23","auditedTime":"2018-02-02 23:59:55","timeDelta":1,"createdHour":10,"auditedHour":23,"feedback":["Clicked"],"target":0},{"metadata_ownerId":71284,"date":"2018-02-02","createdTime":"2018-02-02 18:15:05","auditedTime":"2018-02-02 23:59:55","timeDelta":0,"createdHour":18,"auditedHour":23,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.02 01:15:06 INFO  Pushed Record for 2018-02-02 23:59:56: [{"metadata_ownerId":79133,"date":"2018-02-02","createdTime":"2015-12-24 16:03:32","auditedTime":"2018-02-02 23:59:56","timeDelta":771,"createdHour":16,"auditedHour":23,"feedback":["Ignored"],"target":0},{"metadata_ownerId":33266,"date":"2018-02-02","createdTime":"2018-02-02 22:44:18","auditedTime":"2018-02-02 23:59:56","timeDelta":0,"createdHour":22,"auditedHour":23,"feedback":["Ignored"],"target":0},{"metadata_ownerId":15503,"date":"2018-02-02","createdTime":"2018-02-02 02:10:41","auditedTime":"2018-02-02 23:59:56","timeDelta":0,"createdHour":2,"auditedHour":23,"feedback":["Ignored"],"target":0},{"metadata_ownerId":36831,"date":"2018-02-02","createdTime":"2018-02-01 12:12:21","auditedTime":"2018-02-02 23:59:56","timeDelta":1,"createdHour":12,"auditedHour":23,"feedback":["Ignored"],"target":0},{"metadata_ownerId":7106,"date":"2018-02-02","createdTime":"2018-02-02 18:09:58","auditedTime":"2018-02-02 23:59:56","timeDelta":0,"createdHour":18,"auditedHour":23,"feedback":["Ignored"],"target":0},{"metadata_ownerId":59266,"date":"2018-02-02","createdTime":"2018-01-31 16:34:00","auditedTime":"2018-02-02 23:59:56","timeDelta":2,"createdHour":16,"auditedHour":23,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.02 01:15:10 INFO  Pushed Record for 2018-02-02 23:59:57: [{"metadata_ownerId":51305,"date":"2018-02-02","createdTime":"2018-02-02 21:20:37","auditedTime":"2018-02-02 23:59:57","timeDelta":0,"createdHour":21,"auditedHour":23,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.02 01:15:14 INFO  Pushed Record for 2018-02-02 23:59:58: [{"metadata_ownerId":13917,"date":"2018-02-02","createdTime":"2018-02-02 18:41:34","auditedTime":"2018-02-02 23:59:58","timeDelta":0,"createdHour":18,"auditedHour":23,"feedback":["Ignored"],"target":0},{"metadata_ownerId":20750,"date":"2018-02-02","createdTime":"2018-02-02 20:57:10","auditedTime":"2018-02-02 23:59:58","timeDelta":0,"createdHour":20,"auditedHour":23,"feedback":["Ignored"],"target":0},{"metadata_ownerId":26098,"date":"2018-02-02","createdTime":"2018-01-31 02:00:58","auditedTime":"2018-02-02 23:59:58","timeDelta":2,"createdHour":2,"auditedHour":23,"feedback":["Ignored"],"target":0},{"metadata_ownerId":8965,"date":"2018-02-02","createdTime":"2018-02-02 19:15:11","auditedTime":"2018-02-02 23:59:58","timeDelta":0,"createdHour":19,"auditedHour":23,"feedback":["Ignored"],"target":0},{"metadata_ownerId":45675,"date":"2018-02-02","createdTime":"2018-02-01 12:29:30","auditedTime":"2018-02-02 23:59:58","timeDelta":1,"createdHour":12,"auditedHour":23,"feedback":["Ignored"],"target":0},{"metadata_ownerId":27615,"date":"2018-02-02","createdTime":"2018-02-02 10:38:46","auditedTime":"2018-02-02 23:59:58","timeDelta":0,"createdHour":10,"auditedHour":23,"feedback":["Ignored"],"target":0},{"metadata_ownerId":3886,"date":"2018-02-02","createdTime":"2018-02-01 18:36:39","auditedTime":"2018-02-02 23:59:58","timeDelta":1,"createdHour":18,"auditedHour":23,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.02 01:15:18 INFO  Pushed Record for 2018-02-02 23:59:59: [{"metadata_ownerId":44226,"date":"2018-02-02","createdTime":"2018-01-31 21:49:44","auditedTime":"2018-02-02 23:59:59","timeDelta":2,"createdHour":21,"auditedHour":23,"feedback":["Clicked","Liked"],"target":1},{"metadata_ownerId":12660,"date":"2018-02-02","createdTime":"2018-02-02 23:21:06","auditedTime":"2018-02-02 23:59:59","timeDelta":0,"createdHour":23,"auditedHour":23,"feedback":["Ignored"],"target":0},{"metadata_ownerId":83845,"date":"2018-02-02","createdTime":"2018-01-31 13:57:38","auditedTime":"2018-02-02 23:59:59","timeDelta":2,"createdHour":13,"auditedHour":23,"feedback":["Disliked"],"target":0},{"metadata_ownerId":6697,"date":"2018-02-02","createdTime":"2018-02-02 01:31:05","auditedTime":"2018-02-02 23:59:59","timeDelta":0,"createdHour":1,"auditedHour":23,"feedback":["Ignored"],"target":0},{"metadata_ownerId":63852,"date":"2018-02-02","createdTime":"2018-02-01 08:15:04","auditedTime":"2018-02-02 23:59:59","timeDelta":1,"createdHour":8,"auditedHour":23,"feedback":["Ignored"],"target":0},{"metadata_ownerId":42737,"date":"2018-02-02","createdTime":"2018-02-02 22:31:43","auditedTime":"2018-02-02 23:59:59","timeDelta":0,"createdHour":22,"auditedHour":23,"feedback":["Ignored"],"target":0},{"metadata_ownerId":39823,"date":"2018-02-02","createdTime":"2018-02-02 22:54:53","auditedTime":"2018-02-02 23:59:59","timeDelta":0,"createdHour":22,"auditedHour":23,"feedback":["Ignored"],"target":0},{"metadata_ownerId":6800,"date":"2018-02-02","createdTime":"2018-01-31 13:40:26","auditedTime":"2018-02-02 23:59:59","timeDelta":2,"createdHour":13,"auditedHour":23,"feedback":["Clicked"],"target":0}][0m
[0m2021.07.02 01:15:20 INFO  Day 2018-02-03 loaded 380747.0 rows.[0m
[0m2021.07.02 01:15:35 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.02 01:24:05 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.02 01:24:07 INFO  time: compiled ht3 in 1.52s[0m
[0m2021.07.02 01:26:18 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.02 01:26:18 INFO  time: compiled ht3 in 0.29s[0m
[0m2021.07.02 01:27:01 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.02 01:27:02 INFO  time: compiled ht3 in 1.27s[0m
[0m2021.07.02 01:27:20 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.02 01:27:21 INFO  time: compiled ht3 in 1.23s[0m
[0m2021.07.02 01:27:21 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 1h 18m 31.508s)[0m
[0m2021.07.02 01:27:21 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.02 01:27:22 INFO  time: compiled ht3-test in 0.2s[0m
[0m2021.07.02 01:27:22 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.02 01:27:22 INFO  Listening for transport dt_socket at address: 54533[0m
[0m2021.07.02 01:27:22 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.02 01:27:22 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.02 01:27:22 INFO  Trying to attach to remote debuggee VM localhost:54533 .[0m
[0m2021.07.02 01:27:22 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.02 01:27:22 ERROR Exception in thread "main" com.typesafe.config.ConfigException$Parse: application.json @ file:/home/hwait/dev/ht3/src/main/resources/application.json: 15: expecting a field name after a comma, got a close brace } instead (if you intended '}' to be part of a key or string value, try enclosing the key or value in double quotes)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:431)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:475)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:633)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:262)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:250)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:309)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.Parseable$ParseableResources.rawParseValue(Parseable.java:738)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.Parseable$ParseableResources.rawParseValue(Parseable.java:701)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:152)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:172)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.impl.ConfigImpl.parseResourcesAnySyntax(ConfigImpl.java:133)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.ConfigFactory.parseResourcesAnySyntax(ConfigFactory.java:1082)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:117)[0m
[0m2021.07.02 01:27:22 ERROR 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:78)[0m
[0m2021.07.02 01:27:22 ERROR 	at ru.otus.bigdataml.ht3.DataProvider$.main(DataProvider.scala:22)[0m
[0m2021.07.02 01:27:22 ERROR 	at ru.otus.bigdataml.ht3.DataProvider.main(DataProvider.scala)[0m
[0m2021.07.02 01:27:22 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.02 01:27:37 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 1h 18m 47.141s)[0m
[0m2021.07.02 01:27:37 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.02 01:27:37 INFO  time: compiled ht3-test in 0.2s[0m
[0m2021.07.02 01:27:38 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.02 01:27:37 INFO  Listening for transport dt_socket at address: 50145[0m
[0m2021.07.02 01:27:38 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.02 01:27:38 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.02 01:27:37 INFO  Trying to attach to remote debuggee VM localhost:50145 .[0m
[0m2021.07.02 01:27:37 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.02 01:27:39 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.02 01:27:39 ERROR 21/07/02 01:27:39 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.152.53 instead (on interface eth0)[0m
[0m2021.07.02 01:27:39 ERROR 21/07/02 01:27:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.02 01:27:40 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.02 01:27:40 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.02 01:27:40 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.02 01:27:40 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.02 01:27:40 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.02 01:27:40 ERROR 21/07/02 01:27:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.02 01:27:48 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.02 01:27:53 INFO  Pushed Record for 2018-02-01 03:00:00: [{"metadata_ownerId":49367,"date":"2018-02-01","createdTime":"2018-02-01 00:25:14","auditedTime":"2018-02-01 03:00:00","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Clicked","Unliked","Liked"],"target":1},{"metadata_ownerId":76851,"date":"2018-02-01","createdTime":"2018-01-30 10:30:14","auditedTime":"2018-02-01 03:00:00","timeDelta":1,"createdHour":10,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":19357,"date":"2018-02-01","createdTime":"2018-02-01 02:36:21","auditedTime":"2018-02-01 03:00:00","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.02 01:27:57 INFO  Pushed Record for 2018-02-01 03:00:01: [{"metadata_ownerId":77159,"date":"2018-02-01","createdTime":"2018-02-01 01:50:18","auditedTime":"2018-02-01 03:00:01","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.02 01:27:59 INFO  Pushed Record for 2018-02-01 03:00:02: [{"metadata_ownerId":25161,"date":"2018-02-01","createdTime":"2018-02-01 00:03:09","auditedTime":"2018-02-01 03:00:02","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":80203,"date":"2018-02-01","createdTime":"2018-01-29 20:21:50","auditedTime":"2018-02-01 03:00:02","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":65675,"date":"2018-02-01","createdTime":"2018-01-23 15:15:04","auditedTime":"2018-02-01 03:00:02","timeDelta":8,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.02 01:28:01 INFO  Pushed Record for 2018-02-01 03:00:03: [{"metadata_ownerId":75062,"date":"2018-02-01","createdTime":"2018-02-01 01:00:21","auditedTime":"2018-02-01 03:00:03","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":68682,"date":"2018-02-01","createdTime":"2018-01-31 15:06:46","auditedTime":"2018-02-01 03:00:03","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":49570,"date":"2018-02-01","createdTime":"2018-01-31 13:00:48","auditedTime":"2018-02-01 03:00:03","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.02 01:28:04 INFO  Pushed Record for 2018-02-01 03:00:04: [{"metadata_ownerId":40612,"date":"2018-02-01","createdTime":"2018-02-01 02:50:09","auditedTime":"2018-02-01 03:00:04","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":27554,"date":"2018-02-01","createdTime":"2018-01-15 14:22:34","auditedTime":"2018-02-01 03:00:04","timeDelta":16,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.02 01:28:06 INFO  Pushed Record for 2018-02-01 03:00:05: [{"metadata_ownerId":64597,"date":"2018-02-01","createdTime":"2018-01-31 06:00:05","auditedTime":"2018-02-01 03:00:05","timeDelta":0,"createdHour":6,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":33827,"date":"2018-02-01","createdTime":"2018-01-31 03:04:19","auditedTime":"2018-02-01 03:00:05","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.02 01:28:08 INFO  Pushed Record for 2018-02-01 03:00:06: [{"metadata_ownerId":24079,"date":"2018-02-01","createdTime":"2018-02-01 00:26:49","auditedTime":"2018-02-01 03:00:06","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":22580,"date":"2018-02-01","createdTime":"2018-01-31 20:01:09","auditedTime":"2018-02-01 03:00:06","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":35049,"date":"2018-02-01","createdTime":"2017-10-05 15:45:48","auditedTime":"2018-02-01 03:00:06","timeDelta":118,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":5862,"date":"2018-02-01","createdTime":"2018-01-30 19:36:22","auditedTime":"2018-02-01 03:00:06","timeDelta":1,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.02 01:28:10 INFO  Pushed Record for 2018-02-01 03:00:07: [{"metadata_ownerId":5049,"date":"2018-02-01","createdTime":"2018-01-31 22:16:22","auditedTime":"2018-02-01 03:00:07","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.02 01:28:12 INFO  Pushed Record for 2018-02-01 03:00:08: [{"metadata_ownerId":45299,"date":"2018-02-01","createdTime":"2018-02-01 01:01:15","auditedTime":"2018-02-01 03:00:08","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":80229,"date":"2018-02-01","createdTime":"2018-01-29 10:16:19","auditedTime":"2018-02-01 03:00:08","timeDelta":2,"createdHour":10,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":13290,"date":"2018-02-01","createdTime":"2018-01-31 02:02:07","auditedTime":"2018-02-01 03:00:08","timeDelta":1,"createdHour":2,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":1846,"date":"2018-02-01","createdTime":"2018-02-01 02:20:42","auditedTime":"2018-02-01 03:00:08","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Liked","Ignored"],"target":1}][0m
[0m2021.07.02 01:28:14 INFO  Pushed Record for 2018-02-01 03:00:09: [{"metadata_ownerId":25161,"date":"2018-02-01","createdTime":"2018-02-01 00:03:09","auditedTime":"2018-02-01 03:00:09","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.02 01:28:16 INFO  Pushed Record for 2018-02-01 03:00:10: [{"metadata_ownerId":20606,"date":"2018-02-01","createdTime":"2018-01-31 13:10:42","auditedTime":"2018-02-01 03:00:10","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":39412,"date":"2018-02-01","createdTime":"2018-01-30 16:47:35","auditedTime":"2018-02-01 03:00:10","timeDelta":1,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.02 01:28:17 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.02 01:28:39 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.02 01:28:40 INFO  time: compiled ht3 in 1.41s[0m
Jul 02, 2021 1:28:53 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4029
[0m2021.07.02 01:51:18 INFO  shutting down Metals[0m
[0m2021.07.02 01:51:18 INFO  Shut down connection with build server.[0m
[0m2021.07.02 01:51:18 INFO  Shut down connection with build server.[0m
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.07.02 14:03:18 INFO  Started: Metals version 0.10.4 in workspace '/home/hwait/dev/ht3' for client Visual Studio Code 1.57.1.[0m
[0m2021.07.02 14:03:21 INFO  time: initialize in 2.62s[0m
[0m2021.07.02 14:03:21 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.07.02 14:03:21 WARN  no build target for: /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4839382457881135176/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.07.02 14:03:21 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.07.02 14:03:24 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8-19-4d9f966b...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.12.10.
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.6/jline-2.14.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.12/jansi-1.12.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.10/scala-compiler-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.10/scala-reflect-2.12.10.jar
[0m[32m[D][0m Configured SemanticDB in projects 'ht3-test', 'ht3'
[0m[32m[D][0m Loading previous analysis for 'ht3-test' from '/home/hwait/dev/ht3/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Loading previous analysis for 'ht3' from '/home/hwait/dev/ht3/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4839382457881135176/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4839382457881135176/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.02 14:03:25 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.02 14:03:25 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1105914520005744076/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1105914520005744076/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1105914520005744076/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.02 14:03:26 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.02 14:03:26 INFO  time: Connected to build server in 4.72s[0m
[0m2021.07.02 14:03:26 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
[0m2021.07.02 14:03:26 INFO  time: Imported build in 0.31s[0m
[0m2021.07.02 14:03:28 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
[0m2021.07.02 14:03:28 INFO  time: code lens generation in 6.56s[0m
[0m2021.07.02 14:03:29 INFO  time: code lens generation in 7.42s[0m
[0m2021.07.02 14:03:30 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.07.02 14:03:30 INFO  time: indexed workspace in 4.36s[0m
[0m2021.07.02 14:03:32 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.02 14:03:35 INFO  time: compiled ht3-test in 2.26s[0m
Jul 02, 2021 7:39:13 PM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTrace
Jul 02, 2021 7:41:08 PM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTrace
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala in Array[String]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala, 603, 603, 616)
[0m2021.07.03 00:16:45 INFO  shutting down Metals[0m
[0m2021.07.03 00:16:45 INFO  Shut down connection with build server.[0m
[0m2021.07.03 00:16:45 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.07.03 22:55:41 INFO  Started: Metals version 0.10.4 in workspace '/home/hwait/dev/ht3' for client Visual Studio Code 1.57.1.[0m
[0m2021.07.03 22:55:43 INFO  time: initialize in 2.65s[0m
[0m2021.07.03 22:55:43 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher16328682862198505251/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.07.03 22:55:43 WARN  no build target for: /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala[0m
[0m2021.07.03 22:55:43 WARN  no build target for: /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLRegressionFromKafka.scala[0m
[0m2021.07.03 22:55:43 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.07.03 22:55:46 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8-19-4d9f966b...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.12.10.
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.6/jline-2.14.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.12/jansi-1.12.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.10/scala-compiler-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.10/scala-reflect-2.12.10.jar
[0m[32m[D][0m Configured SemanticDB in projects 'ht3-test', 'ht3'
[0m[32m[D][0m Loading previous analysis for 'ht3-test' from '/home/hwait/dev/ht3/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Loading previous analysis for 'ht3' from '/home/hwait/dev/ht3/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher16328682862198505251/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher16328682862198505251/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.03 22:55:47 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.03 22:55:47 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher16799098833452360172/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher16799098833452360172/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher16799098833452360172/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.03 22:55:48 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.03 22:55:48 INFO  time: Connected to build server in 4.52s[0m
[0m2021.07.03 22:55:48 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
[0m2021.07.03 22:55:48 INFO  time: Imported build in 0.32s[0m
[0m2021.07.03 22:55:50 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
[0m2021.07.03 22:55:50 INFO  time: code lens generation in 6s[0m
[0m2021.07.03 22:55:51 INFO  time: code lens generation in 7.23s[0m
[0m2021.07.03 22:55:52 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.07.03 22:55:52 INFO  time: indexed workspace in 4.51s[0m
[0m2021.07.03 22:55:55 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.03 22:55:57 INFO  time: compiled ht3-test in 2.23s[0m
[0m2021.07.03 22:57:46 INFO  compiling ht3 (1 scala source)[0m
Jul 03, 2021 10:57:46 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 15
[0m2021.07.03 22:57:48 INFO  time: compiled ht3 in 1.64s[0m
[0m2021.07.03 22:58:28 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.03 22:58:31 WARN  there was one deprecation warning (since 3.0.0); re-run with -deprecation for details[0m
[0m2021.07.03 22:58:31 INFO  time: compiled ht3 in 3.28s[0m
Jul 03, 2021 11:21:11 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 96
Jul 03, 2021 11:21:21 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 109
Jul 03, 2021 11:21:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 122
Jul 03, 2021 11:21:39 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 142
Jul 03, 2021 11:21:42 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 148
Jul 03, 2021 11:21:44 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 154
Jul 03, 2021 11:21:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 169
Jul 03, 2021 11:22:07 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 176
Jul 03, 2021 11:22:28 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 210
Jul 03, 2021 11:22:40 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 231
Jul 03, 2021 11:22:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 259
Jul 03, 2021 11:23:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 273
Jul 03, 2021 11:23:19 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 296
Jul 03, 2021 11:23:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 312
Jul 03, 2021 11:24:19 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 360
Jul 03, 2021 11:24:36 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 373
Jul 03, 2021 11:24:49 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 403
Jul 03, 2021 11:25:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 420
Jul 03, 2021 11:25:03 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 431
Jul 03, 2021 11:25:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 443
[0m2021.07.03 23:25:23 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.03 23:25:23 INFO  time: compiled ht3 in 0.2s[0m
[0m2021.07.03 23:25:23 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.03 23:25:23 INFO  time: compiled ht3 in 87ms[0m
[0m2021.07.03 23:25:36 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.03 23:25:36 INFO  time: compiled ht3 in 0.16s[0m
[0m2021.07.03 23:25:39 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.03 23:25:39 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 29m 50.75s)[0m
[0m2021.07.03 23:25:39 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.03 23:25:39 INFO  time: compiled ht3 in 0.12s[0m
[0m2021.07.03 23:25:39 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.03 23:25:39 INFO  time: compiled ht3 in 65ms[0m
Jul 03, 2021 11:25:39 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
java.util.concurrent.CompletionException: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:29)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:26)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:237)
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:229)
	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417)
	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
	... 4 more

[0m2021.07.03 23:26:21 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.03 23:26:21 INFO  time: compiled ht3 in 0.59s[0m
Jul 03, 2021 11:26:28 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 562
Jul 03, 2021 11:26:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 577
[0m2021.07.03 23:26:45 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.03 23:26:45 INFO  time: compiled ht3 in 0.47s[0m
Jul 03, 2021 11:26:53 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 595
Jul 03, 2021 11:26:58 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 606
[0m2021.07.03 23:27:06 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.03 23:27:06 INFO  time: compiled ht3 in 0.4s[0m
[0m2021.07.03 23:27:20 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.03 23:27:20 INFO  time: compiled ht3 in 0.4s[0m
Jul 03, 2021 11:27:25 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 646
[0m2021.07.03 23:27:30 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.03 23:27:30 INFO  time: compiled ht3 in 0.38s[0m
[0m2021.07.03 23:27:30 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.03 23:27:30 INFO  time: compiled ht3 in 0.28s[0m
Jul 03, 2021 11:27:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 658
[0m2021.07.03 23:27:37 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.03 23:27:39 INFO  time: compiled ht3 in 1.49s[0m
Jul 03, 2021 11:28:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 686
Jul 03, 2021 11:28:42 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 704
Jul 03, 2021 11:28:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 709
[0m2021.07.03 23:29:10 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.03 23:29:12 INFO  time: compiled ht3 in 1.13s[0m
Jul 03, 2021 11:29:16 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 823
Jul 03, 2021 11:29:21 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 834
Jul 03, 2021 11:29:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 847
[0m2021.07.03 23:29:36 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.03 23:29:36 INFO  time: compiled ht3 in 0.97s[0m
[0m2021.07.03 23:29:40 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 33m 51.963s)[0m
[0m2021.07.03 23:29:40 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.03 23:29:40 INFO  time: compiled ht3-test in 0.2s[0m
[0m2021.07.03 23:29:40 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.03 23:29:40 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.03 23:29:40 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.03 23:29:40 INFO  Listening for transport dt_socket at address: 59005[0m
[0m2021.07.03 23:29:40 INFO  Trying to attach to remote debuggee VM localhost:59005 .[0m
[0m2021.07.03 23:29:40 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.03 23:29:42 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.03 23:29:42 ERROR 21/07/03 23:29:42 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.130.195 instead (on interface eth0)[0m
[0m2021.07.03 23:29:42 ERROR 21/07/03 23:29:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.03 23:29:43 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.03 23:29:43 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.03 23:29:43 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.03 23:29:43 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.03 23:29:43 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.03 23:29:43 ERROR 21/07/03 23:29:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.03 23:29:51 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.04 00:29:16 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.04 00:29:23 INFO  shutting down Metals[0m
[0m2021.07.04 00:29:23 INFO  Shut down connection with build server.[0m
[0m2021.07.04 00:29:23 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.07.07 14:56:32 INFO  Started: Metals version 0.10.4 in workspace '/home/hwait/dev/ht3' for client Visual Studio Code 1.57.1.[0m
[0m2021.07.07 14:56:35 INFO  time: initialize in 2.57s[0m
[0m2021.07.07 14:56:35 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher15090852595747781169/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.07.07 14:56:35 WARN  no build target for: /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala[0m
[0m2021.07.07 14:56:35 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.07.07 14:56:38 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8-19-4d9f966b...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.12.10.
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.6/jline-2.14.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.12/jansi-1.12.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.10/scala-compiler-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.10/scala-reflect-2.12.10.jar
[0m[32m[D][0m Configured SemanticDB in projects 'ht3', 'ht3-test'
[0m[32m[D][0m Loading previous analysis for 'ht3-test' from '/home/hwait/dev/ht3/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Loading previous analysis for 'ht3' from '/home/hwait/dev/ht3/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher15090852595747781169/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher15090852595747781169/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.07 14:56:39 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.07 14:56:39 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8667366410588842671/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8667366410588842671/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8667366410588842671/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.07 14:56:40 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.07 14:56:40 INFO  time: Connected to build server in 4.9s[0m
[0m2021.07.07 14:56:40 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
[0m2021.07.07 14:56:40 INFO  time: Imported build in 0.29s[0m
[0m2021.07.07 14:56:42 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
[0m2021.07.07 14:56:42 INFO  time: code lens generation in 5.66s[0m
[0m2021.07.07 14:56:43 INFO  time: code lens generation in 7.37s[0m
[0m2021.07.07 14:56:44 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.07.07 14:56:44 INFO  time: indexed workspace in 4.66s[0m
[0m2021.07.07 14:56:47 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 14:56:49 INFO  time: compiled ht3-test in 2.4s[0m
[0m2021.07.07 15:59:05 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 15:59:06 INFO  time: compiled ht3 in 1.42s[0m
[0m2021.07.07 15:59:13 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 15:59:13 INFO  time: compiled ht3 in 0.78s[0m
[0m2021.07.07 16:00:17 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:00:17 INFO  time: compiled ht3 in 0.93s[0m
[0m2021.07.07 16:02:02 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:02:05 INFO  time: compiled ht3 in 3.38s[0m
[0m2021.07.07 16:02:14 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:02:14 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 1h 5m 33.817s)[0m
[0m2021.07.07 16:02:14 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:02:15 INFO  time: compiled ht3 in 1.43s[0m
[0m2021.07.07 16:02:15 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 16:02:15 INFO  time: compiled ht3-test in 0.19s[0m
[0m2021.07.07 16:02:15 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 16:02:15 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 16:02:16 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 16:02:15 INFO  Listening for transport dt_socket at address: 51357[0m
[0m2021.07.07 16:02:15 INFO  Trying to attach to remote debuggee VM localhost:51357 .[0m
[0m2021.07.07 16:02:15 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 16:02:17 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 16:02:17 ERROR 21/07/07 16:02:17 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 16:02:17 ERROR 21/07/07 16:02:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 16:02:18 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 16:02:18 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 16:02:18 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 16:02:18 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 16:02:18 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 16:02:18 ERROR 21/07/07 16:02:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 16:02:27 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 16:02:27 INFO  [Ljava.lang.String;@3368cf02[0m
[0m2021.07.07 16:02:28 INFO  +---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 16:02:28 INFO  |instanceId_objectType|audit_resourceType| metadata_ownerType|membership_status|log_ownerId|log_authorId|[0m
[0m2021.07.07 16:02:28 INFO  +---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 16:02:28 INFO  |                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 16:02:28 INFO  |                 Post|                 8|         GROUP_OPEN|                A|        4.0|         6.0|[0m
[0m2021.07.07 16:02:28 INFO  |                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        4.0|         6.0|[0m
[0m2021.07.07 16:02:28 INFO  |                 Post|                 7|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 16:02:28 INFO  |                 Post|                14|GROUP_OPEN_OFFICIAL|             null|        4.0|         6.0|[0m
[0m2021.07.07 16:02:28 INFO  |                 Post|                14|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 16:02:28 INFO  |                Photo|                 3|         GROUP_OPEN|             null|        5.0|         6.0|[0m
[0m2021.07.07 16:02:28 INFO  |                 Post|                 8|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 16:02:28 INFO  |                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 16:02:28 INFO  |                 Post|                 7|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 16:02:28 INFO  +---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 16:02:28 INFO  only showing top 10 rows[0m
[0m2021.07.07 16:02:28 INFO  [0m
[0m2021.07.07 16:02:28 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 16:20:49 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:20:51 INFO  time: compiled ht3 in 1.91s[0m
[0m2021.07.07 16:20:53 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 1h 24m 13.537s)[0m
[0m2021.07.07 16:20:53 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 16:20:53 INFO  time: compiled ht3-test in 0.28s[0m
[0m2021.07.07 16:20:54 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 16:20:53 INFO  Listening for transport dt_socket at address: 40387[0m
[0m2021.07.07 16:20:54 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 16:20:54 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 16:20:53 INFO  Trying to attach to remote debuggee VM localhost:40387 .[0m
[0m2021.07.07 16:20:53 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 16:20:55 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 16:20:55 ERROR 21/07/07 16:20:55 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 16:20:55 ERROR 21/07/07 16:20:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 16:20:56 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 16:20:56 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 16:20:56 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 16:20:56 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 16:20:56 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 16:20:56 ERROR 21/07/07 16:20:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 16:21:05 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 16:21:05 ERROR Exception in thread "main" java.lang.UnsupportedOperationException: Schema for type Any is not supported[0m
[0m2021.07.07 16:21:05 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$schemaFor$1(ScalaReflection.scala:769)[0m
[0m2021.07.07 16:21:05 ERROR 	at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69)[0m
[0m2021.07.07 16:21:05 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:904)[0m
[0m2021.07.07 16:21:05 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:903)[0m
[0m2021.07.07 16:21:05 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:49)[0m
[0m2021.07.07 16:21:05 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:693)[0m
[0m2021.07.07 16:21:05 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:689)[0m
[0m2021.07.07 16:21:05 ERROR 	at org.apache.spark.sql.functions$.$anonfun$udf$6(functions.scala:4667)[0m
[0m2021.07.07 16:21:05 ERROR 	at scala.Option.getOrElse(Option.scala:189)[0m
[0m2021.07.07 16:21:05 ERROR 	at org.apache.spark.sql.functions$.udf(functions.scala:4667)[0m
[0m2021.07.07 16:21:05 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.loadDay$1(SparkMLModelTraining.scala:51)[0m
[0m2021.07.07 16:21:05 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:67)[0m
[0m2021.07.07 16:21:05 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
Jul 07, 2021 4:21:06 PM org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer fireError
SEVERE: java.net.SocketException: Broken pipe (Write failed)
org.eclipse.lsp4j.jsonrpc.JsonRpcException: java.net.SocketException: Broken pipe (Write failed)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:72)
	at scala.meta.internal.metals.debug.SocketEndpoint.consume(SocketEndpoint.scala:22)
	at scala.meta.internal.metals.debug.MessageIdAdapter.consume(MessageIdAdapter.scala:43)
	at scala.meta.internal.metals.debug.ServerAdapter.send(ServerAdapter.scala:30)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleClientMessage$1(DebugProxy.scala:89)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToClient$1(DebugProxy.scala:54)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketException: Broken pipe (Write failed)
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)
	at java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:69)
	... 21 more

[0m2021.07.07 16:21:06 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 16:29:18 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 16:29:18 INFO  time: compiled ht3-test in 0.28s[0m
[0m2021.07.07 16:29:19 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 16:29:19 INFO  time: compiled ht3-test in 0.15s[0m
[0m2021.07.07 16:29:41 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:29:41 INFO  time: compiled ht3 in 0.15s[0m
Jul 07, 2021 4:29:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1465
[0m2021.07.07 16:29:52 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:29:54 INFO  time: compiled ht3 in 2.17s[0m
[0m2021.07.07 16:29:55 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 1h 33m 15.499s)[0m
[0m2021.07.07 16:29:55 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 16:29:55 INFO  time: compiled ht3-test in 0.24s[0m
[0m2021.07.07 16:29:56 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 16:29:55 INFO  Listening for transport dt_socket at address: 38977[0m
[0m2021.07.07 16:29:56 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 16:29:56 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 16:29:55 INFO  Trying to attach to remote debuggee VM localhost:38977 .[0m
[0m2021.07.07 16:29:55 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 16:29:57 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 16:29:57 ERROR 21/07/07 16:29:57 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 16:29:57 ERROR 21/07/07 16:29:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 16:29:58 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 16:29:58 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 16:29:58 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 16:29:58 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 16:29:58 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 16:29:58 ERROR 21/07/07 16:29:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 16:30:07 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 16:30:07 ERROR Exception in thread "main" java.lang.UnsupportedOperationException: Schema for type Any is not supported[0m
[0m2021.07.07 16:30:07 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$schemaFor$1(ScalaReflection.scala:769)[0m
[0m2021.07.07 16:30:07 ERROR 	at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69)[0m
[0m2021.07.07 16:30:07 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:904)[0m
[0m2021.07.07 16:30:07 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:903)[0m
[0m2021.07.07 16:30:07 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:49)[0m
[0m2021.07.07 16:30:07 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:693)[0m
[0m2021.07.07 16:30:07 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:689)[0m
[0m2021.07.07 16:30:07 ERROR 	at org.apache.spark.sql.functions$.$anonfun$udf$6(functions.scala:4667)[0m
[0m2021.07.07 16:30:07 ERROR 	at scala.Option.getOrElse(Option.scala:189)[0m
[0m2021.07.07 16:30:07 ERROR 	at org.apache.spark.sql.functions$.udf(functions.scala:4667)[0m
[0m2021.07.07 16:30:07 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.loadDay$1(SparkMLModelTraining.scala:52)[0m
[0m2021.07.07 16:30:07 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:68)[0m
[0m2021.07.07 16:30:07 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
[0m2021.07.07 16:30:08 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 16:37:39 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 16:37:39 INFO  time: compiled ht3-test in 0.26s[0m
[0m2021.07.07 16:37:39 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:37:41 INFO  time: compiled ht3 in 1.49s[0m
[0m2021.07.07 16:37:42 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 1h 41m 2.552s)[0m
[0m2021.07.07 16:37:42 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 16:37:42 INFO  time: compiled ht3-test in 0.21s[0m
[0m2021.07.07 16:37:43 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 16:37:42 INFO  Listening for transport dt_socket at address: 45123[0m
[0m2021.07.07 16:37:43 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 16:37:43 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 16:37:42 INFO  Trying to attach to remote debuggee VM localhost:45123 .[0m
[0m2021.07.07 16:37:42 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 16:37:44 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 16:37:44 ERROR 21/07/07 16:37:44 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 16:37:44 ERROR 21/07/07 16:37:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 16:37:45 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 16:37:45 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 16:37:45 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 16:37:45 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 16:37:45 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 16:37:45 ERROR 21/07/07 16:37:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 16:37:54 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 16:37:54 ERROR Exception in thread "main" java.lang.UnsupportedOperationException: Schema for type Any is not supported[0m
[0m2021.07.07 16:37:54 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$schemaFor$1(ScalaReflection.scala:769)[0m
[0m2021.07.07 16:37:54 ERROR 	at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69)[0m
[0m2021.07.07 16:37:54 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:904)[0m
[0m2021.07.07 16:37:54 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:903)[0m
[0m2021.07.07 16:37:54 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:49)[0m
[0m2021.07.07 16:37:54 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:693)[0m
[0m2021.07.07 16:37:54 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:689)[0m
[0m2021.07.07 16:37:54 ERROR 	at org.apache.spark.sql.functions$.$anonfun$udf$6(functions.scala:4667)[0m
[0m2021.07.07 16:37:54 ERROR 	at scala.Option.getOrElse(Option.scala:189)[0m
[0m2021.07.07 16:37:54 ERROR 	at org.apache.spark.sql.functions$.udf(functions.scala:4667)[0m
[0m2021.07.07 16:37:54 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.loadDay$1(SparkMLModelTraining.scala:52)[0m
[0m2021.07.07 16:37:54 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:68)[0m
[0m2021.07.07 16:37:54 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
Jul 07, 2021 4:37:55 PM org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer fireError
SEVERE: java.net.SocketException: Broken pipe (Write failed)
org.eclipse.lsp4j.jsonrpc.JsonRpcException: java.net.SocketException: Broken pipe (Write failed)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:72)
	at scala.meta.internal.metals.debug.SocketEndpoint.consume(SocketEndpoint.scala:22)
	at scala.meta.internal.metals.debug.MessageIdAdapter.consume(MessageIdAdapter.scala:43)
	at scala.meta.internal.metals.debug.ServerAdapter.send(ServerAdapter.scala:30)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleClientMessage$1(DebugProxy.scala:89)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToClient$1(DebugProxy.scala:54)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketException: Broken pipe (Write failed)
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)
	at java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:69)
	... 21 more

[0m2021.07.07 16:37:55 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 16:40:11 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:40:13 INFO  time: compiled ht3 in 1.51s[0m
[0m2021.07.07 16:40:28 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:40:29 INFO  time: compiled ht3 in 1.46s[0m
[0m2021.07.07 16:40:40 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 1h 44m 0.522s)[0m
[0m2021.07.07 16:40:40 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 16:40:40 INFO  time: compiled ht3-test in 0.21s[0m
[0m2021.07.07 16:40:41 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 16:40:40 INFO  Listening for transport dt_socket at address: 44679[0m
[0m2021.07.07 16:40:41 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 16:40:41 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 16:40:40 INFO  Trying to attach to remote debuggee VM localhost:44679 .[0m
[0m2021.07.07 16:40:40 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 16:40:42 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 16:40:42 ERROR 21/07/07 16:40:42 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 16:40:42 ERROR 21/07/07 16:40:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 16:40:43 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 16:40:43 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 16:40:43 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 16:40:43 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 16:40:43 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 16:40:43 ERROR 21/07/07 16:40:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 16:40:51 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 16:40:51 INFO  [Ljava.lang.String;@7bc239db[0m
[0m2021.07.07 16:40:53 INFO  +-------------------------+---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 16:40:53 INFO  |instanceId_objectType_num|instanceId_objectType|audit_resourceType| metadata_ownerType|membership_status|log_ownerId|log_authorId|[0m
[0m2021.07.07 16:40:53 INFO  +-------------------------+---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 16:40:53 INFO  |                      0.0|                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 16:40:53 INFO  |                      0.0|                 Post|                 8|         GROUP_OPEN|                A|        4.0|         6.0|[0m
[0m2021.07.07 16:40:53 INFO  |                      0.0|                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        4.0|         6.0|[0m
[0m2021.07.07 16:40:53 INFO  |                      0.0|                 Post|                 7|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 16:40:53 INFO  |                      0.0|                 Post|                14|GROUP_OPEN_OFFICIAL|             null|        4.0|         6.0|[0m
[0m2021.07.07 16:40:53 INFO  |                      0.0|                 Post|                14|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 16:40:53 INFO  |                      1.0|                Photo|                 3|         GROUP_OPEN|             null|        5.0|         6.0|[0m
[0m2021.07.07 16:40:53 INFO  |                      0.0|                 Post|                 8|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 16:40:53 INFO  |                      0.0|                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 16:40:53 INFO  |                      0.0|                 Post|                 7|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 16:40:53 INFO  +-------------------------+---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 16:40:53 INFO  only showing top 10 rows[0m
[0m2021.07.07 16:40:53 INFO  [0m
[0m2021.07.07 16:40:53 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in Map[String,Double]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 2169, 2169, 2187)
[0m2021.07.07 16:41:19 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:41:19 INFO  time: compiled ht3 in 0.34s[0m
[0m2021.07.07 16:41:34 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:41:35 INFO  time: compiled ht3 in 1.39s[0m
[0m2021.07.07 16:42:58 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:42:58 INFO  time: compiled ht3 in 0.13s[0m
[0m2021.07.07 16:43:20 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:43:21 INFO  time: compiled ht3 in 1.35s[0m
[0m2021.07.07 16:43:40 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:43:41 INFO  time: compiled ht3 in 1.34s[0m
[0m2021.07.07 16:50:36 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 16:50:36 INFO  time: compiled ht3-test in 0.22s[0m
[0m2021.07.07 16:50:44 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 16:50:44 INFO  time: compiled ht3-test in 0.2s[0m
[0m2021.07.07 16:50:44 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 16:50:44 INFO  time: compiled ht3-test in 0.12s[0m
[0m2021.07.07 16:50:59 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 16:50:59 INFO  time: compiled ht3-test in 0.22s[0m
[0m2021.07.07 16:50:59 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:51:00 INFO  time: compiled ht3 in 1.35s[0m
[0m2021.07.07 16:51:03 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 1h 54m 23.542s)[0m
[0m2021.07.07 16:51:03 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 16:51:03 INFO  time: compiled ht3-test in 0.2s[0m
[0m2021.07.07 16:51:04 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 16:51:03 INFO  Listening for transport dt_socket at address: 43151[0m
[0m2021.07.07 16:51:04 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 16:51:04 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 16:51:03 INFO  Trying to attach to remote debuggee VM localhost:43151 .[0m
[0m2021.07.07 16:51:03 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 16:51:05 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 16:51:05 ERROR 21/07/07 16:51:05 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 16:51:05 ERROR 21/07/07 16:51:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 16:51:06 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 16:51:06 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 16:51:06 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 16:51:06 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 16:51:06 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 16:51:06 ERROR 21/07/07 16:51:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 16:51:15 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 16:51:15 ERROR Exception in thread "main" java.lang.UnsupportedOperationException: Schema for type AnyVal is not supported[0m
[0m2021.07.07 16:51:15 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$schemaFor$1(ScalaReflection.scala:769)[0m
[0m2021.07.07 16:51:15 ERROR 	at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69)[0m
[0m2021.07.07 16:51:15 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:904)[0m
[0m2021.07.07 16:51:15 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:903)[0m
[0m2021.07.07 16:51:15 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:49)[0m
[0m2021.07.07 16:51:15 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:693)[0m
[0m2021.07.07 16:51:15 ERROR 	at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:689)[0m
[0m2021.07.07 16:51:15 ERROR 	at org.apache.spark.sql.functions$.$anonfun$udf$6(functions.scala:4667)[0m
[0m2021.07.07 16:51:15 ERROR 	at scala.Option.getOrElse(Option.scala:189)[0m
[0m2021.07.07 16:51:15 ERROR 	at org.apache.spark.sql.functions$.udf(functions.scala:4667)[0m
[0m2021.07.07 16:51:15 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.loadDay$1(SparkMLModelTraining.scala:53)[0m
[0m2021.07.07 16:51:15 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:76)[0m
[0m2021.07.07 16:51:15 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
Jul 07, 2021 4:51:16 PM org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer fireError
SEVERE: java.net.SocketException: Broken pipe (Write failed)
org.eclipse.lsp4j.jsonrpc.JsonRpcException: java.net.SocketException: Broken pipe (Write failed)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:72)
	at scala.meta.internal.metals.debug.SocketEndpoint.consume(SocketEndpoint.scala:22)
	at scala.meta.internal.metals.debug.MessageIdAdapter.consume(MessageIdAdapter.scala:43)
	at scala.meta.internal.metals.debug.ServerAdapter.send(ServerAdapter.scala:30)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleClientMessage$1(DebugProxy.scala:89)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToClient$1(DebugProxy.scala:54)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketException: Broken pipe (Write failed)
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)
	at java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:69)
	... 21 more

[0m2021.07.07 16:51:16 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 16:51:44 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 16:51:44 INFO  time: compiled ht3-test in 0.24s[0m
[0m2021.07.07 16:52:00 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:52:00 INFO  time: compiled ht3 in 0.44s[0m
[0m2021.07.07 16:53:05 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:53:06 INFO  time: compiled ht3 in 1.23s[0m
[0m2021.07.07 16:53:11 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 1h 56m 31.012s)[0m
[0m2021.07.07 16:53:11 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 16:53:11 INFO  time: compiled ht3-test in 0.17s[0m
[0m2021.07.07 16:53:11 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 16:53:11 INFO  Listening for transport dt_socket at address: 38291[0m
[0m2021.07.07 16:53:11 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 16:53:11 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 16:53:11 INFO  Trying to attach to remote debuggee VM localhost:38291 .[0m
[0m2021.07.07 16:53:11 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 16:53:13 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 16:53:13 ERROR 21/07/07 16:53:13 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 16:53:13 ERROR 21/07/07 16:53:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 16:53:14 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 16:53:14 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 16:53:14 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 16:53:14 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 16:53:14 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 16:53:14 ERROR 21/07/07 16:53:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 16:53:22 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 16:53:22 INFO  [Ljava.lang.String;@4628f386[0m
[0m2021.07.07 16:53:23 INFO  +-------------------------+---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 16:53:23 INFO  |instanceId_objectType_num|instanceId_objectType|audit_resourceType| metadata_ownerType|membership_status|log_ownerId|log_authorId|[0m
[0m2021.07.07 16:53:23 INFO  +-------------------------+---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 16:53:23 INFO  |                        0|                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 16:53:23 INFO  |                        0|                 Post|                 8|         GROUP_OPEN|                A|        4.0|         6.0|[0m
[0m2021.07.07 16:53:23 INFO  |                        0|                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        4.0|         6.0|[0m
[0m2021.07.07 16:53:23 INFO  |                        0|                 Post|                 7|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 16:53:23 INFO  |                        0|                 Post|                14|GROUP_OPEN_OFFICIAL|             null|        4.0|         6.0|[0m
[0m2021.07.07 16:53:23 INFO  |                        0|                 Post|                14|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 16:53:23 INFO  |                        1|                Photo|                 3|         GROUP_OPEN|             null|        5.0|         6.0|[0m
[0m2021.07.07 16:53:23 INFO  |                        0|                 Post|                 8|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 16:53:23 INFO  |                        0|                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 16:53:23 INFO  |                        0|                 Post|                 7|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 16:53:23 INFO  +-------------------------+---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 16:53:23 INFO  only showing top 10 rows[0m
[0m2021.07.07 16:53:23 INFO  [0m
[0m2021.07.07 16:53:23 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
Jul 07, 2021 4:55:41 PM scala.meta.internal.pc.CompilerAccess retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Jul 07, 2021 4:55:41 PM scala.meta.internal.pc.CompilerAccess handleError
SEVERE: null
java.lang.NullPointerException
	at scala.reflect.internal.Definitions$DefinitionsClass.isByNameParamType(Definitions.scala:417)
	at scala.reflect.internal.TreeInfo.isStableIdent(TreeInfo.scala:137)
	at scala.reflect.internal.TreeInfo.isStableIdentifier(TreeInfo.scala:113)
	at scala.reflect.internal.TreeInfo.isPath(TreeInfo.scala:102)
	at scala.reflect.internal.TreeInfo.admitsTypeSelection(TreeInfo.scala:155)
	at scala.tools.nsc.interactive.Global.stabilizedType(Global.scala:959)
	at scala.tools.nsc.interactive.Global.typedTreeAt(Global.scala:807)
	at scala.meta.internal.pc.HoverProvider.typedHoverTreeAt(HoverProvider.scala:239)
	at scala.meta.internal.pc.HoverProvider.hover(HoverProvider.scala:26)

Jul 07, 2021 4:55:57 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2509
[0m2021.07.07 16:56:21 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 16:56:21 INFO  time: compiled ht3-test in 0.2s[0m
[0m2021.07.07 16:57:09 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 16:57:09 INFO  time: compiled ht3 in 0.28s[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in List[<error>]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 1926, 1926, 1933)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in Map[<error>,Int]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 2043, 2043, 2053)
[0m2021.07.07 17:01:38 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:01:38 INFO  time: compiled ht3 in 0.26s[0m
[0m2021.07.07 17:06:33 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:06:33 INFO  time: compiled ht3 in 0.3s[0m
[0m2021.07.07 17:07:39 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:07:39 INFO  time: compiled ht3 in 0.31s[0m
[0m2021.07.07 17:12:14 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:12:14 INFO  time: compiled ht3 in 0.33s[0m
[0m2021.07.07 17:13:11 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:13:11 INFO  time: compiled ht3 in 0.32s[0m
[0m2021.07.07 17:15:45 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:15:45 INFO  time: compiled ht3 in 0.33s[0m
[0m2021.07.07 17:15:45 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:15:45 INFO  time: compiled ht3 in 0.18s[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in List[A]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 1929, 1929, 1936)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in List[A]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 1929, 1929, 1936)
[0m2021.07.07 17:16:17 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:16:17 INFO  time: compiled ht3 in 0.27s[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in List[<error>]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 1935, 1935, 1942)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in List[<error>]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 1935, 1935, 1942)
[0m2021.07.07 17:16:39 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:16:39 INFO  time: compiled ht3 in 0.29s[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in Map[A,Int]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 2026, 2026, 2036)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in Map[A,Int]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 2026, 2026, 2036)
[0m2021.07.07 17:17:18 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:17:18 INFO  time: compiled ht3 in 0.24s[0m
[0m2021.07.07 17:18:25 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:18:25 INFO  time: compiled ht3 in 0.28s[0m
Exception in thread "pool-5-thread-1" java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[0m2021.07.07 17:25:53 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:25:53 INFO  time: compiled ht3 in 0.33s[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in Map[T,Int]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 2047, 2047, 2057)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in List[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 1950, 1950, 1957)
[0m2021.07.07 17:26:32 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:26:32 INFO  time: compiled ht3 in 0.25s[0m
[0m2021.07.07 17:27:23 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:27:23 INFO  time: compiled ht3 in 0.33s[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in List[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 1990, 1990, 1997)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in reflect.runtime.universe.TypeTag[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 1964, 1964, 1973)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in reflect.runtime.universe.TypeTag[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 1964, 1964, 1973)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in reflect.runtime.universe.TypeTag[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 1964, 1964, 1973)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in reflect.runtime.universe.TypeTag[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 1964, 1964, 1973)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in reflect.runtime.universe.TypeTag[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 1964, 1964, 1973)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in Map[T,Int]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 2144, 2144, 2154)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in Map[T,Int]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 2144, 2144, 2154)
[0m2021.07.07 17:31:14 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:31:14 INFO  time: compiled ht3 in 0.39s[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in reflect.runtime.universe.TypeTag[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 2005, 2005, 2015)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in reflect.runtime.universe.TypeTag[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 2005, 2005, 2015)
[0m2021.07.07 17:31:22 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:31:22 INFO  time: compiled ht3 in 0.34s[0m
[0m2021.07.07 17:31:50 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:31:50 INFO  time: compiled ht3 in 0.39s[0m
[0m2021.07.07 17:32:30 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:32:32 INFO  time: compiled ht3 in 1.4s[0m
[0m2021.07.07 17:32:35 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 2h 35m 55.493s)[0m
[0m2021.07.07 17:32:35 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 17:32:35 INFO  time: compiled ht3-test in 0.25s[0m
[0m2021.07.07 17:32:36 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 17:32:35 INFO  Listening for transport dt_socket at address: 40785[0m
[0m2021.07.07 17:32:36 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 17:32:36 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 17:32:35 INFO  Trying to attach to remote debuggee VM localhost:40785 .[0m
[0m2021.07.07 17:32:35 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 17:32:37 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 17:32:37 ERROR 21/07/07 17:32:37 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 17:32:37 ERROR 21/07/07 17:32:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 17:32:38 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 17:32:38 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 17:32:38 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 17:32:38 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 17:32:38 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 17:32:38 ERROR 21/07/07 17:32:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 17:32:47 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 17:32:47 INFO  [Ljava.lang.String;@5ff12345[0m
[0m2021.07.07 17:32:48 INFO  +-------------------------+---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 17:32:48 INFO  |instanceId_objectType_num|instanceId_objectType|audit_resourceType| metadata_ownerType|membership_status|log_ownerId|log_authorId|[0m
[0m2021.07.07 17:32:48 INFO  +-------------------------+---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 17:32:48 INFO  |                        0|                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 17:32:48 INFO  |                        0|                 Post|                 8|         GROUP_OPEN|                A|        4.0|         6.0|[0m
[0m2021.07.07 17:32:48 INFO  |                        0|                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        4.0|         6.0|[0m
[0m2021.07.07 17:32:48 INFO  |                        0|                 Post|                 7|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:32:48 INFO  |                        0|                 Post|                14|GROUP_OPEN_OFFICIAL|             null|        4.0|         6.0|[0m
[0m2021.07.07 17:32:48 INFO  |                        0|                 Post|                14|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:32:48 INFO  |                        1|                Photo|                 3|         GROUP_OPEN|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:32:48 INFO  |                        0|                 Post|                 8|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:32:48 INFO  |                        0|                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 17:32:48 INFO  |                        0|                 Post|                 7|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 17:32:48 INFO  +-------------------------+---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 17:32:48 INFO  only showing top 10 rows[0m
[0m2021.07.07 17:32:48 INFO  [0m
[0m2021.07.07 17:32:49 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 17:33:56 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:33:57 INFO  time: compiled ht3 in 1.35s[0m
[0m2021.07.07 17:34:01 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 2h 37m 21.564s)[0m
[0m2021.07.07 17:34:01 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 17:34:01 INFO  time: compiled ht3-test in 0.19s[0m
[0m2021.07.07 17:34:02 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 17:34:01 INFO  Listening for transport dt_socket at address: 46615[0m
[0m2021.07.07 17:34:02 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 17:34:02 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 17:34:01 INFO  Trying to attach to remote debuggee VM localhost:46615 .[0m
[0m2021.07.07 17:34:01 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 17:34:03 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 17:34:03 ERROR 21/07/07 17:34:03 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 17:34:03 ERROR 21/07/07 17:34:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 17:34:04 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 17:34:04 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 17:34:04 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 17:34:04 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 17:34:04 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 17:34:04 ERROR 21/07/07 17:34:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 17:34:13 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 17:34:13 INFO  [Ljava.lang.String;@4d5d945d[0m
[0m2021.07.07 17:34:14 INFO  +-------------------------+---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 17:34:14 INFO  |instanceId_objectType_num|instanceId_objectType|audit_resourceType| metadata_ownerType|membership_status|log_ownerId|log_authorId|[0m
[0m2021.07.07 17:34:14 INFO  +-------------------------+---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 17:34:14 INFO  |                        0|                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 17:34:14 INFO  |                        0|                 Post|                 8|         GROUP_OPEN|                A|        4.0|         6.0|[0m
[0m2021.07.07 17:34:14 INFO  |                        0|                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        4.0|         6.0|[0m
[0m2021.07.07 17:34:14 INFO  |                        0|                 Post|                 7|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:34:14 INFO  |                        0|                 Post|                14|GROUP_OPEN_OFFICIAL|             null|        4.0|         6.0|[0m
[0m2021.07.07 17:34:14 INFO  |                        0|                 Post|                14|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:34:14 INFO  |                        1|                Photo|                 3|         GROUP_OPEN|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:34:14 INFO  |                        0|                 Post|                 8|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:34:14 INFO  |                        0|                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 17:34:14 INFO  |                        0|                 Post|                 7|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 17:34:14 INFO  +-------------------------+---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 17:34:14 INFO  only showing top 10 rows[0m
[0m2021.07.07 17:34:14 INFO  [0m
[0m2021.07.07 17:34:15 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 17:34:47 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 2h 38m 6.86s)[0m
[0m2021.07.07 17:34:47 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:34:47 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:34:48 INFO  time: compiled ht3 in 1.27s[0m
[0m2021.07.07 17:34:48 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 17:34:48 INFO  time: compiled ht3-test in 0.1s[0m
[0m2021.07.07 17:34:48 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 17:34:48 INFO  Listening for transport dt_socket at address: 34685[0m
[0m2021.07.07 17:34:48 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 17:34:48 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 17:34:48 INFO  Trying to attach to remote debuggee VM localhost:34685 .[0m
[0m2021.07.07 17:34:48 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 17:34:50 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 17:34:50 ERROR 21/07/07 17:34:50 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 17:34:50 ERROR 21/07/07 17:34:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 17:34:51 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 17:34:51 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 17:34:51 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 17:34:51 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 17:34:51 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 17:34:51 ERROR 21/07/07 17:34:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 17:34:59 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 17:34:59 INFO  [Ljava.lang.String;@25291901[0m
[0m2021.07.07 17:35:01 INFO  +-------------------------+---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 17:35:01 INFO  |instanceId_objectType_num|instanceId_objectType|audit_resourceType| metadata_ownerType|membership_status|log_ownerId|log_authorId|[0m
[0m2021.07.07 17:35:01 INFO  +-------------------------+---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 17:35:01 INFO  |                        0|                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 17:35:01 INFO  |                        0|                 Post|                 8|         GROUP_OPEN|                A|        4.0|         6.0|[0m
[0m2021.07.07 17:35:01 INFO  |                        0|                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        4.0|         6.0|[0m
[0m2021.07.07 17:35:01 INFO  |                        0|                 Post|                 7|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:35:01 INFO  |                        0|                 Post|                14|GROUP_OPEN_OFFICIAL|             null|        4.0|         6.0|[0m
[0m2021.07.07 17:35:01 INFO  |                        0|                 Post|                14|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:35:01 INFO  |                        1|                Photo|                 3|         GROUP_OPEN|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:35:01 INFO  |                        0|                 Post|                 8|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:35:01 INFO  |                        0|                 Post|                 8|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 17:35:01 INFO  |                        0|                 Post|                 7|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 17:35:01 INFO  +-------------------------+---------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 17:35:01 INFO  only showing top 10 rows[0m
[0m2021.07.07 17:35:01 INFO  [0m
[0m2021.07.07 17:35:01 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 17:36:35 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:36:37 INFO  time: compiled ht3 in 1.32s[0m
[0m2021.07.07 17:36:39 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 2h 39m 58.702s)[0m
[0m2021.07.07 17:36:39 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 17:36:39 INFO  time: compiled ht3-test in 0.25s[0m
[0m2021.07.07 17:36:39 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 17:36:39 INFO  Listening for transport dt_socket at address: 57487[0m
[0m2021.07.07 17:36:39 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 17:36:39 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 17:36:39 INFO  Trying to attach to remote debuggee VM localhost:57487 .[0m
[0m2021.07.07 17:36:39 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 17:36:40 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 17:36:40 ERROR 21/07/07 17:36:40 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 17:36:40 ERROR 21/07/07 17:36:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 17:36:41 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 17:36:41 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 17:36:41 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 17:36:41 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 17:36:41 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 17:36:41 ERROR 21/07/07 17:36:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 17:36:50 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 17:36:50 INFO  [Ljava.lang.String;@23263586[0m
[0m2021.07.07 17:36:51 INFO  +-------------------------+-------------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 17:36:51 INFO  |instanceId_objectType_num|instanceId_objectType_num|audit_resourceType| metadata_ownerType|membership_status|log_ownerId|log_authorId|[0m
[0m2021.07.07 17:36:51 INFO  +-------------------------+-------------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 17:36:51 INFO  |                        0|                        0|                 8|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 17:36:51 INFO  |                        0|                        0|                 8|         GROUP_OPEN|                A|        4.0|         6.0|[0m
[0m2021.07.07 17:36:51 INFO  |                        0|                        0|                 8|GROUP_OPEN_OFFICIAL|                A|        4.0|         6.0|[0m
[0m2021.07.07 17:36:51 INFO  |                        0|                        0|                 7|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:36:51 INFO  |                        0|                        0|                14|GROUP_OPEN_OFFICIAL|             null|        4.0|         6.0|[0m
[0m2021.07.07 17:36:51 INFO  |                        0|                        0|                14|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:36:51 INFO  |                        1|                        1|                 3|         GROUP_OPEN|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:36:51 INFO  |                        0|                        0|                 8|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:36:51 INFO  |                        0|                        0|                 8|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 17:36:51 INFO  |                        0|                        0|                 7|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 17:36:51 INFO  +-------------------------+-------------------------+------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 17:36:51 INFO  only showing top 10 rows[0m
[0m2021.07.07 17:36:51 INFO  [0m
[0m2021.07.07 17:36:51 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 17:37:26 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:37:27 INFO  time: compiled ht3 in 1.41s[0m
[0m2021.07.07 17:37:36 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 2h 40m 55.711s)[0m
[0m2021.07.07 17:37:36 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 17:37:36 INFO  time: compiled ht3-test in 0.23s[0m
[0m2021.07.07 17:37:36 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 17:37:36 INFO  Listening for transport dt_socket at address: 44633[0m
[0m2021.07.07 17:37:36 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 17:37:36 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 17:37:36 INFO  Trying to attach to remote debuggee VM localhost:44633 .[0m
[0m2021.07.07 17:37:36 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 17:37:37 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 17:37:37 ERROR 21/07/07 17:37:37 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 17:37:37 ERROR 21/07/07 17:37:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 17:37:38 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 17:37:38 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 17:37:38 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 17:37:38 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 17:37:38 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 17:37:38 ERROR 21/07/07 17:37:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 17:37:47 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 17:37:47 INFO  [Ljava.lang.String;@23263586[0m
[0m2021.07.07 17:37:48 INFO  +-------------------------+----------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 17:37:48 INFO  |instanceId_objectType_num|audit_resourceType_num| metadata_ownerType|membership_status|log_ownerId|log_authorId|[0m
[0m2021.07.07 17:37:48 INFO  +-------------------------+----------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 17:37:48 INFO  |                        0|                     3|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 17:37:48 INFO  |                        0|                     3|         GROUP_OPEN|                A|        4.0|         6.0|[0m
[0m2021.07.07 17:37:48 INFO  |                        0|                     3|GROUP_OPEN_OFFICIAL|                A|        4.0|         6.0|[0m
[0m2021.07.07 17:37:48 INFO  |                        0|                     2|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:37:48 INFO  |                        0|                     4|GROUP_OPEN_OFFICIAL|             null|        4.0|         6.0|[0m
[0m2021.07.07 17:37:48 INFO  |                        0|                     4|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:37:48 INFO  |                        1|                     0|         GROUP_OPEN|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:37:48 INFO  |                        0|                     3|GROUP_OPEN_OFFICIAL|             null|        5.0|         6.0|[0m
[0m2021.07.07 17:37:48 INFO  |                        0|                     3|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 17:37:48 INFO  |                        0|                     2|GROUP_OPEN_OFFICIAL|                A|        5.0|         6.0|[0m
[0m2021.07.07 17:37:48 INFO  +-------------------------+----------------------+-------------------+-----------------+-----------+------------+[0m
[0m2021.07.07 17:37:48 INFO  only showing top 10 rows[0m
[0m2021.07.07 17:37:48 INFO  [0m
[0m2021.07.07 17:37:48 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 17:39:38 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:39:39 INFO  time: compiled ht3 in 1.21s[0m
[0m2021.07.07 17:39:57 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 17:39:57 INFO  time: compiled ht3-test in 0.2s[0m
Jul 07, 2021 5:44:48 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.InterruptedException
java.util.concurrent.CompletionException: java.lang.InterruptedException
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:319)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:718)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:42)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.pc.VirtualFileParams.checkCanceled(VirtualFileParams.java:25)
	at scala.meta.internal.pc.CompletionProvider.$anonfun$completions$1(CompletionProvider.scala:77)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
	at scala.collection.Iterator.toStream(Iterator.scala:1415)
	at scala.collection.Iterator.toStream$(Iterator.scala:1414)
	at scala.collection.AbstractIterator.toStream(Iterator.scala:1429)
	at scala.collection.Iterator.$anonfun$toStream$1(Iterator.scala:1415)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1171)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1161)
	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1061)
	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1050)
	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1050)
	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1055)
	at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:31)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:134)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:40)
	at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.write(TypeAdapterRuntimeTypeWrapper.java:69)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.write(ReflectiveTypeAdapterFactory.java:127)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.write(ReflectiveTypeAdapterFactory.java:245)
	at com.google.gson.Gson.toJson(Gson.java:704)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:423)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:55)
	at com.google.gson.Gson.toJson(Gson.java:704)
	at com.google.gson.Gson.toJson(Gson.java:683)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:145)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:140)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:59)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.lambda$handleRequest$1(RemoteEndpoint.java:281)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:714)
	... 8 more

[0m2021.07.07 17:45:35 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 17:45:35 INFO  time: compiled ht3-test in 0.23s[0m
[0m2021.07.07 17:46:23 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 17:46:23 INFO  time: compiled ht3-test in 0.2s[0m
[0m2021.07.07 17:46:23 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 17:46:23 INFO  time: compiled ht3-test in 0.11s[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in List[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 1969, 1969, 1976)
[0m2021.07.07 17:47:58 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 17:47:58 INFO  time: compiled ht3-test in 0.19s[0m
[0m2021.07.07 17:48:05 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:48:05 INFO  time: compiled ht3 in 0.13s[0m
[0m2021.07.07 17:49:31 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:49:32 INFO  time: compiled ht3 in 1.38s[0m
[0m2021.07.07 17:49:45 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:49:46 INFO  time: compiled ht3 in 1.26s[0m
[0m2021.07.07 17:49:49 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 2h 53m 9.532s)[0m
[0m2021.07.07 17:49:49 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 17:49:49 INFO  time: compiled ht3-test in 0.18s[0m
[0m2021.07.07 17:49:50 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 17:49:49 INFO  Listening for transport dt_socket at address: 44195[0m
[0m2021.07.07 17:49:50 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 17:49:50 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 17:49:49 INFO  Trying to attach to remote debuggee VM localhost:44195 .[0m
[0m2021.07.07 17:49:49 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 17:49:51 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 17:49:51 ERROR 21/07/07 17:49:51 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 17:49:51 ERROR 21/07/07 17:49:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 17:49:52 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 17:49:52 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 17:49:52 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 17:49:52 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 17:49:52 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 17:49:52 ERROR 21/07/07 17:49:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 17:50:01 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 17:50:01 INFO  [Ljava.lang.String;@a3f1f32[0m
[0m2021.07.07 17:50:02 INFO  +-------------------------+----------------------+----------------------+---------------------+-----------+------------+[0m
[0m2021.07.07 17:50:02 INFO  |instanceId_objectType_num|audit_resourceType_num|metadata_ownerType_num|membership_status_num|log_ownerId|log_authorId|[0m
[0m2021.07.07 17:50:02 INFO  +-------------------------+----------------------+----------------------+---------------------+-----------+------------+[0m
[0m2021.07.07 17:50:02 INFO  |                        0|                     3|                  null|                    1|        5.0|         6.0|[0m
[0m2021.07.07 17:50:02 INFO  |                        0|                     3|                  null|                    1|        4.0|         6.0|[0m
[0m2021.07.07 17:50:02 INFO  |                        0|                     3|                  null|                    1|        4.0|         6.0|[0m
[0m2021.07.07 17:50:02 INFO  |                        0|                     2|                  null|                    8|        5.0|         6.0|[0m
[0m2021.07.07 17:50:02 INFO  |                        0|                     4|                  null|                    8|        4.0|         6.0|[0m
[0m2021.07.07 17:50:02 INFO  |                        0|                     4|                  null|                    8|        5.0|         6.0|[0m
[0m2021.07.07 17:50:02 INFO  |                        1|                     0|                  null|                    8|        5.0|         6.0|[0m
[0m2021.07.07 17:50:02 INFO  |                        0|                     3|                  null|                    8|        5.0|         6.0|[0m
[0m2021.07.07 17:50:02 INFO  |                        0|                     3|                  null|                    1|        5.0|         6.0|[0m
[0m2021.07.07 17:50:02 INFO  |                        0|                     2|                  null|                    1|        5.0|         6.0|[0m
[0m2021.07.07 17:50:02 INFO  +-------------------------+----------------------+----------------------+---------------------+-----------+------------+[0m
[0m2021.07.07 17:50:02 INFO  only showing top 10 rows[0m
[0m2021.07.07 17:50:02 INFO  [0m
[0m2021.07.07 17:50:02 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 17:50:29 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 17:50:29 INFO  time: compiled ht3-test in 0.2s[0m
[0m2021.07.07 17:50:31 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:50:34 INFO  time: compiled ht3 in 2.27s[0m
[0m2021.07.07 17:50:35 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 2h 53m 55.408s)[0m
[0m2021.07.07 17:50:35 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 17:50:35 INFO  time: compiled ht3-test in 0.21s[0m
[0m2021.07.07 17:50:36 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 17:50:35 INFO  Listening for transport dt_socket at address: 47631[0m
[0m2021.07.07 17:50:36 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 17:50:36 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 17:50:35 INFO  Trying to attach to remote debuggee VM localhost:47631 .[0m
[0m2021.07.07 17:50:35 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 17:50:37 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 17:50:37 ERROR 21/07/07 17:50:37 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 17:50:37 ERROR 21/07/07 17:50:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 17:50:38 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 17:50:38 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 17:50:38 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 17:50:38 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 17:50:38 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 17:50:38 ERROR 21/07/07 17:50:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 17:50:46 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 17:50:46 INFO  [Ljava.lang.String;@a3f1f32[0m
[0m2021.07.07 17:50:48 INFO  +-------------------------+----------------------+----------------------+---------------------+-----------+------------+[0m
[0m2021.07.07 17:50:48 INFO  |instanceId_objectType_num|audit_resourceType_num|metadata_ownerType_num|membership_status_num|log_ownerId|log_authorId|[0m
[0m2021.07.07 17:50:48 INFO  +-------------------------+----------------------+----------------------+---------------------+-----------+------------+[0m
[0m2021.07.07 17:50:48 INFO  |                        0|                     3|                     0|                    1|        5.0|         6.0|[0m
[0m2021.07.07 17:50:48 INFO  |                        0|                     3|                     1|                    1|        4.0|         6.0|[0m
[0m2021.07.07 17:50:48 INFO  |                        0|                     3|                     0|                    1|        4.0|         6.0|[0m
[0m2021.07.07 17:50:48 INFO  |                        0|                     2|                     0|                    8|        5.0|         6.0|[0m
[0m2021.07.07 17:50:48 INFO  |                        0|                     4|                     0|                    8|        4.0|         6.0|[0m
[0m2021.07.07 17:50:48 INFO  |                        0|                     4|                     0|                    8|        5.0|         6.0|[0m
[0m2021.07.07 17:50:48 INFO  |                        1|                     0|                     1|                    8|        5.0|         6.0|[0m
[0m2021.07.07 17:50:48 INFO  |                        0|                     3|                     0|                    8|        5.0|         6.0|[0m
[0m2021.07.07 17:50:48 INFO  |                        0|                     3|                     0|                    1|        5.0|         6.0|[0m
[0m2021.07.07 17:50:48 INFO  |                        0|                     2|                     0|                    1|        5.0|         6.0|[0m
[0m2021.07.07 17:50:48 INFO  +-------------------------+----------------------+----------------------+---------------------+-----------+------------+[0m
[0m2021.07.07 17:50:48 INFO  only showing top 10 rows[0m
[0m2021.07.07 17:50:48 INFO  [0m
[0m2021.07.07 17:50:48 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 17:51:28 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 17:51:29 INFO  time: compiled ht3 in 1.23s[0m
[0m2021.07.07 22:06:29 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 22:06:31 INFO  time: compiled ht3 in 1.99s[0m
[0m2021.07.07 22:06:33 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 22:06:33 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 7h 9m 52.759s)[0m
[0m2021.07.07 22:06:33 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 22:06:34 INFO  time: compiled ht3 in 1.04s[0m
[0m2021.07.07 22:06:34 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 22:06:34 INFO  time: compiled ht3 in 0.66s[0m
Jul 07, 2021 10:06:34 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
java.util.concurrent.CompletionException: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:29)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:26)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:237)
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:229)
	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417)
	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
	... 4 more

[0m2021.07.07 22:06:47 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 7h 10m 7.613s)[0m
[0m2021.07.07 22:06:47 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 22:06:47 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 22:06:48 INFO  time: compiled ht3 in 1.05s[0m
[0m2021.07.07 22:06:48 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 22:06:48 INFO  time: compiled ht3 in 0.59s[0m
Jul 07, 2021 10:06:49 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
java.util.concurrent.CompletionException: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:29)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:26)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:237)
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:229)
	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417)
	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
	... 4 more

Jul 07, 2021 10:42:55 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5361
Jul 07, 2021 10:42:55 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5362
[0m2021.07.07 23:08:51 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 23:08:53 INFO  time: compiled ht3-test in 1.67s[0m
Jul 07, 2021 11:12:42 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5771
[0m2021.07.07 23:14:41 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 23:14:43 INFO  time: compiled ht3 in 1.59s[0m
[0m2021.07.07 23:16:00 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 23:16:01 INFO  time: compiled ht3 in 1.41s[0m
[0m2021.07.07 23:16:08 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 23:16:09 INFO  time: compiled ht3 in 1.17s[0m
[0m2021.07.07 23:18:16 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 23:18:20 INFO  time: compiled ht3 in 3.73s[0m
[0m2021.07.07 23:18:25 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 8h 21m 45.495s)[0m
[0m2021.07.07 23:18:25 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 23:18:26 INFO  time: compiled ht3-test in 0.78s[0m
[0m2021.07.07 23:18:26 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 23:18:26 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 23:18:27 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 23:18:26 INFO  Listening for transport dt_socket at address: 33337[0m
[0m2021.07.07 23:18:26 INFO  Trying to attach to remote debuggee VM localhost:33337 .[0m
[0m2021.07.07 23:18:26 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 23:18:28 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 23:18:28 ERROR 21/07/07 23:18:28 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 23:18:28 ERROR 21/07/07 23:18:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 23:18:29 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 23:18:29 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 23:18:29 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 23:18:29 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 23:18:29 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 23:18:29 ERROR 21/07/07 23:18:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 23:18:40 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 23:18:40 INFO  [Ljava.lang.String;@4dfe56dc[0m
[0m2021.07.07 23:18:40 ERROR 21/07/07 23:18:41 ERROR Instrumentation: java.lang.IllegalArgumentException: Data type string of column createdTime is not supported.[0m
[0m2021.07.07 23:18:40 ERROR Data type string of column auditedTime is not supported.[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:168)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)[0m
[0m2021.07.07 23:18:40 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)[0m
[0m2021.07.07 23:18:40 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)[0m
[0m2021.07.07 23:18:40 ERROR 	at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:198)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:134)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m2021.07.07 23:18:40 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)[0m
[0m2021.07.07 23:18:40 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:100)[0m
[0m2021.07.07 23:18:40 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
[0m2021.07.07 23:18:40 ERROR [0m
[0m2021.07.07 23:18:40 ERROR Exception in thread "main" java.lang.IllegalArgumentException: Data type string of column createdTime is not supported.[0m
[0m2021.07.07 23:18:40 ERROR Data type string of column auditedTime is not supported.[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:168)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)[0m
[0m2021.07.07 23:18:40 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)[0m
[0m2021.07.07 23:18:40 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)[0m
[0m2021.07.07 23:18:40 ERROR 	at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:198)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:134)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m2021.07.07 23:18:40 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m2021.07.07 23:18:40 ERROR 	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)[0m
[0m2021.07.07 23:18:40 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:100)[0m
[0m2021.07.07 23:18:40 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
[0m2021.07.07 23:18:41 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 23:19:29 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 23:19:32 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 8h 22m 52.054s)[0m
[0m2021.07.07 23:19:32 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 23:19:32 INFO  time: compiled ht3 in 0.48s[0m
[0m2021.07.07 23:19:32 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 23:19:33 INFO  time: compiled ht3-test in 0.39s[0m
[0m2021.07.07 23:19:33 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 23:19:33 INFO  Listening for transport dt_socket at address: 43945[0m
[0m2021.07.07 23:19:33 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 23:19:33 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 23:19:33 INFO  Trying to attach to remote debuggee VM localhost:43945 .[0m
[0m2021.07.07 23:19:33 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 23:19:35 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 23:19:35 ERROR 21/07/07 23:19:35 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 23:19:35 ERROR 21/07/07 23:19:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 23:19:36 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 23:19:36 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 23:19:36 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 23:19:36 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 23:19:36 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 23:19:36 ERROR 21/07/07 23:19:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 23:19:44 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 23:19:44 INFO  [Ljava.lang.String;@a3f1f32[0m
[0m2021.07.07 23:19:44 ERROR 21/07/07 23:19:45 ERROR Instrumentation: java.lang.IllegalArgumentException: Data type string of column auditedTime is not supported.[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:168)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)[0m
[0m2021.07.07 23:19:44 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)[0m
[0m2021.07.07 23:19:44 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)[0m
[0m2021.07.07 23:19:44 ERROR 	at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:198)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:134)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m2021.07.07 23:19:44 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)[0m
[0m2021.07.07 23:19:44 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:100)[0m
[0m2021.07.07 23:19:44 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
[0m2021.07.07 23:19:44 ERROR [0m
[0m2021.07.07 23:19:44 ERROR Exception in thread "main" java.lang.IllegalArgumentException: Data type string of column auditedTime is not supported.[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:168)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)[0m
[0m2021.07.07 23:19:44 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)[0m
[0m2021.07.07 23:19:44 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)[0m
[0m2021.07.07 23:19:44 ERROR 	at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:198)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:134)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m2021.07.07 23:19:44 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m2021.07.07 23:19:44 ERROR 	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)[0m
[0m2021.07.07 23:19:44 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:100)[0m
[0m2021.07.07 23:19:44 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
[0m2021.07.07 23:19:46 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 23:20:14 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 23:20:17 INFO  time: compiled ht3 in 2.52s[0m
[0m2021.07.07 23:20:17 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 8h 23m 36.828s)[0m
[0m2021.07.07 23:20:17 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 23:20:17 INFO  time: compiled ht3-test in 0.2s[0m
[0m2021.07.07 23:20:17 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 23:20:17 INFO  Listening for transport dt_socket at address: 60331[0m
[0m2021.07.07 23:20:17 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 23:20:17 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 23:20:17 INFO  Trying to attach to remote debuggee VM localhost:60331 .[0m
[0m2021.07.07 23:20:17 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 23:20:18 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 23:20:18 ERROR 21/07/07 23:20:18 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 23:20:18 ERROR 21/07/07 23:20:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 23:20:19 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 23:20:19 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 23:20:19 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 23:20:19 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 23:20:19 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 23:20:19 ERROR 21/07/07 23:20:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 23:20:28 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 23:20:28 INFO  [Ljava.lang.String;@a3f1f32[0m
[0m2021.07.07 23:20:33 ERROR 21/07/07 23:20:33 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.07 23:20:33 ERROR 21/07/07 23:20:33 ERROR Executor: Exception in task 1.0 in stage 16.0 (TID 167)[0m
[0m2021.07.07 23:20:33 ERROR org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$4302/0x0000000841967840: (struct<audit_pos_double_vecAssembler_0ad2c484cfbd:double,metadata_numCompanions_double_vecAssembler_0ad2c484cfbd:double,metadata_numPhotos_double_vecAssembler_0ad2c484cfbd:double,metadata_numPolls_double_vecAssembler_0ad2c484cfbd:double,metadata_numSymbols_double_vecAssembler_0ad2c484cfbd:double,metadata_numTokens_double_vecAssembler_0ad2c484cfbd:double,metadata_numVideos_double_vecAssembler_0ad2c484cfbd:double,metadata_totalVideoLength_double_vecAssembler_0ad2c484cfbd:double,userOwnerCounters_USER_FEED_REMOVE:double,userOwnerCounters_USER_PROFILE_VIEW:double,userOwnerCounters_VOTE_POLL:double,userOwnerCounters_USER_SEND_MESSAGE:double,userOwnerCounters_USER_DELETE_MESSAGE:double,userOwnerCounters_USER_INTERNAL_LIKE:double,userOwnerCounters_USER_INTERNAL_UNLIKE:double,userOwnerCounters_USER_STATUS_COMMENT_CREATE:double,userOwnerCounters_PHOTO_COMMENT_CREATE:double,userOwnerCounters_MOVIE_COMMENT_CREATE:double,userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,userOwnerCounters_COMMENT_INTERNAL_LIKE:double,userOwnerCounters_USER_FORUM_MESSAGE_CREATE:double,userOwnerCounters_PHOTO_MARK_CREATE:double,userOwnerCounters_PHOTO_VIEW:double,userOwnerCounters_PHOTO_PIN_BATCH_CREATE:double,userOwnerCounters_PHOTO_PIN_UPDATE:double,userOwnerCounters_USER_PRESENT_SEND:double,userOwnerCounters_UNKNOWN:double,userOwnerCounters_CREATE_TOPIC:double,userOwnerCounters_CREATE_IMAGE:double,userOwnerCounters_CREATE_MOVIE:double,userOwnerCounters_CREATE_COMMENT:double,userOwnerCounters_CREATE_LIKE:double,userOwnerCounters_TEXT:double,userOwnerCounters_IMAGE:double,userOwnerCounters_VIDEO:double,ownerUserCounters_USER_FEED_REMOVE:double,ownerUserCounters_USER_PROFILE_VIEW:double,ownerUserCounters_VOTE_POLL:double,ownerUserCounters_USER_SEND_MESSAGE:double,ownerUserCounters_USER_DELETE_MESSAGE:double,ownerUserCounters_USER_INTERNAL_LIKE:double,ownerUserCounters_USER_INTERNAL_UNLIKE:double,ownerUserCounters_USER_STATUS_COMMENT_CREATE:double,ownerUserCounters_PHOTO_COMMENT_CREATE:double,ownerUserCounters_MOVIE_COMMENT_CREATE:double,ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,ownerUserCounters_COMMENT_INTERNAL_LIKE:double,ownerUserCounters_USER_FORUM_MESSAGE_CREATE:double,ownerUserCounters_PHOTO_MARK_CREATE:double,ownerUserCounters_PHOTO_VIEW:double,ownerUserCounters_PHOTO_PIN_BATCH_CREATE:double,ownerUserCounters_PHOTO_PIN_UPDATE:double,ownerUserCounters_USER_PRESENT_SEND:double,ownerUserCounters_UNKNOWN:double,ownerUserCounters_CREATE_TOPIC:double,ownerUserCounters_CREATE_IMAGE:double,ownerUserCounters_CREATE_MOVIE:double,ownerUserCounters_CREATE_COMMENT:double,ownerUserCounters_CREATE_LIKE:double,ownerUserCounters_TEXT:double,ownerUserCounters_IMAGE:double,ownerUserCounters_VIDEO:double,owner_gender_double_vecAssembler_0ad2c484cfbd:double,owner_status_double_vecAssembler_0ad2c484cfbd:double,owner_ID_country_double_vecAssembler_0ad2c484cfbd:double,owner_ID_Location_double_vecAssembler_0ad2c484cfbd:double,owner_is_active_double_vecAssembler_0ad2c484cfbd:double,owner_is_deleted_double_vecAssembler_0ad2c484cfbd:double,owner_is_abused_double_vecAssembler_0ad2c484cfbd:double,owner_is_activated_double_vecAssembler_0ad2c484cfbd:double,owner_change_datime_double_vecAssembler_0ad2c484cfbd:double,owner_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,owner_region_double_vecAssembler_0ad2c484cfbd:double,user_create_date_double_vecAssembler_0ad2c484cfbd:double,user_birth_date_double_vecAssembler_0ad2c484cfbd:double,user_gender_double_vecAssembler_0ad2c484cfbd:double,user_status_double_vecAssembler_0ad2c484cfbd:double,user_ID_country_double_vecAssembler_0ad2c484cfbd:double,user_ID_Location_double_vecAssembler_0ad2c484cfbd:double,user_is_active_double_vecAssembler_0ad2c484cfbd:double,user_is_deleted_double_vecAssembler_0ad2c484cfbd:double,user_is_abused_double_vecAssembler_0ad2c484cfbd:double,user_is_activated_double_vecAssembler_0ad2c484cfbd:double,user_change_datime_double_vecAssembler_0ad2c484cfbd:double,user_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,user_region_double_vecAssembler_0ad2c484cfbd:double,auditweights_ageMs:double,auditweights_closed:double,auditweights_ctr_gender:double,auditweights_ctr_high:double,auditweights_ctr_negative:double,auditweights_dailyRecency:double,auditweights_feedOwner_RECOMMENDED_GROUP:double,auditweights_feedStats:double,auditweights_friendCommentFeeds:double,auditweights_friendCommenters:double,auditweights_friendLikes:double,auditweights_friendLikes_actors:double,auditweights_hasDetectedText:double,auditweights_hasText:double,auditweights_isPymk:double,auditweights_isRandom:double,auditweights_likersFeedStats_hyper:double,auditweights_likersSvd_prelaunch_hyper:double,auditweights_matrix:double,auditweights_notOriginalPhoto:double,auditweights_numDislikes:double,auditweights_numLikes:double,auditweights_numShows:double,auditweights_onlineVideo:double,auditweights_partAge:double,auditweights_partCtr:double,auditweights_partSvd:double,auditweights_processedVideo:double,auditweights_relationMasks:double,auditweights_source_LIVE_TOP:double,auditweights_source_MOVIE_TOP:double,auditweights_svd_prelaunch:double,auditweights_svd_spark:double,auditweights_userAge:double,auditweights_userOwner_CREATE_COMMENT:double,auditweights_userOwner_CREATE_IMAGE:double,auditweights_userOwner_CREATE_LIKE:double,auditweights_userOwner_IMAGE:double,auditweights_userOwner_MOVIE_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_MARK_CREATE:double,auditweights_userOwner_PHOTO_VIEW:double,auditweights_userOwner_TEXT:double,auditweights_userOwner_UNKNOWN:double,auditweights_userOwner_USER_DELETE_MESSAGE:double,auditweights_userOwner_USER_FEED_REMOVE:double,auditweights_userOwner_USER_FORUM_MESSAGE_CREATE:double,auditweights_userOwner_USER_INTERNAL_LIKE:double,auditweights_userOwner_USER_INTERNAL_UNLIKE:double,auditweights_userOwner_USER_PRESENT_SEND:double,auditweights_userOwner_USER_PROFILE_VIEW:double,auditweights_userOwner_USER_SEND_MESSAGE:double,auditweights_userOwner_USER_STATUS_COMMENT_CREATE:double,auditweights_userOwner_VIDEO:double,auditweights_userOwner_VOTE_POLL:double,auditweights_x_ActorsRelations_double_vecAssembler_0ad2c484cfbd:double,auditweights_likersSvd_spark_hyper:double,auditweights_source_PROMO:double,timeDelta_double_vecAssembler_0ad2c484cfbd:double,createdHour_double_vecAssembler_0ad2c484cfbd:double,auditedHour_double_vecAssembler_0ad2c484cfbd:double,instanceId_objectType_num_double_vecAssembler_0ad2c484cfbd:double,audit_resourceType_num_double_vecAssembler_0ad2c484cfbd:double,metadata_ownerType_num_double_vecAssembler_0ad2c484cfbd:double,membership_status_num_double_vecAssembler_0ad2c484cfbd:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.07 23:20:33 ERROR Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider[0m
[0m2021.07.07 23:20:33 ERROR removing nulls from dataset or using handleInvalid = "keep" or "skip".[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)[0m
[0m2021.07.07 23:20:33 ERROR 	... 25 more[0m
[0m2021.07.07 23:20:33 ERROR 21/07/07 23:20:33 ERROR Executor: Exception in task 6.0 in stage 16.0 (TID 172)[0m
[0m2021.07.07 23:20:33 ERROR org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$4302/0x0000000841967840: (struct<audit_pos_double_vecAssembler_0ad2c484cfbd:double,metadata_numCompanions_double_vecAssembler_0ad2c484cfbd:double,metadata_numPhotos_double_vecAssembler_0ad2c484cfbd:double,metadata_numPolls_double_vecAssembler_0ad2c484cfbd:double,metadata_numSymbols_double_vecAssembler_0ad2c484cfbd:double,metadata_numTokens_double_vecAssembler_0ad2c484cfbd:double,metadata_numVideos_double_vecAssembler_0ad2c484cfbd:double,metadata_totalVideoLength_double_vecAssembler_0ad2c484cfbd:double,userOwnerCounters_USER_FEED_REMOVE:double,userOwnerCounters_USER_PROFILE_VIEW:double,userOwnerCounters_VOTE_POLL:double,userOwnerCounters_USER_SEND_MESSAGE:double,userOwnerCounters_USER_DELETE_MESSAGE:double,userOwnerCounters_USER_INTERNAL_LIKE:double,userOwnerCounters_USER_INTERNAL_UNLIKE:double,userOwnerCounters_USER_STATUS_COMMENT_CREATE:double,userOwnerCounters_PHOTO_COMMENT_CREATE:double,userOwnerCounters_MOVIE_COMMENT_CREATE:double,userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,userOwnerCounters_COMMENT_INTERNAL_LIKE:double,userOwnerCounters_USER_FORUM_MESSAGE_CREATE:double,userOwnerCounters_PHOTO_MARK_CREATE:double,userOwnerCounters_PHOTO_VIEW:double,userOwnerCounters_PHOTO_PIN_BATCH_CREATE:double,userOwnerCounters_PHOTO_PIN_UPDATE:double,userOwnerCounters_USER_PRESENT_SEND:double,userOwnerCounters_UNKNOWN:double,userOwnerCounters_CREATE_TOPIC:double,userOwnerCounters_CREATE_IMAGE:double,userOwnerCounters_CREATE_MOVIE:double,userOwnerCounters_CREATE_COMMENT:double,userOwnerCounters_CREATE_LIKE:double,userOwnerCounters_TEXT:double,userOwnerCounters_IMAGE:double,userOwnerCounters_VIDEO:double,ownerUserCounters_USER_FEED_REMOVE:double,ownerUserCounters_USER_PROFILE_VIEW:double,ownerUserCounters_VOTE_POLL:double,ownerUserCounters_USER_SEND_MESSAGE:double,ownerUserCounters_USER_DELETE_MESSAGE:double,ownerUserCounters_USER_INTERNAL_LIKE:double,ownerUserCounters_USER_INTERNAL_UNLIKE:double,ownerUserCounters_USER_STATUS_COMMENT_CREATE:double,ownerUserCounters_PHOTO_COMMENT_CREATE:double,ownerUserCounters_MOVIE_COMMENT_CREATE:double,ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,ownerUserCounters_COMMENT_INTERNAL_LIKE:double,ownerUserCounters_USER_FORUM_MESSAGE_CREATE:double,ownerUserCounters_PHOTO_MARK_CREATE:double,ownerUserCounters_PHOTO_VIEW:double,ownerUserCounters_PHOTO_PIN_BATCH_CREATE:double,ownerUserCounters_PHOTO_PIN_UPDATE:double,ownerUserCounters_USER_PRESENT_SEND:double,ownerUserCounters_UNKNOWN:double,ownerUserCounters_CREATE_TOPIC:double,ownerUserCounters_CREATE_IMAGE:double,ownerUserCounters_CREATE_MOVIE:double,ownerUserCounters_CREATE_COMMENT:double,ownerUserCounters_CREATE_LIKE:double,ownerUserCounters_TEXT:double,ownerUserCounters_IMAGE:double,ownerUserCounters_VIDEO:double,owner_gender_double_vecAssembler_0ad2c484cfbd:double,owner_status_double_vecAssembler_0ad2c484cfbd:double,owner_ID_country_double_vecAssembler_0ad2c484cfbd:double,owner_ID_Location_double_vecAssembler_0ad2c484cfbd:double,owner_is_active_double_vecAssembler_0ad2c484cfbd:double,owner_is_deleted_double_vecAssembler_0ad2c484cfbd:double,owner_is_abused_double_vecAssembler_0ad2c484cfbd:double,owner_is_activated_double_vecAssembler_0ad2c484cfbd:double,owner_change_datime_double_vecAssembler_0ad2c484cfbd:double,owner_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,owner_region_double_vecAssembler_0ad2c484cfbd:double,user_create_date_double_vecAssembler_0ad2c484cfbd:double,user_birth_date_double_vecAssembler_0ad2c484cfbd:double,user_gender_double_vecAssembler_0ad2c484cfbd:double,user_status_double_vecAssembler_0ad2c484cfbd:double,user_ID_country_double_vecAssembler_0ad2c484cfbd:double,user_ID_Location_double_vecAssembler_0ad2c484cfbd:double,user_is_active_double_vecAssembler_0ad2c484cfbd:double,user_is_deleted_double_vecAssembler_0ad2c484cfbd:double,user_is_abused_double_vecAssembler_0ad2c484cfbd:double,user_is_activated_double_vecAssembler_0ad2c484cfbd:double,user_change_datime_double_vecAssembler_0ad2c484cfbd:double,user_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,user_region_double_vecAssembler_0ad2c484cfbd:double,auditweights_ageMs:double,auditweights_closed:double,auditweights_ctr_gender:double,auditweights_ctr_high:double,auditweights_ctr_negative:double,auditweights_dailyRecency:double,auditweights_feedOwner_RECOMMENDED_GROUP:double,auditweights_feedStats:double,auditweights_friendCommentFeeds:double,auditweights_friendCommenters:double,auditweights_friendLikes:double,auditweights_friendLikes_actors:double,auditweights_hasDetectedText:double,auditweights_hasText:double,auditweights_isPymk:double,auditweights_isRandom:double,auditweights_likersFeedStats_hyper:double,auditweights_likersSvd_prelaunch_hyper:double,auditweights_matrix:double,auditweights_notOriginalPhoto:double,auditweights_numDislikes:double,auditweights_numLikes:double,auditweights_numShows:double,auditweights_onlineVideo:double,auditweights_partAge:double,auditweights_partCtr:double,auditweights_partSvd:double,auditweights_processedVideo:double,auditweights_relationMasks:double,auditweights_source_LIVE_TOP:double,auditweights_source_MOVIE_TOP:double,auditweights_svd_prelaunch:double,auditweights_svd_spark:double,auditweights_userAge:double,auditweights_userOwner_CREATE_COMMENT:double,auditweights_userOwner_CREATE_IMAGE:double,auditweights_userOwner_CREATE_LIKE:double,auditweights_userOwner_IMAGE:double,auditweights_userOwner_MOVIE_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_MARK_CREATE:double,auditweights_userOwner_PHOTO_VIEW:double,auditweights_userOwner_TEXT:double,auditweights_userOwner_UNKNOWN:double,auditweights_userOwner_USER_DELETE_MESSAGE:double,auditweights_userOwner_USER_FEED_REMOVE:double,auditweights_userOwner_USER_FORUM_MESSAGE_CREATE:double,auditweights_userOwner_USER_INTERNAL_LIKE:double,auditweights_userOwner_USER_INTERNAL_UNLIKE:double,auditweights_userOwner_USER_PRESENT_SEND:double,auditweights_userOwner_USER_PROFILE_VIEW:double,auditweights_userOwner_USER_SEND_MESSAGE:double,auditweights_userOwner_USER_STATUS_COMMENT_CREATE:double,auditweights_userOwner_VIDEO:double,auditweights_userOwner_VOTE_POLL:double,auditweights_x_ActorsRelations_double_vecAssembler_0ad2c484cfbd:double,auditweights_likersSvd_spark_hyper:double,auditweights_source_PROMO:double,timeDelta_double_vecAssembler_0ad2c484cfbd:double,createdHour_double_vecAssembler_0ad2c484cfbd:double,auditedHour_double_vecAssembler_0ad2c484cfbd:double,instanceId_objectType_num_double_vecAssembler_0ad2c484cfbd:double,audit_resourceType_num_double_vecAssembler_0ad2c484cfbd:double,metadata_ownerType_num_double_vecAssembler_0ad2c484cfbd:double,membership_status_num_double_vecAssembler_0ad2c484cfbd:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.07 23:20:33 ERROR Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider[0m
[0m2021.07.07 23:20:33 ERROR removing nulls from dataset or using handleInvalid = "keep" or "skip".[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)[0m
[0m2021.07.07 23:20:33 ERROR 	... 25 more[0m
[0m2021.07.07 23:20:33 ERROR 21/07/07 23:20:33 ERROR Executor: Exception in task 5.0 in stage 16.0 (TID 171)[0m
[0m2021.07.07 23:20:33 ERROR org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$4302/0x0000000841967840: (struct<audit_pos_double_vecAssembler_0ad2c484cfbd:double,metadata_numCompanions_double_vecAssembler_0ad2c484cfbd:double,metadata_numPhotos_double_vecAssembler_0ad2c484cfbd:double,metadata_numPolls_double_vecAssembler_0ad2c484cfbd:double,metadata_numSymbols_double_vecAssembler_0ad2c484cfbd:double,metadata_numTokens_double_vecAssembler_0ad2c484cfbd:double,metadata_numVideos_double_vecAssembler_0ad2c484cfbd:double,metadata_totalVideoLength_double_vecAssembler_0ad2c484cfbd:double,userOwnerCounters_USER_FEED_REMOVE:double,userOwnerCounters_USER_PROFILE_VIEW:double,userOwnerCounters_VOTE_POLL:double,userOwnerCounters_USER_SEND_MESSAGE:double,userOwnerCounters_USER_DELETE_MESSAGE:double,userOwnerCounters_USER_INTERNAL_LIKE:double,userOwnerCounters_USER_INTERNAL_UNLIKE:double,userOwnerCounters_USER_STATUS_COMMENT_CREATE:double,userOwnerCounters_PHOTO_COMMENT_CREATE:double,userOwnerCounters_MOVIE_COMMENT_CREATE:double,userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,userOwnerCounters_COMMENT_INTERNAL_LIKE:double,userOwnerCounters_USER_FORUM_MESSAGE_CREATE:double,userOwnerCounters_PHOTO_MARK_CREATE:double,userOwnerCounters_PHOTO_VIEW:double,userOwnerCounters_PHOTO_PIN_BATCH_CREATE:double,userOwnerCounters_PHOTO_PIN_UPDATE:double,userOwnerCounters_USER_PRESENT_SEND:double,userOwnerCounters_UNKNOWN:double,userOwnerCounters_CREATE_TOPIC:double,userOwnerCounters_CREATE_IMAGE:double,userOwnerCounters_CREATE_MOVIE:double,userOwnerCounters_CREATE_COMMENT:double,userOwnerCounters_CREATE_LIKE:double,userOwnerCounters_TEXT:double,userOwnerCounters_IMAGE:double,userOwnerCounters_VIDEO:double,ownerUserCounters_USER_FEED_REMOVE:double,ownerUserCounters_USER_PROFILE_VIEW:double,ownerUserCounters_VOTE_POLL:double,ownerUserCounters_USER_SEND_MESSAGE:double,ownerUserCounters_USER_DELETE_MESSAGE:double,ownerUserCounters_USER_INTERNAL_LIKE:double,ownerUserCounters_USER_INTERNAL_UNLIKE:double,ownerUserCounters_USER_STATUS_COMMENT_CREATE:double,ownerUserCounters_PHOTO_COMMENT_CREATE:double,ownerUserCounters_MOVIE_COMMENT_CREATE:double,ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,ownerUserCounters_COMMENT_INTERNAL_LIKE:double,ownerUserCounters_USER_FORUM_MESSAGE_CREATE:double,ownerUserCounters_PHOTO_MARK_CREATE:double,ownerUserCounters_PHOTO_VIEW:double,ownerUserCounters_PHOTO_PIN_BATCH_CREATE:double,ownerUserCounters_PHOTO_PIN_UPDATE:double,ownerUserCounters_USER_PRESENT_SEND:double,ownerUserCounters_UNKNOWN:double,ownerUserCounters_CREATE_TOPIC:double,ownerUserCounters_CREATE_IMAGE:double,ownerUserCounters_CREATE_MOVIE:double,ownerUserCounters_CREATE_COMMENT:double,ownerUserCounters_CREATE_LIKE:double,ownerUserCounters_TEXT:double,ownerUserCounters_IMAGE:double,ownerUserCounters_VIDEO:double,owner_gender_double_vecAssembler_0ad2c484cfbd:double,owner_status_double_vecAssembler_0ad2c484cfbd:double,owner_ID_country_double_vecAssembler_0ad2c484cfbd:double,owner_ID_Location_double_vecAssembler_0ad2c484cfbd:double,owner_is_active_double_vecAssembler_0ad2c484cfbd:double,owner_is_deleted_double_vecAssembler_0ad2c484cfbd:double,owner_is_abused_double_vecAssembler_0ad2c484cfbd:double,owner_is_activated_double_vecAssembler_0ad2c484cfbd:double,owner_change_datime_double_vecAssembler_0ad2c484cfbd:double,owner_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,owner_region_double_vecAssembler_0ad2c484cfbd:double,user_create_date_double_vecAssembler_0ad2c484cfbd:double,user_birth_date_double_vecAssembler_0ad2c484cfbd:double,user_gender_double_vecAssembler_0ad2c484cfbd:double,user_status_double_vecAssembler_0ad2c484cfbd:double,user_ID_country_double_vecAssembler_0ad2c484cfbd:double,user_ID_Location_double_vecAssembler_0ad2c484cfbd:double,user_is_active_double_vecAssembler_0ad2c484cfbd:double,user_is_deleted_double_vecAssembler_0ad2c484cfbd:double,user_is_abused_double_vecAssembler_0ad2c484cfbd:double,user_is_activated_double_vecAssembler_0ad2c484cfbd:double,user_change_datime_double_vecAssembler_0ad2c484cfbd:double,user_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,user_region_double_vecAssembler_0ad2c484cfbd:double,auditweights_ageMs:double,auditweights_closed:double,auditweights_ctr_gender:double,auditweights_ctr_high:double,auditweights_ctr_negative:double,auditweights_dailyRecency:double,auditweights_feedOwner_RECOMMENDED_GROUP:double,auditweights_feedStats:double,auditweights_friendCommentFeeds:double,auditweights_friendCommenters:double,auditweights_friendLikes:double,auditweights_friendLikes_actors:double,auditweights_hasDetectedText:double,auditweights_hasText:double,auditweights_isPymk:double,auditweights_isRandom:double,auditweights_likersFeedStats_hyper:double,auditweights_likersSvd_prelaunch_hyper:double,auditweights_matrix:double,auditweights_notOriginalPhoto:double,auditweights_numDislikes:double,auditweights_numLikes:double,auditweights_numShows:double,auditweights_onlineVideo:double,auditweights_partAge:double,auditweights_partCtr:double,auditweights_partSvd:double,auditweights_processedVideo:double,auditweights_relationMasks:double,auditweights_source_LIVE_TOP:double,auditweights_source_MOVIE_TOP:double,auditweights_svd_prelaunch:double,auditweights_svd_spark:double,auditweights_userAge:double,auditweights_userOwner_CREATE_COMMENT:double,auditweights_userOwner_CREATE_IMAGE:double,auditweights_userOwner_CREATE_LIKE:double,auditweights_userOwner_IMAGE:double,auditweights_userOwner_MOVIE_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_MARK_CREATE:double,auditweights_userOwner_PHOTO_VIEW:double,auditweights_userOwner_TEXT:double,auditweights_userOwner_UNKNOWN:double,auditweights_userOwner_USER_DELETE_MESSAGE:double,auditweights_userOwner_USER_FEED_REMOVE:double,auditweights_userOwner_USER_FORUM_MESSAGE_CREATE:double,auditweights_userOwner_USER_INTERNAL_LIKE:double,auditweights_userOwner_USER_INTERNAL_UNLIKE:double,auditweights_userOwner_USER_PRESENT_SEND:double,auditweights_userOwner_USER_PROFILE_VIEW:double,auditweights_userOwner_USER_SEND_MESSAGE:double,auditweights_userOwner_USER_STATUS_COMMENT_CREATE:double,auditweights_userOwner_VIDEO:double,auditweights_userOwner_VOTE_POLL:double,auditweights_x_ActorsRelations_double_vecAssembler_0ad2c484cfbd:double,auditweights_likersSvd_spark_hyper:double,auditweights_source_PROMO:double,timeDelta_double_vecAssembler_0ad2c484cfbd:double,createdHour_double_vecAssembler_0ad2c484cfbd:double,auditedHour_double_vecAssembler_0ad2c484cfbd:double,instanceId_objectType_num_double_vecAssembler_0ad2c484cfbd:double,audit_resourceType_num_double_vecAssembler_0ad2c484cfbd:double,metadata_ownerType_num_double_vecAssembler_0ad2c484cfbd:double,membership_status_num_double_vecAssembler_0ad2c484cfbd:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.07 23:20:33 ERROR Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider[0m
[0m2021.07.07 23:20:33 ERROR removing nulls from dataset or using handleInvalid = "keep" or "skip".[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)[0m
[0m2021.07.07 23:20:33 ERROR 	... 25 more[0m
[0m2021.07.07 23:20:33 ERROR 21/07/07 23:20:33 ERROR Executor: Exception in task 0.0 in stage 16.0 (TID 166)[0m
[0m2021.07.07 23:20:33 ERROR org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$4302/0x0000000841967840: (struct<audit_pos_double_vecAssembler_0ad2c484cfbd:double,metadata_numCompanions_double_vecAssembler_0ad2c484cfbd:double,metadata_numPhotos_double_vecAssembler_0ad2c484cfbd:double,metadata_numPolls_double_vecAssembler_0ad2c484cfbd:double,metadata_numSymbols_double_vecAssembler_0ad2c484cfbd:double,metadata_numTokens_double_vecAssembler_0ad2c484cfbd:double,metadata_numVideos_double_vecAssembler_0ad2c484cfbd:double,metadata_totalVideoLength_double_vecAssembler_0ad2c484cfbd:double,userOwnerCounters_USER_FEED_REMOVE:double,userOwnerCounters_USER_PROFILE_VIEW:double,userOwnerCounters_VOTE_POLL:double,userOwnerCounters_USER_SEND_MESSAGE:double,userOwnerCounters_USER_DELETE_MESSAGE:double,userOwnerCounters_USER_INTERNAL_LIKE:double,userOwnerCounters_USER_INTERNAL_UNLIKE:double,userOwnerCounters_USER_STATUS_COMMENT_CREATE:double,userOwnerCounters_PHOTO_COMMENT_CREATE:double,userOwnerCounters_MOVIE_COMMENT_CREATE:double,userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,userOwnerCounters_COMMENT_INTERNAL_LIKE:double,userOwnerCounters_USER_FORUM_MESSAGE_CREATE:double,userOwnerCounters_PHOTO_MARK_CREATE:double,userOwnerCounters_PHOTO_VIEW:double,userOwnerCounters_PHOTO_PIN_BATCH_CREATE:double,userOwnerCounters_PHOTO_PIN_UPDATE:double,userOwnerCounters_USER_PRESENT_SEND:double,userOwnerCounters_UNKNOWN:double,userOwnerCounters_CREATE_TOPIC:double,userOwnerCounters_CREATE_IMAGE:double,userOwnerCounters_CREATE_MOVIE:double,userOwnerCounters_CREATE_COMMENT:double,userOwnerCounters_CREATE_LIKE:double,userOwnerCounters_TEXT:double,userOwnerCounters_IMAGE:double,userOwnerCounters_VIDEO:double,ownerUserCounters_USER_FEED_REMOVE:double,ownerUserCounters_USER_PROFILE_VIEW:double,ownerUserCounters_VOTE_POLL:double,ownerUserCounters_USER_SEND_MESSAGE:double,ownerUserCounters_USER_DELETE_MESSAGE:double,ownerUserCounters_USER_INTERNAL_LIKE:double,ownerUserCounters_USER_INTERNAL_UNLIKE:double,ownerUserCounters_USER_STATUS_COMMENT_CREATE:double,ownerUserCounters_PHOTO_COMMENT_CREATE:double,ownerUserCounters_MOVIE_COMMENT_CREATE:double,ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,ownerUserCounters_COMMENT_INTERNAL_LIKE:double,ownerUserCounters_USER_FORUM_MESSAGE_CREATE:double,ownerUserCounters_PHOTO_MARK_CREATE:double,ownerUserCounters_PHOTO_VIEW:double,ownerUserCounters_PHOTO_PIN_BATCH_CREATE:double,ownerUserCounters_PHOTO_PIN_UPDATE:double,ownerUserCounters_USER_PRESENT_SEND:double,ownerUserCounters_UNKNOWN:double,ownerUserCounters_CREATE_TOPIC:double,ownerUserCounters_CREATE_IMAGE:double,ownerUserCounters_CREATE_MOVIE:double,ownerUserCounters_CREATE_COMMENT:double,ownerUserCounters_CREATE_LIKE:double,ownerUserCounters_TEXT:double,ownerUserCounters_IMAGE:double,ownerUserCounters_VIDEO:double,owner_gender_double_vecAssembler_0ad2c484cfbd:double,owner_status_double_vecAssembler_0ad2c484cfbd:double,owner_ID_country_double_vecAssembler_0ad2c484cfbd:double,owner_ID_Location_double_vecAssembler_0ad2c484cfbd:double,owner_is_active_double_vecAssembler_0ad2c484cfbd:double,owner_is_deleted_double_vecAssembler_0ad2c484cfbd:double,owner_is_abused_double_vecAssembler_0ad2c484cfbd:double,owner_is_activated_double_vecAssembler_0ad2c484cfbd:double,owner_change_datime_double_vecAssembler_0ad2c484cfbd:double,owner_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,owner_region_double_vecAssembler_0ad2c484cfbd:double,user_create_date_double_vecAssembler_0ad2c484cfbd:double,user_birth_date_double_vecAssembler_0ad2c484cfbd:double,user_gender_double_vecAssembler_0ad2c484cfbd:double,user_status_double_vecAssembler_0ad2c484cfbd:double,user_ID_country_double_vecAssembler_0ad2c484cfbd:double,user_ID_Location_double_vecAssembler_0ad2c484cfbd:double,user_is_active_double_vecAssembler_0ad2c484cfbd:double,user_is_deleted_double_vecAssembler_0ad2c484cfbd:double,user_is_abused_double_vecAssembler_0ad2c484cfbd:double,user_is_activated_double_vecAssembler_0ad2c484cfbd:double,user_change_datime_double_vecAssembler_0ad2c484cfbd:double,user_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,user_region_double_vecAssembler_0ad2c484cfbd:double,auditweights_ageMs:double,auditweights_closed:double,auditweights_ctr_gender:double,auditweights_ctr_high:double,auditweights_ctr_negative:double,auditweights_dailyRecency:double,auditweights_feedOwner_RECOMMENDED_GROUP:double,auditweights_feedStats:double,auditweights_friendCommentFeeds:double,auditweights_friendCommenters:double,auditweights_friendLikes:double,auditweights_friendLikes_actors:double,auditweights_hasDetectedText:double,auditweights_hasText:double,auditweights_isPymk:double,auditweights_isRandom:double,auditweights_likersFeedStats_hyper:double,auditweights_likersSvd_prelaunch_hyper:double,auditweights_matrix:double,auditweights_notOriginalPhoto:double,auditweights_numDislikes:double,auditweights_numLikes:double,auditweights_numShows:double,auditweights_onlineVideo:double,auditweights_partAge:double,auditweights_partCtr:double,auditweights_partSvd:double,auditweights_processedVideo:double,auditweights_relationMasks:double,auditweights_source_LIVE_TOP:double,auditweights_source_MOVIE_TOP:double,auditweights_svd_prelaunch:double,auditweights_svd_spark:double,auditweights_userAge:double,auditweights_userOwner_CREATE_COMMENT:double,auditweights_userOwner_CREATE_IMAGE:double,auditweights_userOwner_CREATE_LIKE:double,auditweights_userOwner_IMAGE:double,auditweights_userOwner_MOVIE_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_MARK_CREATE:double,auditweights_userOwner_PHOTO_VIEW:double,auditweights_userOwner_TEXT:double,auditweights_userOwner_UNKNOWN:double,auditweights_userOwner_USER_DELETE_MESSAGE:double,auditweights_userOwner_USER_FEED_REMOVE:double,auditweights_userOwner_USER_FORUM_MESSAGE_CREATE:double,auditweights_userOwner_USER_INTERNAL_LIKE:double,auditweights_userOwner_USER_INTERNAL_UNLIKE:double,auditweights_userOwner_USER_PRESENT_SEND:double,auditweights_userOwner_USER_PROFILE_VIEW:double,auditweights_userOwner_USER_SEND_MESSAGE:double,auditweights_userOwner_USER_STATUS_COMMENT_CREATE:double,auditweights_userOwner_VIDEO:double,auditweights_userOwner_VOTE_POLL:double,auditweights_x_ActorsRelations_double_vecAssembler_0ad2c484cfbd:double,auditweights_likersSvd_spark_hyper:double,auditweights_source_PROMO:double,timeDelta_double_vecAssembler_0ad2c484cfbd:double,createdHour_double_vecAssembler_0ad2c484cfbd:double,auditedHour_double_vecAssembler_0ad2c484cfbd:double,instanceId_objectType_num_double_vecAssembler_0ad2c484cfbd:double,audit_resourceType_num_double_vecAssembler_0ad2c484cfbd:double,metadata_ownerType_num_double_vecAssembler_0ad2c484cfbd:double,membership_status_num_double_vecAssembler_0ad2c484cfbd:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.07 23:20:33 ERROR Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider[0m
[0m2021.07.07 23:20:33 ERROR removing nulls from dataset or using handleInvalid = "keep" or "skip".[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)[0m
[0m2021.07.07 23:20:33 ERROR 	... 25 more[0m
[0m2021.07.07 23:20:33 ERROR 21/07/07 23:20:33 ERROR Executor: Exception in task 3.0 in stage 16.0 (TID 169)[0m
[0m2021.07.07 23:20:33 ERROR org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$4302/0x0000000841967840: (struct<audit_pos_double_vecAssembler_0ad2c484cfbd:double,metadata_numCompanions_double_vecAssembler_0ad2c484cfbd:double,metadata_numPhotos_double_vecAssembler_0ad2c484cfbd:double,metadata_numPolls_double_vecAssembler_0ad2c484cfbd:double,metadata_numSymbols_double_vecAssembler_0ad2c484cfbd:double,metadata_numTokens_double_vecAssembler_0ad2c484cfbd:double,metadata_numVideos_double_vecAssembler_0ad2c484cfbd:double,metadata_totalVideoLength_double_vecAssembler_0ad2c484cfbd:double,userOwnerCounters_USER_FEED_REMOVE:double,userOwnerCounters_USER_PROFILE_VIEW:double,userOwnerCounters_VOTE_POLL:double,userOwnerCounters_USER_SEND_MESSAGE:double,userOwnerCounters_USER_DELETE_MESSAGE:double,userOwnerCounters_USER_INTERNAL_LIKE:double,userOwnerCounters_USER_INTERNAL_UNLIKE:double,userOwnerCounters_USER_STATUS_COMMENT_CREATE:double,userOwnerCounters_PHOTO_COMMENT_CREATE:double,userOwnerCounters_MOVIE_COMMENT_CREATE:double,userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,userOwnerCounters_COMMENT_INTERNAL_LIKE:double,userOwnerCounters_USER_FORUM_MESSAGE_CREATE:double,userOwnerCounters_PHOTO_MARK_CREATE:double,userOwnerCounters_PHOTO_VIEW:double,userOwnerCounters_PHOTO_PIN_BATCH_CREATE:double,userOwnerCounters_PHOTO_PIN_UPDATE:double,userOwnerCounters_USER_PRESENT_SEND:double,userOwnerCounters_UNKNOWN:double,userOwnerCounters_CREATE_TOPIC:double,userOwnerCounters_CREATE_IMAGE:double,userOwnerCounters_CREATE_MOVIE:double,userOwnerCounters_CREATE_COMMENT:double,userOwnerCounters_CREATE_LIKE:double,userOwnerCounters_TEXT:double,userOwnerCounters_IMAGE:double,userOwnerCounters_VIDEO:double,ownerUserCounters_USER_FEED_REMOVE:double,ownerUserCounters_USER_PROFILE_VIEW:double,ownerUserCounters_VOTE_POLL:double,ownerUserCounters_USER_SEND_MESSAGE:double,ownerUserCounters_USER_DELETE_MESSAGE:double,ownerUserCounters_USER_INTERNAL_LIKE:double,ownerUserCounters_USER_INTERNAL_UNLIKE:double,ownerUserCounters_USER_STATUS_COMMENT_CREATE:double,ownerUserCounters_PHOTO_COMMENT_CREATE:double,ownerUserCounters_MOVIE_COMMENT_CREATE:double,ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,ownerUserCounters_COMMENT_INTERNAL_LIKE:double,ownerUserCounters_USER_FORUM_MESSAGE_CREATE:double,ownerUserCounters_PHOTO_MARK_CREATE:double,ownerUserCounters_PHOTO_VIEW:double,ownerUserCounters_PHOTO_PIN_BATCH_CREATE:double,ownerUserCounters_PHOTO_PIN_UPDATE:double,ownerUserCounters_USER_PRESENT_SEND:double,ownerUserCounters_UNKNOWN:double,ownerUserCounters_CREATE_TOPIC:double,ownerUserCounters_CREATE_IMAGE:double,ownerUserCounters_CREATE_MOVIE:double,ownerUserCounters_CREATE_COMMENT:double,ownerUserCounters_CREATE_LIKE:double,ownerUserCounters_TEXT:double,ownerUserCounters_IMAGE:double,ownerUserCounters_VIDEO:double,owner_gender_double_vecAssembler_0ad2c484cfbd:double,owner_status_double_vecAssembler_0ad2c484cfbd:double,owner_ID_country_double_vecAssembler_0ad2c484cfbd:double,owner_ID_Location_double_vecAssembler_0ad2c484cfbd:double,owner_is_active_double_vecAssembler_0ad2c484cfbd:double,owner_is_deleted_double_vecAssembler_0ad2c484cfbd:double,owner_is_abused_double_vecAssembler_0ad2c484cfbd:double,owner_is_activated_double_vecAssembler_0ad2c484cfbd:double,owner_change_datime_double_vecAssembler_0ad2c484cfbd:double,owner_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,owner_region_double_vecAssembler_0ad2c484cfbd:double,user_create_date_double_vecAssembler_0ad2c484cfbd:double,user_birth_date_double_vecAssembler_0ad2c484cfbd:double,user_gender_double_vecAssembler_0ad2c484cfbd:double,user_status_double_vecAssembler_0ad2c484cfbd:double,user_ID_country_double_vecAssembler_0ad2c484cfbd:double,user_ID_Location_double_vecAssembler_0ad2c484cfbd:double,user_is_active_double_vecAssembler_0ad2c484cfbd:double,user_is_deleted_double_vecAssembler_0ad2c484cfbd:double,user_is_abused_double_vecAssembler_0ad2c484cfbd:double,user_is_activated_double_vecAssembler_0ad2c484cfbd:double,user_change_datime_double_vecAssembler_0ad2c484cfbd:double,user_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,user_region_double_vecAssembler_0ad2c484cfbd:double,auditweights_ageMs:double,auditweights_closed:double,auditweights_ctr_gender:double,auditweights_ctr_high:double,auditweights_ctr_negative:double,auditweights_dailyRecency:double,auditweights_feedOwner_RECOMMENDED_GROUP:double,auditweights_feedStats:double,auditweights_friendCommentFeeds:double,auditweights_friendCommenters:double,auditweights_friendLikes:double,auditweights_friendLikes_actors:double,auditweights_hasDetectedText:double,auditweights_hasText:double,auditweights_isPymk:double,auditweights_isRandom:double,auditweights_likersFeedStats_hyper:double,auditweights_likersSvd_prelaunch_hyper:double,auditweights_matrix:double,auditweights_notOriginalPhoto:double,auditweights_numDislikes:double,auditweights_numLikes:double,auditweights_numShows:double,auditweights_onlineVideo:double,auditweights_partAge:double,auditweights_partCtr:double,auditweights_partSvd:double,auditweights_processedVideo:double,auditweights_relationMasks:double,auditweights_source_LIVE_TOP:double,auditweights_source_MOVIE_TOP:double,auditweights_svd_prelaunch:double,auditweights_svd_spark:double,auditweights_userAge:double,auditweights_userOwner_CREATE_COMMENT:double,auditweights_userOwner_CREATE_IMAGE:double,auditweights_userOwner_CREATE_LIKE:double,auditweights_userOwner_IMAGE:double,auditweights_userOwner_MOVIE_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_MARK_CREATE:double,auditweights_userOwner_PHOTO_VIEW:double,auditweights_userOwner_TEXT:double,auditweights_userOwner_UNKNOWN:double,auditweights_userOwner_USER_DELETE_MESSAGE:double,auditweights_userOwner_USER_FEED_REMOVE:double,auditweights_userOwner_USER_FORUM_MESSAGE_CREATE:double,auditweights_userOwner_USER_INTERNAL_LIKE:double,auditweights_userOwner_USER_INTERNAL_UNLIKE:double,auditweights_userOwner_USER_PRESENT_SEND:double,auditweights_userOwner_USER_PROFILE_VIEW:double,auditweights_userOwner_USER_SEND_MESSAGE:double,auditweights_userOwner_USER_STATUS_COMMENT_CREATE:double,auditweights_userOwner_VIDEO:double,auditweights_userOwner_VOTE_POLL:double,auditweights_x_ActorsRelations_double_vecAssembler_0ad2c484cfbd:double,auditweights_likersSvd_spark_hyper:double,auditweights_source_PROMO:double,timeDelta_double_vecAssembler_0ad2c484cfbd:double,createdHour_double_vecAssembler_0ad2c484cfbd:double,auditedHour_double_vecAssembler_0ad2c484cfbd:double,instanceId_objectType_num_double_vecAssembler_0ad2c484cfbd:double,audit_resourceType_num_double_vecAssembler_0ad2c484cfbd:double,metadata_ownerType_num_double_vecAssembler_0ad2c484cfbd:double,membership_status_num_double_vecAssembler_0ad2c484cfbd:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.07 23:20:33 ERROR Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider[0m
[0m2021.07.07 23:20:33 ERROR removing nulls from dataset or using handleInvalid = "keep" or "skip".[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)[0m
[0m2021.07.07 23:20:33 ERROR 	... 25 more[0m
[0m2021.07.07 23:20:33 ERROR 21/07/07 23:20:33 ERROR Executor: Exception in task 4.0 in stage 16.0 (TID 170)[0m
[0m2021.07.07 23:20:33 ERROR org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$4302/0x0000000841967840: (struct<audit_pos_double_vecAssembler_0ad2c484cfbd:double,metadata_numCompanions_double_vecAssembler_0ad2c484cfbd:double,metadata_numPhotos_double_vecAssembler_0ad2c484cfbd:double,metadata_numPolls_double_vecAssembler_0ad2c484cfbd:double,metadata_numSymbols_double_vecAssembler_0ad2c484cfbd:double,metadata_numTokens_double_vecAssembler_0ad2c484cfbd:double,metadata_numVideos_double_vecAssembler_0ad2c484cfbd:double,metadata_totalVideoLength_double_vecAssembler_0ad2c484cfbd:double,userOwnerCounters_USER_FEED_REMOVE:double,userOwnerCounters_USER_PROFILE_VIEW:double,userOwnerCounters_VOTE_POLL:double,userOwnerCounters_USER_SEND_MESSAGE:double,userOwnerCounters_USER_DELETE_MESSAGE:double,userOwnerCounters_USER_INTERNAL_LIKE:double,userOwnerCounters_USER_INTERNAL_UNLIKE:double,userOwnerCounters_USER_STATUS_COMMENT_CREATE:double,userOwnerCounters_PHOTO_COMMENT_CREATE:double,userOwnerCounters_MOVIE_COMMENT_CREATE:double,userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,userOwnerCounters_COMMENT_INTERNAL_LIKE:double,userOwnerCounters_USER_FORUM_MESSAGE_CREATE:double,userOwnerCounters_PHOTO_MARK_CREATE:double,userOwnerCounters_PHOTO_VIEW:double,userOwnerCounters_PHOTO_PIN_BATCH_CREATE:double,userOwnerCounters_PHOTO_PIN_UPDATE:double,userOwnerCounters_USER_PRESENT_SEND:double,userOwnerCounters_UNKNOWN:double,userOwnerCounters_CREATE_TOPIC:double,userOwnerCounters_CREATE_IMAGE:double,userOwnerCounters_CREATE_MOVIE:double,userOwnerCounters_CREATE_COMMENT:double,userOwnerCounters_CREATE_LIKE:double,userOwnerCounters_TEXT:double,userOwnerCounters_IMAGE:double,userOwnerCounters_VIDEO:double,ownerUserCounters_USER_FEED_REMOVE:double,ownerUserCounters_USER_PROFILE_VIEW:double,ownerUserCounters_VOTE_POLL:double,ownerUserCounters_USER_SEND_MESSAGE:double,ownerUserCounters_USER_DELETE_MESSAGE:double,ownerUserCounters_USER_INTERNAL_LIKE:double,ownerUserCounters_USER_INTERNAL_UNLIKE:double,ownerUserCounters_USER_STATUS_COMMENT_CREATE:double,ownerUserCounters_PHOTO_COMMENT_CREATE:double,ownerUserCounters_MOVIE_COMMENT_CREATE:double,ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,ownerUserCounters_COMMENT_INTERNAL_LIKE:double,ownerUserCounters_USER_FORUM_MESSAGE_CREATE:double,ownerUserCounters_PHOTO_MARK_CREATE:double,ownerUserCounters_PHOTO_VIEW:double,ownerUserCounters_PHOTO_PIN_BATCH_CREATE:double,ownerUserCounters_PHOTO_PIN_UPDATE:double,ownerUserCounters_USER_PRESENT_SEND:double,ownerUserCounters_UNKNOWN:double,ownerUserCounters_CREATE_TOPIC:double,ownerUserCounters_CREATE_IMAGE:double,ownerUserCounters_CREATE_MOVIE:double,ownerUserCounters_CREATE_COMMENT:double,ownerUserCounters_CREATE_LIKE:double,ownerUserCounters_TEXT:double,ownerUserCounters_IMAGE:double,ownerUserCounters_VIDEO:double,owner_gender_double_vecAssembler_0ad2c484cfbd:double,owner_status_double_vecAssembler_0ad2c484cfbd:double,owner_ID_country_double_vecAssembler_0ad2c484cfbd:double,owner_ID_Location_double_vecAssembler_0ad2c484cfbd:double,owner_is_active_double_vecAssembler_0ad2c484cfbd:double,owner_is_deleted_double_vecAssembler_0ad2c484cfbd:double,owner_is_abused_double_vecAssembler_0ad2c484cfbd:double,owner_is_activated_double_vecAssembler_0ad2c484cfbd:double,owner_change_datime_double_vecAssembler_0ad2c484cfbd:double,owner_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,owner_region_double_vecAssembler_0ad2c484cfbd:double,user_create_date_double_vecAssembler_0ad2c484cfbd:double,user_birth_date_double_vecAssembler_0ad2c484cfbd:double,user_gender_double_vecAssembler_0ad2c484cfbd:double,user_status_double_vecAssembler_0ad2c484cfbd:double,user_ID_country_double_vecAssembler_0ad2c484cfbd:double,user_ID_Location_double_vecAssembler_0ad2c484cfbd:double,user_is_active_double_vecAssembler_0ad2c484cfbd:double,user_is_deleted_double_vecAssembler_0ad2c484cfbd:double,user_is_abused_double_vecAssembler_0ad2c484cfbd:double,user_is_activated_double_vecAssembler_0ad2c484cfbd:double,user_change_datime_double_vecAssembler_0ad2c484cfbd:double,user_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,user_region_double_vecAssembler_0ad2c484cfbd:double,auditweights_ageMs:double,auditweights_closed:double,auditweights_ctr_gender:double,auditweights_ctr_high:double,auditweights_ctr_negative:double,auditweights_dailyRecency:double,auditweights_feedOwner_RECOMMENDED_GROUP:double,auditweights_feedStats:double,auditweights_friendCommentFeeds:double,auditweights_friendCommenters:double,auditweights_friendLikes:double,auditweights_friendLikes_actors:double,auditweights_hasDetectedText:double,auditweights_hasText:double,auditweights_isPymk:double,auditweights_isRandom:double,auditweights_likersFeedStats_hyper:double,auditweights_likersSvd_prelaunch_hyper:double,auditweights_matrix:double,auditweights_notOriginalPhoto:double,auditweights_numDislikes:double,auditweights_numLikes:double,auditweights_numShows:double,auditweights_onlineVideo:double,auditweights_partAge:double,auditweights_partCtr:double,auditweights_partSvd:double,auditweights_processedVideo:double,auditweights_relationMasks:double,auditweights_source_LIVE_TOP:double,auditweights_source_MOVIE_TOP:double,auditweights_svd_prelaunch:double,auditweights_svd_spark:double,auditweights_userAge:double,auditweights_userOwner_CREATE_COMMENT:double,auditweights_userOwner_CREATE_IMAGE:double,auditweights_userOwner_CREATE_LIKE:double,auditweights_userOwner_IMAGE:double,auditweights_userOwner_MOVIE_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_MARK_CREATE:double,auditweights_userOwner_PHOTO_VIEW:double,auditweights_userOwner_TEXT:double,auditweights_userOwner_UNKNOWN:double,auditweights_userOwner_USER_DELETE_MESSAGE:double,auditweights_userOwner_USER_FEED_REMOVE:double,auditweights_userOwner_USER_FORUM_MESSAGE_CREATE:double,auditweights_userOwner_USER_INTERNAL_LIKE:double,auditweights_userOwner_USER_INTERNAL_UNLIKE:double,auditweights_userOwner_USER_PRESENT_SEND:double,auditweights_userOwner_USER_PROFILE_VIEW:double,auditweights_userOwner_USER_SEND_MESSAGE:double,auditweights_userOwner_USER_STATUS_COMMENT_CREATE:double,auditweights_userOwner_VIDEO:double,auditweights_userOwner_VOTE_POLL:double,auditweights_x_ActorsRelations_double_vecAssembler_0ad2c484cfbd:double,auditweights_likersSvd_spark_hyper:double,auditweights_source_PROMO:double,timeDelta_double_vecAssembler_0ad2c484cfbd:double,createdHour_double_vecAssembler_0ad2c484cfbd:double,auditedHour_double_vecAssembler_0ad2c484cfbd:double,instanceId_objectType_num_double_vecAssembler_0ad2c484cfbd:double,audit_resourceType_num_double_vecAssembler_0ad2c484cfbd:double,metadata_ownerType_num_double_vecAssembler_0ad2c484cfbd:double,membership_status_num_double_vecAssembler_0ad2c484cfbd:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.07 23:20:33 ERROR Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider[0m
[0m2021.07.07 23:20:33 ERROR removing nulls from dataset or using handleInvalid = "keep" or "skip".[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)[0m
[0m2021.07.07 23:20:33 ERROR 	... 25 more[0m
[0m2021.07.07 23:20:33 ERROR 21/07/07 23:20:33 ERROR Executor: Exception in task 2.0 in stage 16.0 (TID 168)[0m
[0m2021.07.07 23:20:33 ERROR org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$4302/0x0000000841967840: (struct<audit_pos_double_vecAssembler_0ad2c484cfbd:double,metadata_numCompanions_double_vecAssembler_0ad2c484cfbd:double,metadata_numPhotos_double_vecAssembler_0ad2c484cfbd:double,metadata_numPolls_double_vecAssembler_0ad2c484cfbd:double,metadata_numSymbols_double_vecAssembler_0ad2c484cfbd:double,metadata_numTokens_double_vecAssembler_0ad2c484cfbd:double,metadata_numVideos_double_vecAssembler_0ad2c484cfbd:double,metadata_totalVideoLength_double_vecAssembler_0ad2c484cfbd:double,userOwnerCounters_USER_FEED_REMOVE:double,userOwnerCounters_USER_PROFILE_VIEW:double,userOwnerCounters_VOTE_POLL:double,userOwnerCounters_USER_SEND_MESSAGE:double,userOwnerCounters_USER_DELETE_MESSAGE:double,userOwnerCounters_USER_INTERNAL_LIKE:double,userOwnerCounters_USER_INTERNAL_UNLIKE:double,userOwnerCounters_USER_STATUS_COMMENT_CREATE:double,userOwnerCounters_PHOTO_COMMENT_CREATE:double,userOwnerCounters_MOVIE_COMMENT_CREATE:double,userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,userOwnerCounters_COMMENT_INTERNAL_LIKE:double,userOwnerCounters_USER_FORUM_MESSAGE_CREATE:double,userOwnerCounters_PHOTO_MARK_CREATE:double,userOwnerCounters_PHOTO_VIEW:double,userOwnerCounters_PHOTO_PIN_BATCH_CREATE:double,userOwnerCounters_PHOTO_PIN_UPDATE:double,userOwnerCounters_USER_PRESENT_SEND:double,userOwnerCounters_UNKNOWN:double,userOwnerCounters_CREATE_TOPIC:double,userOwnerCounters_CREATE_IMAGE:double,userOwnerCounters_CREATE_MOVIE:double,userOwnerCounters_CREATE_COMMENT:double,userOwnerCounters_CREATE_LIKE:double,userOwnerCounters_TEXT:double,userOwnerCounters_IMAGE:double,userOwnerCounters_VIDEO:double,ownerUserCounters_USER_FEED_REMOVE:double,ownerUserCounters_USER_PROFILE_VIEW:double,ownerUserCounters_VOTE_POLL:double,ownerUserCounters_USER_SEND_MESSAGE:double,ownerUserCounters_USER_DELETE_MESSAGE:double,ownerUserCounters_USER_INTERNAL_LIKE:double,ownerUserCounters_USER_INTERNAL_UNLIKE:double,ownerUserCounters_USER_STATUS_COMMENT_CREATE:double,ownerUserCounters_PHOTO_COMMENT_CREATE:double,ownerUserCounters_MOVIE_COMMENT_CREATE:double,ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,ownerUserCounters_COMMENT_INTERNAL_LIKE:double,ownerUserCounters_USER_FORUM_MESSAGE_CREATE:double,ownerUserCounters_PHOTO_MARK_CREATE:double,ownerUserCounters_PHOTO_VIEW:double,ownerUserCounters_PHOTO_PIN_BATCH_CREATE:double,ownerUserCounters_PHOTO_PIN_UPDATE:double,ownerUserCounters_USER_PRESENT_SEND:double,ownerUserCounters_UNKNOWN:double,ownerUserCounters_CREATE_TOPIC:double,ownerUserCounters_CREATE_IMAGE:double,ownerUserCounters_CREATE_MOVIE:double,ownerUserCounters_CREATE_COMMENT:double,ownerUserCounters_CREATE_LIKE:double,ownerUserCounters_TEXT:double,ownerUserCounters_IMAGE:double,ownerUserCounters_VIDEO:double,owner_gender_double_vecAssembler_0ad2c484cfbd:double,owner_status_double_vecAssembler_0ad2c484cfbd:double,owner_ID_country_double_vecAssembler_0ad2c484cfbd:double,owner_ID_Location_double_vecAssembler_0ad2c484cfbd:double,owner_is_active_double_vecAssembler_0ad2c484cfbd:double,owner_is_deleted_double_vecAssembler_0ad2c484cfbd:double,owner_is_abused_double_vecAssembler_0ad2c484cfbd:double,owner_is_activated_double_vecAssembler_0ad2c484cfbd:double,owner_change_datime_double_vecAssembler_0ad2c484cfbd:double,owner_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,owner_region_double_vecAssembler_0ad2c484cfbd:double,user_create_date_double_vecAssembler_0ad2c484cfbd:double,user_birth_date_double_vecAssembler_0ad2c484cfbd:double,user_gender_double_vecAssembler_0ad2c484cfbd:double,user_status_double_vecAssembler_0ad2c484cfbd:double,user_ID_country_double_vecAssembler_0ad2c484cfbd:double,user_ID_Location_double_vecAssembler_0ad2c484cfbd:double,user_is_active_double_vecAssembler_0ad2c484cfbd:double,user_is_deleted_double_vecAssembler_0ad2c484cfbd:double,user_is_abused_double_vecAssembler_0ad2c484cfbd:double,user_is_activated_double_vecAssembler_0ad2c484cfbd:double,user_change_datime_double_vecAssembler_0ad2c484cfbd:double,user_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,user_region_double_vecAssembler_0ad2c484cfbd:double,auditweights_ageMs:double,auditweights_closed:double,auditweights_ctr_gender:double,auditweights_ctr_high:double,auditweights_ctr_negative:double,auditweights_dailyRecency:double,auditweights_feedOwner_RECOMMENDED_GROUP:double,auditweights_feedStats:double,auditweights_friendCommentFeeds:double,auditweights_friendCommenters:double,auditweights_friendLikes:double,auditweights_friendLikes_actors:double,auditweights_hasDetectedText:double,auditweights_hasText:double,auditweights_isPymk:double,auditweights_isRandom:double,auditweights_likersFeedStats_hyper:double,auditweights_likersSvd_prelaunch_hyper:double,auditweights_matrix:double,auditweights_notOriginalPhoto:double,auditweights_numDislikes:double,auditweights_numLikes:double,auditweights_numShows:double,auditweights_onlineVideo:double,auditweights_partAge:double,auditweights_partCtr:double,auditweights_partSvd:double,auditweights_processedVideo:double,auditweights_relationMasks:double,auditweights_source_LIVE_TOP:double,auditweights_source_MOVIE_TOP:double,auditweights_svd_prelaunch:double,auditweights_svd_spark:double,auditweights_userAge:double,auditweights_userOwner_CREATE_COMMENT:double,auditweights_userOwner_CREATE_IMAGE:double,auditweights_userOwner_CREATE_LIKE:double,auditweights_userOwner_IMAGE:double,auditweights_userOwner_MOVIE_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_MARK_CREATE:double,auditweights_userOwner_PHOTO_VIEW:double,auditweights_userOwner_TEXT:double,auditweights_userOwner_UNKNOWN:double,auditweights_userOwner_USER_DELETE_MESSAGE:double,auditweights_userOwner_USER_FEED_REMOVE:double,auditweights_userOwner_USER_FORUM_MESSAGE_CREATE:double,auditweights_userOwner_USER_INTERNAL_LIKE:double,auditweights_userOwner_USER_INTERNAL_UNLIKE:double,auditweights_userOwner_USER_PRESENT_SEND:double,auditweights_userOwner_USER_PROFILE_VIEW:double,auditweights_userOwner_USER_SEND_MESSAGE:double,auditweights_userOwner_USER_STATUS_COMMENT_CREATE:double,auditweights_userOwner_VIDEO:double,auditweights_userOwner_VOTE_POLL:double,auditweights_x_ActorsRelations_double_vecAssembler_0ad2c484cfbd:double,auditweights_likersSvd_spark_hyper:double,auditweights_source_PROMO:double,timeDelta_double_vecAssembler_0ad2c484cfbd:double,createdHour_double_vecAssembler_0ad2c484cfbd:double,auditedHour_double_vecAssembler_0ad2c484cfbd:double,instanceId_objectType_num_double_vecAssembler_0ad2c484cfbd:double,audit_resourceType_num_double_vecAssembler_0ad2c484cfbd:double,metadata_ownerType_num_double_vecAssembler_0ad2c484cfbd:double,membership_status_num_double_vecAssembler_0ad2c484cfbd:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.07 23:20:33 ERROR Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider[0m
[0m2021.07.07 23:20:33 ERROR removing nulls from dataset or using handleInvalid = "keep" or "skip".[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)[0m
[0m2021.07.07 23:20:33 ERROR 	... 25 more[0m
[0m2021.07.07 23:20:33 ERROR 21/07/07 23:20:33 WARN TaskSetManager: Lost task 2.0 in stage 16.0 (TID 168) (172.26.87.27 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$4302/0x0000000841967840: (struct<audit_pos_double_vecAssembler_0ad2c484cfbd:double,metadata_numCompanions_double_vecAssembler_0ad2c484cfbd:double,metadata_numPhotos_double_vecAssembler_0ad2c484cfbd:double,metadata_numPolls_double_vecAssembler_0ad2c484cfbd:double,metadata_numSymbols_double_vecAssembler_0ad2c484cfbd:double,metadata_numTokens_double_vecAssembler_0ad2c484cfbd:double,metadata_numVideos_double_vecAssembler_0ad2c484cfbd:double,metadata_totalVideoLength_double_vecAssembler_0ad2c484cfbd:double,userOwnerCounters_USER_FEED_REMOVE:double,userOwnerCounters_USER_PROFILE_VIEW:double,userOwnerCounters_VOTE_POLL:double,userOwnerCounters_USER_SEND_MESSAGE:double,userOwnerCounters_USER_DELETE_MESSAGE:double,userOwnerCounters_USER_INTERNAL_LIKE:double,userOwnerCounters_USER_INTERNAL_UNLIKE:double,userOwnerCounters_USER_STATUS_COMMENT_CREATE:double,userOwnerCounters_PHOTO_COMMENT_CREATE:double,userOwnerCounters_MOVIE_COMMENT_CREATE:double,userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,userOwnerCounters_COMMENT_INTERNAL_LIKE:double,userOwnerCounters_USER_FORUM_MESSAGE_CREATE:double,userOwnerCounters_PHOTO_MARK_CREATE:double,userOwnerCounters_PHOTO_VIEW:double,userOwnerCounters_PHOTO_PIN_BATCH_CREATE:double,userOwnerCounters_PHOTO_PIN_UPDATE:double,userOwnerCounters_USER_PRESENT_SEND:double,userOwnerCounters_UNKNOWN:double,userOwnerCounters_CREATE_TOPIC:double,userOwnerCounters_CREATE_IMAGE:double,userOwnerCounters_CREATE_MOVIE:double,userOwnerCounters_CREATE_COMMENT:double,userOwnerCounters_CREATE_LIKE:double,userOwnerCounters_TEXT:double,userOwnerCounters_IMAGE:double,userOwnerCounters_VIDEO:double,ownerUserCounters_USER_FEED_REMOVE:double,ownerUserCounters_USER_PROFILE_VIEW:double,ownerUserCounters_VOTE_POLL:double,ownerUserCounters_USER_SEND_MESSAGE:double,ownerUserCounters_USER_DELETE_MESSAGE:double,ownerUserCounters_USER_INTERNAL_LIKE:double,ownerUserCounters_USER_INTERNAL_UNLIKE:double,ownerUserCounters_USER_STATUS_COMMENT_CREATE:double,ownerUserCounters_PHOTO_COMMENT_CREATE:double,ownerUserCounters_MOVIE_COMMENT_CREATE:double,ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,ownerUserCounters_COMMENT_INTERNAL_LIKE:double,ownerUserCounters_USER_FORUM_MESSAGE_CREATE:double,ownerUserCounters_PHOTO_MARK_CREATE:double,ownerUserCounters_PHOTO_VIEW:double,ownerUserCounters_PHOTO_PIN_BATCH_CREATE:double,ownerUserCounters_PHOTO_PIN_UPDATE:double,ownerUserCounters_USER_PRESENT_SEND:double,ownerUserCounters_UNKNOWN:double,ownerUserCounters_CREATE_TOPIC:double,ownerUserCounters_CREATE_IMAGE:double,ownerUserCounters_CREATE_MOVIE:double,ownerUserCounters_CREATE_COMMENT:double,ownerUserCounters_CREATE_LIKE:double,ownerUserCounters_TEXT:double,ownerUserCounters_IMAGE:double,ownerUserCounters_VIDEO:double,owner_gender_double_vecAssembler_0ad2c484cfbd:double,owner_status_double_vecAssembler_0ad2c484cfbd:double,owner_ID_country_double_vecAssembler_0ad2c484cfbd:double,owner_ID_Location_double_vecAssembler_0ad2c484cfbd:double,owner_is_active_double_vecAssembler_0ad2c484cfbd:double,owner_is_deleted_double_vecAssembler_0ad2c484cfbd:double,owner_is_abused_double_vecAssembler_0ad2c484cfbd:double,owner_is_activated_double_vecAssembler_0ad2c484cfbd:double,owner_change_datime_double_vecAssembler_0ad2c484cfbd:double,owner_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,owner_region_double_vecAssembler_0ad2c484cfbd:double,user_create_date_double_vecAssembler_0ad2c484cfbd:double,user_birth_date_double_vecAssembler_0ad2c484cfbd:double,user_gender_double_vecAssembler_0ad2c484cfbd:double,user_status_double_vecAssembler_0ad2c484cfbd:double,user_ID_country_double_vecAssembler_0ad2c484cfbd:double,user_ID_Location_double_vecAssembler_0ad2c484cfbd:double,user_is_active_double_vecAssembler_0ad2c484cfbd:double,user_is_deleted_double_vecAssembler_0ad2c484cfbd:double,user_is_abused_double_vecAssembler_0ad2c484cfbd:double,user_is_activated_double_vecAssembler_0ad2c484cfbd:double,user_change_datime_double_vecAssembler_0ad2c484cfbd:double,user_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,user_region_double_vecAssembler_0ad2c484cfbd:double,auditweights_ageMs:double,auditweights_closed:double,auditweights_ctr_gender:double,auditweights_ctr_high:double,auditweights_ctr_negative:double,auditweights_dailyRecency:double,auditweights_feedOwner_RECOMMENDED_GROUP:double,auditweights_feedStats:double,auditweights_friendCommentFeeds:double,auditweights_friendCommenters:double,auditweights_friendLikes:double,auditweights_friendLikes_actors:double,auditweights_hasDetectedText:double,auditweights_hasText:double,auditweights_isPymk:double,auditweights_isRandom:double,auditweights_likersFeedStats_hyper:double,auditweights_likersSvd_prelaunch_hyper:double,auditweights_matrix:double,auditweights_notOriginalPhoto:double,auditweights_numDislikes:double,auditweights_numLikes:double,auditweights_numShows:double,auditweights_onlineVideo:double,auditweights_partAge:double,auditweights_partCtr:double,auditweights_partSvd:double,auditweights_processedVideo:double,auditweights_relationMasks:double,auditweights_source_LIVE_TOP:double,auditweights_source_MOVIE_TOP:double,auditweights_svd_prelaunch:double,auditweights_svd_spark:double,auditweights_userAge:double,auditweights_userOwner_CREATE_COMMENT:double,auditweights_userOwner_CREATE_IMAGE:double,auditweights_userOwner_CREATE_LIKE:double,auditweights_userOwner_IMAGE:double,auditweights_userOwner_MOVIE_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_MARK_CREATE:double,auditweights_userOwner_PHOTO_VIEW:double,auditweights_userOwner_TEXT:double,auditweights_userOwner_UNKNOWN:double,auditweights_userOwner_USER_DELETE_MESSAGE:double,auditweights_userOwner_USER_FEED_REMOVE:double,auditweights_userOwner_USER_FORUM_MESSAGE_CREATE:double,auditweights_userOwner_USER_INTERNAL_LIKE:double,auditweights_userOwner_USER_INTERNAL_UNLIKE:double,auditweights_userOwner_USER_PRESENT_SEND:double,auditweights_userOwner_USER_PROFILE_VIEW:double,auditweights_userOwner_USER_SEND_MESSAGE:double,auditweights_userOwner_USER_STATUS_COMMENT_CREATE:double,auditweights_userOwner_VIDEO:double,auditweights_userOwner_VOTE_POLL:double,auditweights_x_ActorsRelations_double_vecAssembler_0ad2c484cfbd:double,auditweights_likersSvd_spark_hyper:double,auditweights_source_PROMO:double,timeDelta_double_vecAssembler_0ad2c484cfbd:double,createdHour_double_vecAssembler_0ad2c484cfbd:double,auditedHour_double_vecAssembler_0ad2c484cfbd:double,instanceId_objectType_num_double_vecAssembler_0ad2c484cfbd:double,audit_resourceType_num_double_vecAssembler_0ad2c484cfbd:double,metadata_ownerType_num_double_vecAssembler_0ad2c484cfbd:double,membership_status_num_double_vecAssembler_0ad2c484cfbd:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.07 23:20:33 ERROR Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider[0m
[0m2021.07.07 23:20:33 ERROR removing nulls from dataset or using handleInvalid = "keep" or "skip".[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)[0m
[0m2021.07.07 23:20:33 ERROR 	... 25 more[0m
[0m2021.07.07 23:20:33 ERROR [0m
[0m2021.07.07 23:20:33 ERROR 21/07/07 23:20:33 ERROR TaskSetManager: Task 2 in stage 16.0 failed 1 times; aborting job[0m
[0m2021.07.07 23:20:33 ERROR 21/07/07 23:20:33 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 16.0 failed 1 times, most recent failure: Lost task 2.0 in stage 16.0 (TID 168) (172.26.87.27 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$4302/0x0000000841967840: (struct<audit_pos_double_vecAssembler_0ad2c484cfbd:double,metadata_numCompanions_double_vecAssembler_0ad2c484cfbd:double,metadata_numPhotos_double_vecAssembler_0ad2c484cfbd:double,metadata_numPolls_double_vecAssembler_0ad2c484cfbd:double,metadata_numSymbols_double_vecAssembler_0ad2c484cfbd:double,metadata_numTokens_double_vecAssembler_0ad2c484cfbd:double,metadata_numVideos_double_vecAssembler_0ad2c484cfbd:double,metadata_totalVideoLength_double_vecAssembler_0ad2c484cfbd:double,userOwnerCounters_USER_FEED_REMOVE:double,userOwnerCounters_USER_PROFILE_VIEW:double,userOwnerCounters_VOTE_POLL:double,userOwnerCounters_USER_SEND_MESSAGE:double,userOwnerCounters_USER_DELETE_MESSAGE:double,userOwnerCounters_USER_INTERNAL_LIKE:double,userOwnerCounters_USER_INTERNAL_UNLIKE:double,userOwnerCounters_USER_STATUS_COMMENT_CREATE:double,userOwnerCounters_PHOTO_COMMENT_CREATE:double,userOwnerCounters_MOVIE_COMMENT_CREATE:double,userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,userOwnerCounters_COMMENT_INTERNAL_LIKE:double,userOwnerCounters_USER_FORUM_MESSAGE_CREATE:double,userOwnerCounters_PHOTO_MARK_CREATE:double,userOwnerCounters_PHOTO_VIEW:double,userOwnerCounters_PHOTO_PIN_BATCH_CREATE:double,userOwnerCounters_PHOTO_PIN_UPDATE:double,userOwnerCounters_USER_PRESENT_SEND:double,userOwnerCounters_UNKNOWN:double,userOwnerCounters_CREATE_TOPIC:double,userOwnerCounters_CREATE_IMAGE:double,userOwnerCounters_CREATE_MOVIE:double,userOwnerCounters_CREATE_COMMENT:double,userOwnerCounters_CREATE_LIKE:double,userOwnerCounters_TEXT:double,userOwnerCounters_IMAGE:double,userOwnerCounters_VIDEO:double,ownerUserCounters_USER_FEED_REMOVE:double,ownerUserCounters_USER_PROFILE_VIEW:double,ownerUserCounters_VOTE_POLL:double,ownerUserCounters_USER_SEND_MESSAGE:double,ownerUserCounters_USER_DELETE_MESSAGE:double,ownerUserCounters_USER_INTERNAL_LIKE:double,ownerUserCounters_USER_INTERNAL_UNLIKE:double,ownerUserCounters_USER_STATUS_COMMENT_CREATE:double,ownerUserCounters_PHOTO_COMMENT_CREATE:double,ownerUserCounters_MOVIE_COMMENT_CREATE:double,ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,ownerUserCounters_COMMENT_INTERNAL_LIKE:double,ownerUserCounters_USER_FORUM_MESSAGE_CREATE:double,ownerUserCounters_PHOTO_MARK_CREATE:double,ownerUserCounters_PHOTO_VIEW:double,ownerUserCounters_PHOTO_PIN_BATCH_CREATE:double,ownerUserCounters_PHOTO_PIN_UPDATE:double,ownerUserCounters_USER_PRESENT_SEND:double,ownerUserCounters_UNKNOWN:double,ownerUserCounters_CREATE_TOPIC:double,ownerUserCounters_CREATE_IMAGE:double,ownerUserCounters_CREATE_MOVIE:double,ownerUserCounters_CREATE_COMMENT:double,ownerUserCounters_CREATE_LIKE:double,ownerUserCounters_TEXT:double,ownerUserCounters_IMAGE:double,ownerUserCounters_VIDEO:double,owner_gender_double_vecAssembler_0ad2c484cfbd:double,owner_status_double_vecAssembler_0ad2c484cfbd:double,owner_ID_country_double_vecAssembler_0ad2c484cfbd:double,owner_ID_Location_double_vecAssembler_0ad2c484cfbd:double,owner_is_active_double_vecAssembler_0ad2c484cfbd:double,owner_is_deleted_double_vecAssembler_0ad2c484cfbd:double,owner_is_abused_double_vecAssembler_0ad2c484cfbd:double,owner_is_activated_double_vecAssembler_0ad2c484cfbd:double,owner_change_datime_double_vecAssembler_0ad2c484cfbd:double,owner_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,owner_region_double_vecAssembler_0ad2c484cfbd:double,user_create_date_double_vecAssembler_0ad2c484cfbd:double,user_birth_date_double_vecAssembler_0ad2c484cfbd:double,user_gender_double_vecAssembler_0ad2c484cfbd:double,user_status_double_vecAssembler_0ad2c484cfbd:double,user_ID_country_double_vecAssembler_0ad2c484cfbd:double,user_ID_Location_double_vecAssembler_0ad2c484cfbd:double,user_is_active_double_vecAssembler_0ad2c484cfbd:double,user_is_deleted_double_vecAssembler_0ad2c484cfbd:double,user_is_abused_double_vecAssembler_0ad2c484cfbd:double,user_is_activated_double_vecAssembler_0ad2c484cfbd:double,user_change_datime_double_vecAssembler_0ad2c484cfbd:double,user_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,user_region_double_vecAssembler_0ad2c484cfbd:double,auditweights_ageMs:double,auditweights_closed:double,auditweights_ctr_gender:double,auditweights_ctr_high:double,auditweights_ctr_negative:double,auditweights_dailyRecency:double,auditweights_feedOwner_RECOMMENDED_GROUP:double,auditweights_feedStats:double,auditweights_friendCommentFeeds:double,auditweights_friendCommenters:double,auditweights_friendLikes:double,auditweights_friendLikes_actors:double,auditweights_hasDetectedText:double,auditweights_hasText:double,auditweights_isPymk:double,auditweights_isRandom:double,auditweights_likersFeedStats_hyper:double,auditweights_likersSvd_prelaunch_hyper:double,auditweights_matrix:double,auditweights_notOriginalPhoto:double,auditweights_numDislikes:double,auditweights_numLikes:double,auditweights_numShows:double,auditweights_onlineVideo:double,auditweights_partAge:double,auditweights_partCtr:double,auditweights_partSvd:double,auditweights_processedVideo:double,auditweights_relationMasks:double,auditweights_source_LIVE_TOP:double,auditweights_source_MOVIE_TOP:double,auditweights_svd_prelaunch:double,auditweights_svd_spark:double,auditweights_userAge:double,auditweights_userOwner_CREATE_COMMENT:double,auditweights_userOwner_CREATE_IMAGE:double,auditweights_userOwner_CREATE_LIKE:double,auditweights_userOwner_IMAGE:double,auditweights_userOwner_MOVIE_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_MARK_CREATE:double,auditweights_userOwner_PHOTO_VIEW:double,auditweights_userOwner_TEXT:double,auditweights_userOwner_UNKNOWN:double,auditweights_userOwner_USER_DELETE_MESSAGE:double,auditweights_userOwner_USER_FEED_REMOVE:double,auditweights_userOwner_USER_FORUM_MESSAGE_CREATE:double,auditweights_userOwner_USER_INTERNAL_LIKE:double,auditweights_userOwner_USER_INTERNAL_UNLIKE:double,auditweights_userOwner_USER_PRESENT_SEND:double,auditweights_userOwner_USER_PROFILE_VIEW:double,auditweights_userOwner_USER_SEND_MESSAGE:double,auditweights_userOwner_USER_STATUS_COMMENT_CREATE:double,auditweights_userOwner_VIDEO:double,auditweights_userOwner_VOTE_POLL:double,auditweights_x_ActorsRelations_double_vecAssembler_0ad2c484cfbd:double,auditweights_likersSvd_spark_hyper:double,auditweights_source_PROMO:double,timeDelta_double_vecAssembler_0ad2c484cfbd:double,createdHour_double_vecAssembler_0ad2c484cfbd:double,auditedHour_double_vecAssembler_0ad2c484cfbd:double,instanceId_objectType_num_double_vecAssembler_0ad2c484cfbd:double,audit_resourceType_num_double_vecAssembler_0ad2c484cfbd:double,metadata_ownerType_num_double_vecAssembler_0ad2c484cfbd:double,membership_status_num_double_vecAssembler_0ad2c484cfbd:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.07 23:20:33 ERROR Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider[0m
[0m2021.07.07 23:20:33 ERROR removing nulls from dataset or using handleInvalid = "keep" or "skip".[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)[0m
[0m2021.07.07 23:20:33 ERROR 	... 25 more[0m
[0m2021.07.07 23:20:33 ERROR [0m
[0m2021.07.07 23:20:33 ERROR Driver stacktrace:[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.Option.foreach(Option.scala:407)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2729)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.Dataset.first(Dataset.scala:2736)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.MinMaxScaler.fit(MinMaxScaler.scala:120)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.MinMaxScaler.fit(MinMaxScaler.scala:92)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.Iterator.foreach(Iterator.scala:941)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.Iterator.foreach$(Iterator.scala:941)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)[0m
[0m2021.07.07 23:20:33 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:100)[0m
[0m2021.07.07 23:20:33 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
[0m2021.07.07 23:20:33 ERROR Caused by: org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$4302/0x0000000841967840: (struct<audit_pos_double_vecAssembler_0ad2c484cfbd:double,metadata_numCompanions_double_vecAssembler_0ad2c484cfbd:double,metadata_numPhotos_double_vecAssembler_0ad2c484cfbd:double,metadata_numPolls_double_vecAssembler_0ad2c484cfbd:double,metadata_numSymbols_double_vecAssembler_0ad2c484cfbd:double,metadata_numTokens_double_vecAssembler_0ad2c484cfbd:double,metadata_numVideos_double_vecAssembler_0ad2c484cfbd:double,metadata_totalVideoLength_double_vecAssembler_0ad2c484cfbd:double,userOwnerCounters_USER_FEED_REMOVE:double,userOwnerCounters_USER_PROFILE_VIEW:double,userOwnerCounters_VOTE_POLL:double,userOwnerCounters_USER_SEND_MESSAGE:double,userOwnerCounters_USER_DELETE_MESSAGE:double,userOwnerCounters_USER_INTERNAL_LIKE:double,userOwnerCounters_USER_INTERNAL_UNLIKE:double,userOwnerCounters_USER_STATUS_COMMENT_CREATE:double,userOwnerCounters_PHOTO_COMMENT_CREATE:double,userOwnerCounters_MOVIE_COMMENT_CREATE:double,userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,userOwnerCounters_COMMENT_INTERNAL_LIKE:double,userOwnerCounters_USER_FORUM_MESSAGE_CREATE:double,userOwnerCounters_PHOTO_MARK_CREATE:double,userOwnerCounters_PHOTO_VIEW:double,userOwnerCounters_PHOTO_PIN_BATCH_CREATE:double,userOwnerCounters_PHOTO_PIN_UPDATE:double,userOwnerCounters_USER_PRESENT_SEND:double,userOwnerCounters_UNKNOWN:double,userOwnerCounters_CREATE_TOPIC:double,userOwnerCounters_CREATE_IMAGE:double,userOwnerCounters_CREATE_MOVIE:double,userOwnerCounters_CREATE_COMMENT:double,userOwnerCounters_CREATE_LIKE:double,userOwnerCounters_TEXT:double,userOwnerCounters_IMAGE:double,userOwnerCounters_VIDEO:double,ownerUserCounters_USER_FEED_REMOVE:double,ownerUserCounters_USER_PROFILE_VIEW:double,ownerUserCounters_VOTE_POLL:double,ownerUserCounters_USER_SEND_MESSAGE:double,ownerUserCounters_USER_DELETE_MESSAGE:double,ownerUserCounters_USER_INTERNAL_LIKE:double,ownerUserCounters_USER_INTERNAL_UNLIKE:double,ownerUserCounters_USER_STATUS_COMMENT_CREATE:double,ownerUserCounters_PHOTO_COMMENT_CREATE:double,ownerUserCounters_MOVIE_COMMENT_CREATE:double,ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,ownerUserCounters_COMMENT_INTERNAL_LIKE:double,ownerUserCounters_USER_FORUM_MESSAGE_CREATE:double,ownerUserCounters_PHOTO_MARK_CREATE:double,ownerUserCounters_PHOTO_VIEW:double,ownerUserCounters_PHOTO_PIN_BATCH_CREATE:double,ownerUserCounters_PHOTO_PIN_UPDATE:double,ownerUserCounters_USER_PRESENT_SEND:double,ownerUserCounters_UNKNOWN:double,ownerUserCounters_CREATE_TOPIC:double,ownerUserCounters_CREATE_IMAGE:double,ownerUserCounters_CREATE_MOVIE:double,ownerUserCounters_CREATE_COMMENT:double,ownerUserCounters_CREATE_LIKE:double,ownerUserCounters_TEXT:double,ownerUserCounters_IMAGE:double,ownerUserCounters_VIDEO:double,owner_gender_double_vecAssembler_0ad2c484cfbd:double,owner_status_double_vecAssembler_0ad2c484cfbd:double,owner_ID_country_double_vecAssembler_0ad2c484cfbd:double,owner_ID_Location_double_vecAssembler_0ad2c484cfbd:double,owner_is_active_double_vecAssembler_0ad2c484cfbd:double,owner_is_deleted_double_vecAssembler_0ad2c484cfbd:double,owner_is_abused_double_vecAssembler_0ad2c484cfbd:double,owner_is_activated_double_vecAssembler_0ad2c484cfbd:double,owner_change_datime_double_vecAssembler_0ad2c484cfbd:double,owner_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,owner_region_double_vecAssembler_0ad2c484cfbd:double,user_create_date_double_vecAssembler_0ad2c484cfbd:double,user_birth_date_double_vecAssembler_0ad2c484cfbd:double,user_gender_double_vecAssembler_0ad2c484cfbd:double,user_status_double_vecAssembler_0ad2c484cfbd:double,user_ID_country_double_vecAssembler_0ad2c484cfbd:double,user_ID_Location_double_vecAssembler_0ad2c484cfbd:double,user_is_active_double_vecAssembler_0ad2c484cfbd:double,user_is_deleted_double_vecAssembler_0ad2c484cfbd:double,user_is_abused_double_vecAssembler_0ad2c484cfbd:double,user_is_activated_double_vecAssembler_0ad2c484cfbd:double,user_change_datime_double_vecAssembler_0ad2c484cfbd:double,user_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,user_region_double_vecAssembler_0ad2c484cfbd:double,auditweights_ageMs:double,auditweights_closed:double,auditweights_ctr_gender:double,auditweights_ctr_high:double,auditweights_ctr_negative:double,auditweights_dailyRecency:double,auditweights_feedOwner_RECOMMENDED_GROUP:double,auditweights_feedStats:double,auditweights_friendCommentFeeds:double,auditweights_friendCommenters:double,auditweights_friendLikes:double,auditweights_friendLikes_actors:double,auditweights_hasDetectedText:double,auditweights_hasText:double,auditweights_isPymk:double,auditweights_isRandom:double,auditweights_likersFeedStats_hyper:double,auditweights_likersSvd_prelaunch_hyper:double,auditweights_matrix:double,auditweights_notOriginalPhoto:double,auditweights_numDislikes:double,auditweights_numLikes:double,auditweights_numShows:double,auditweights_onlineVideo:double,auditweights_partAge:double,auditweights_partCtr:double,auditweights_partSvd:double,auditweights_processedVideo:double,auditweights_relationMasks:double,auditweights_source_LIVE_TOP:double,auditweights_source_MOVIE_TOP:double,auditweights_svd_prelaunch:double,auditweights_svd_spark:double,auditweights_userAge:double,auditweights_userOwner_CREATE_COMMENT:double,auditweights_userOwner_CREATE_IMAGE:double,auditweights_userOwner_CREATE_LIKE:double,auditweights_userOwner_IMAGE:double,auditweights_userOwner_MOVIE_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_MARK_CREATE:double,auditweights_userOwner_PHOTO_VIEW:double,auditweights_userOwner_TEXT:double,auditweights_userOwner_UNKNOWN:double,auditweights_userOwner_USER_DELETE_MESSAGE:double,auditweights_userOwner_USER_FEED_REMOVE:double,auditweights_userOwner_USER_FORUM_MESSAGE_CREATE:double,auditweights_userOwner_USER_INTERNAL_LIKE:double,auditweights_userOwner_USER_INTERNAL_UNLIKE:double,auditweights_userOwner_USER_PRESENT_SEND:double,auditweights_userOwner_USER_PROFILE_VIEW:double,auditweights_userOwner_USER_SEND_MESSAGE:double,auditweights_userOwner_USER_STATUS_COMMENT_CREATE:double,auditweights_userOwner_VIDEO:double,auditweights_userOwner_VOTE_POLL:double,auditweights_x_ActorsRelations_double_vecAssembler_0ad2c484cfbd:double,auditweights_likersSvd_spark_hyper:double,auditweights_source_PROMO:double,timeDelta_double_vecAssembler_0ad2c484cfbd:double,createdHour_double_vecAssembler_0ad2c484cfbd:double,auditedHour_double_vecAssembler_0ad2c484cfbd:double,instanceId_objectType_num_double_vecAssembler_0ad2c484cfbd:double,audit_resourceType_num_double_vecAssembler_0ad2c484cfbd:double,metadata_ownerType_num_double_vecAssembler_0ad2c484cfbd:double,membership_status_num_double_vecAssembler_0ad2c484cfbd:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.07 23:20:33 ERROR Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider[0m
[0m2021.07.07 23:20:33 ERROR removing nulls from dataset or using handleInvalid = "keep" or "skip".[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)[0m
[0m2021.07.07 23:20:33 ERROR 	... 25 more[0m
[0m2021.07.07 23:20:33 ERROR [0m
[0m2021.07.07 23:20:33 ERROR Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 16.0 failed 1 times, most recent failure: Lost task 2.0 in stage 16.0 (TID 168) (172.26.87.27 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$4302/0x0000000841967840: (struct<audit_pos_double_vecAssembler_0ad2c484cfbd:double,metadata_numCompanions_double_vecAssembler_0ad2c484cfbd:double,metadata_numPhotos_double_vecAssembler_0ad2c484cfbd:double,metadata_numPolls_double_vecAssembler_0ad2c484cfbd:double,metadata_numSymbols_double_vecAssembler_0ad2c484cfbd:double,metadata_numTokens_double_vecAssembler_0ad2c484cfbd:double,metadata_numVideos_double_vecAssembler_0ad2c484cfbd:double,metadata_totalVideoLength_double_vecAssembler_0ad2c484cfbd:double,userOwnerCounters_USER_FEED_REMOVE:double,userOwnerCounters_USER_PROFILE_VIEW:double,userOwnerCounters_VOTE_POLL:double,userOwnerCounters_USER_SEND_MESSAGE:double,userOwnerCounters_USER_DELETE_MESSAGE:double,userOwnerCounters_USER_INTERNAL_LIKE:double,userOwnerCounters_USER_INTERNAL_UNLIKE:double,userOwnerCounters_USER_STATUS_COMMENT_CREATE:double,userOwnerCounters_PHOTO_COMMENT_CREATE:double,userOwnerCounters_MOVIE_COMMENT_CREATE:double,userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,userOwnerCounters_COMMENT_INTERNAL_LIKE:double,userOwnerCounters_USER_FORUM_MESSAGE_CREATE:double,userOwnerCounters_PHOTO_MARK_CREATE:double,userOwnerCounters_PHOTO_VIEW:double,userOwnerCounters_PHOTO_PIN_BATCH_CREATE:double,userOwnerCounters_PHOTO_PIN_UPDATE:double,userOwnerCounters_USER_PRESENT_SEND:double,userOwnerCounters_UNKNOWN:double,userOwnerCounters_CREATE_TOPIC:double,userOwnerCounters_CREATE_IMAGE:double,userOwnerCounters_CREATE_MOVIE:double,userOwnerCounters_CREATE_COMMENT:double,userOwnerCounters_CREATE_LIKE:double,userOwnerCounters_TEXT:double,userOwnerCounters_IMAGE:double,userOwnerCounters_VIDEO:double,ownerUserCounters_USER_FEED_REMOVE:double,ownerUserCounters_USER_PROFILE_VIEW:double,ownerUserCounters_VOTE_POLL:double,ownerUserCounters_USER_SEND_MESSAGE:double,ownerUserCounters_USER_DELETE_MESSAGE:double,ownerUserCounters_USER_INTERNAL_LIKE:double,ownerUserCounters_USER_INTERNAL_UNLIKE:double,ownerUserCounters_USER_STATUS_COMMENT_CREATE:double,ownerUserCounters_PHOTO_COMMENT_CREATE:double,ownerUserCounters_MOVIE_COMMENT_CREATE:double,ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,ownerUserCounters_COMMENT_INTERNAL_LIKE:double,ownerUserCounters_USER_FORUM_MESSAGE_CREATE:double,ownerUserCounters_PHOTO_MARK_CREATE:double,ownerUserCounters_PHOTO_VIEW:double,ownerUserCounters_PHOTO_PIN_BATCH_CREATE:double,ownerUserCounters_PHOTO_PIN_UPDATE:double,ownerUserCounters_USER_PRESENT_SEND:double,ownerUserCounters_UNKNOWN:double,ownerUserCounters_CREATE_TOPIC:double,ownerUserCounters_CREATE_IMAGE:double,ownerUserCounters_CREATE_MOVIE:double,ownerUserCounters_CREATE_COMMENT:double,ownerUserCounters_CREATE_LIKE:double,ownerUserCounters_TEXT:double,ownerUserCounters_IMAGE:double,ownerUserCounters_VIDEO:double,owner_gender_double_vecAssembler_0ad2c484cfbd:double,owner_status_double_vecAssembler_0ad2c484cfbd:double,owner_ID_country_double_vecAssembler_0ad2c484cfbd:double,owner_ID_Location_double_vecAssembler_0ad2c484cfbd:double,owner_is_active_double_vecAssembler_0ad2c484cfbd:double,owner_is_deleted_double_vecAssembler_0ad2c484cfbd:double,owner_is_abused_double_vecAssembler_0ad2c484cfbd:double,owner_is_activated_double_vecAssembler_0ad2c484cfbd:double,owner_change_datime_double_vecAssembler_0ad2c484cfbd:double,owner_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,owner_region_double_vecAssembler_0ad2c484cfbd:double,user_create_date_double_vecAssembler_0ad2c484cfbd:double,user_birth_date_double_vecAssembler_0ad2c484cfbd:double,user_gender_double_vecAssembler_0ad2c484cfbd:double,user_status_double_vecAssembler_0ad2c484cfbd:double,user_ID_country_double_vecAssembler_0ad2c484cfbd:double,user_ID_Location_double_vecAssembler_0ad2c484cfbd:double,user_is_active_double_vecAssembler_0ad2c484cfbd:double,user_is_deleted_double_vecAssembler_0ad2c484cfbd:double,user_is_abused_double_vecAssembler_0ad2c484cfbd:double,user_is_activated_double_vecAssembler_0ad2c484cfbd:double,user_change_datime_double_vecAssembler_0ad2c484cfbd:double,user_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,user_region_double_vecAssembler_0ad2c484cfbd:double,auditweights_ageMs:double,auditweights_closed:double,auditweights_ctr_gender:double,auditweights_ctr_high:double,auditweights_ctr_negative:double,auditweights_dailyRecency:double,auditweights_feedOwner_RECOMMENDED_GROUP:double,auditweights_feedStats:double,auditweights_friendCommentFeeds:double,auditweights_friendCommenters:double,auditweights_friendLikes:double,auditweights_friendLikes_actors:double,auditweights_hasDetectedText:double,auditweights_hasText:double,auditweights_isPymk:double,auditweights_isRandom:double,auditweights_likersFeedStats_hyper:double,auditweights_likersSvd_prelaunch_hyper:double,auditweights_matrix:double,auditweights_notOriginalPhoto:double,auditweights_numDislikes:double,auditweights_numLikes:double,auditweights_numShows:double,auditweights_onlineVideo:double,auditweights_partAge:double,auditweights_partCtr:double,auditweights_partSvd:double,auditweights_processedVideo:double,auditweights_relationMasks:double,auditweights_source_LIVE_TOP:double,auditweights_source_MOVIE_TOP:double,auditweights_svd_prelaunch:double,auditweights_svd_spark:double,auditweights_userAge:double,auditweights_userOwner_CREATE_COMMENT:double,auditweights_userOwner_CREATE_IMAGE:double,auditweights_userOwner_CREATE_LIKE:double,auditweights_userOwner_IMAGE:double,auditweights_userOwner_MOVIE_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_MARK_CREATE:double,auditweights_userOwner_PHOTO_VIEW:double,auditweights_userOwner_TEXT:double,auditweights_userOwner_UNKNOWN:double,auditweights_userOwner_USER_DELETE_MESSAGE:double,auditweights_userOwner_USER_FEED_REMOVE:double,auditweights_userOwner_USER_FORUM_MESSAGE_CREATE:double,auditweights_userOwner_USER_INTERNAL_LIKE:double,auditweights_userOwner_USER_INTERNAL_UNLIKE:double,auditweights_userOwner_USER_PRESENT_SEND:double,auditweights_userOwner_USER_PROFILE_VIEW:double,auditweights_userOwner_USER_SEND_MESSAGE:double,auditweights_userOwner_USER_STATUS_COMMENT_CREATE:double,auditweights_userOwner_VIDEO:double,auditweights_userOwner_VOTE_POLL:double,auditweights_x_ActorsRelations_double_vecAssembler_0ad2c484cfbd:double,auditweights_likersSvd_spark_hyper:double,auditweights_source_PROMO:double,timeDelta_double_vecAssembler_0ad2c484cfbd:double,createdHour_double_vecAssembler_0ad2c484cfbd:double,auditedHour_double_vecAssembler_0ad2c484cfbd:double,instanceId_objectType_num_double_vecAssembler_0ad2c484cfbd:double,audit_resourceType_num_double_vecAssembler_0ad2c484cfbd:double,metadata_ownerType_num_double_vecAssembler_0ad2c484cfbd:double,membership_status_num_double_vecAssembler_0ad2c484cfbd:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.07 23:20:33 ERROR Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider[0m
[0m2021.07.07 23:20:33 ERROR removing nulls from dataset or using handleInvalid = "keep" or "skip".[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)[0m
[0m2021.07.07 23:20:33 ERROR 	... 25 more[0m
[0m2021.07.07 23:20:33 ERROR [0m
[0m2021.07.07 23:20:33 ERROR Driver stacktrace:[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.Option.foreach(Option.scala:407)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2729)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.Dataset.first(Dataset.scala:2736)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.MinMaxScaler.fit(MinMaxScaler.scala:120)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.MinMaxScaler.fit(MinMaxScaler.scala:92)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.Iterator.foreach(Iterator.scala:941)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.Iterator.foreach$(Iterator.scala:941)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)[0m
[0m2021.07.07 23:20:33 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:100)[0m
[0m2021.07.07 23:20:33 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
[0m2021.07.07 23:20:33 ERROR Caused by: org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$4302/0x0000000841967840: (struct<audit_pos_double_vecAssembler_0ad2c484cfbd:double,metadata_numCompanions_double_vecAssembler_0ad2c484cfbd:double,metadata_numPhotos_double_vecAssembler_0ad2c484cfbd:double,metadata_numPolls_double_vecAssembler_0ad2c484cfbd:double,metadata_numSymbols_double_vecAssembler_0ad2c484cfbd:double,metadata_numTokens_double_vecAssembler_0ad2c484cfbd:double,metadata_numVideos_double_vecAssembler_0ad2c484cfbd:double,metadata_totalVideoLength_double_vecAssembler_0ad2c484cfbd:double,userOwnerCounters_USER_FEED_REMOVE:double,userOwnerCounters_USER_PROFILE_VIEW:double,userOwnerCounters_VOTE_POLL:double,userOwnerCounters_USER_SEND_MESSAGE:double,userOwnerCounters_USER_DELETE_MESSAGE:double,userOwnerCounters_USER_INTERNAL_LIKE:double,userOwnerCounters_USER_INTERNAL_UNLIKE:double,userOwnerCounters_USER_STATUS_COMMENT_CREATE:double,userOwnerCounters_PHOTO_COMMENT_CREATE:double,userOwnerCounters_MOVIE_COMMENT_CREATE:double,userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,userOwnerCounters_COMMENT_INTERNAL_LIKE:double,userOwnerCounters_USER_FORUM_MESSAGE_CREATE:double,userOwnerCounters_PHOTO_MARK_CREATE:double,userOwnerCounters_PHOTO_VIEW:double,userOwnerCounters_PHOTO_PIN_BATCH_CREATE:double,userOwnerCounters_PHOTO_PIN_UPDATE:double,userOwnerCounters_USER_PRESENT_SEND:double,userOwnerCounters_UNKNOWN:double,userOwnerCounters_CREATE_TOPIC:double,userOwnerCounters_CREATE_IMAGE:double,userOwnerCounters_CREATE_MOVIE:double,userOwnerCounters_CREATE_COMMENT:double,userOwnerCounters_CREATE_LIKE:double,userOwnerCounters_TEXT:double,userOwnerCounters_IMAGE:double,userOwnerCounters_VIDEO:double,ownerUserCounters_USER_FEED_REMOVE:double,ownerUserCounters_USER_PROFILE_VIEW:double,ownerUserCounters_VOTE_POLL:double,ownerUserCounters_USER_SEND_MESSAGE:double,ownerUserCounters_USER_DELETE_MESSAGE:double,ownerUserCounters_USER_INTERNAL_LIKE:double,ownerUserCounters_USER_INTERNAL_UNLIKE:double,ownerUserCounters_USER_STATUS_COMMENT_CREATE:double,ownerUserCounters_PHOTO_COMMENT_CREATE:double,ownerUserCounters_MOVIE_COMMENT_CREATE:double,ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:double,ownerUserCounters_COMMENT_INTERNAL_LIKE:double,ownerUserCounters_USER_FORUM_MESSAGE_CREATE:double,ownerUserCounters_PHOTO_MARK_CREATE:double,ownerUserCounters_PHOTO_VIEW:double,ownerUserCounters_PHOTO_PIN_BATCH_CREATE:double,ownerUserCounters_PHOTO_PIN_UPDATE:double,ownerUserCounters_USER_PRESENT_SEND:double,ownerUserCounters_UNKNOWN:double,ownerUserCounters_CREATE_TOPIC:double,ownerUserCounters_CREATE_IMAGE:double,ownerUserCounters_CREATE_MOVIE:double,ownerUserCounters_CREATE_COMMENT:double,ownerUserCounters_CREATE_LIKE:double,ownerUserCounters_TEXT:double,ownerUserCounters_IMAGE:double,ownerUserCounters_VIDEO:double,owner_gender_double_vecAssembler_0ad2c484cfbd:double,owner_status_double_vecAssembler_0ad2c484cfbd:double,owner_ID_country_double_vecAssembler_0ad2c484cfbd:double,owner_ID_Location_double_vecAssembler_0ad2c484cfbd:double,owner_is_active_double_vecAssembler_0ad2c484cfbd:double,owner_is_deleted_double_vecAssembler_0ad2c484cfbd:double,owner_is_abused_double_vecAssembler_0ad2c484cfbd:double,owner_is_activated_double_vecAssembler_0ad2c484cfbd:double,owner_change_datime_double_vecAssembler_0ad2c484cfbd:double,owner_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,owner_region_double_vecAssembler_0ad2c484cfbd:double,user_create_date_double_vecAssembler_0ad2c484cfbd:double,user_birth_date_double_vecAssembler_0ad2c484cfbd:double,user_gender_double_vecAssembler_0ad2c484cfbd:double,user_status_double_vecAssembler_0ad2c484cfbd:double,user_ID_country_double_vecAssembler_0ad2c484cfbd:double,user_ID_Location_double_vecAssembler_0ad2c484cfbd:double,user_is_active_double_vecAssembler_0ad2c484cfbd:double,user_is_deleted_double_vecAssembler_0ad2c484cfbd:double,user_is_abused_double_vecAssembler_0ad2c484cfbd:double,user_is_activated_double_vecAssembler_0ad2c484cfbd:double,user_change_datime_double_vecAssembler_0ad2c484cfbd:double,user_is_semiactivated_double_vecAssembler_0ad2c484cfbd:double,user_region_double_vecAssembler_0ad2c484cfbd:double,auditweights_ageMs:double,auditweights_closed:double,auditweights_ctr_gender:double,auditweights_ctr_high:double,auditweights_ctr_negative:double,auditweights_dailyRecency:double,auditweights_feedOwner_RECOMMENDED_GROUP:double,auditweights_feedStats:double,auditweights_friendCommentFeeds:double,auditweights_friendCommenters:double,auditweights_friendLikes:double,auditweights_friendLikes_actors:double,auditweights_hasDetectedText:double,auditweights_hasText:double,auditweights_isPymk:double,auditweights_isRandom:double,auditweights_likersFeedStats_hyper:double,auditweights_likersSvd_prelaunch_hyper:double,auditweights_matrix:double,auditweights_notOriginalPhoto:double,auditweights_numDislikes:double,auditweights_numLikes:double,auditweights_numShows:double,auditweights_onlineVideo:double,auditweights_partAge:double,auditweights_partCtr:double,auditweights_partSvd:double,auditweights_processedVideo:double,auditweights_relationMasks:double,auditweights_source_LIVE_TOP:double,auditweights_source_MOVIE_TOP:double,auditweights_svd_prelaunch:double,auditweights_svd_spark:double,auditweights_userAge:double,auditweights_userOwner_CREATE_COMMENT:double,auditweights_userOwner_CREATE_IMAGE:double,auditweights_userOwner_CREATE_LIKE:double,auditweights_userOwner_IMAGE:double,auditweights_userOwner_MOVIE_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_COMMENT_CREATE:double,auditweights_userOwner_PHOTO_MARK_CREATE:double,auditweights_userOwner_PHOTO_VIEW:double,auditweights_userOwner_TEXT:double,auditweights_userOwner_UNKNOWN:double,auditweights_userOwner_USER_DELETE_MESSAGE:double,auditweights_userOwner_USER_FEED_REMOVE:double,auditweights_userOwner_USER_FORUM_MESSAGE_CREATE:double,auditweights_userOwner_USER_INTERNAL_LIKE:double,auditweights_userOwner_USER_INTERNAL_UNLIKE:double,auditweights_userOwner_USER_PRESENT_SEND:double,auditweights_userOwner_USER_PROFILE_VIEW:double,auditweights_userOwner_USER_SEND_MESSAGE:double,auditweights_userOwner_USER_STATUS_COMMENT_CREATE:double,auditweights_userOwner_VIDEO:double,auditweights_userOwner_VOTE_POLL:double,auditweights_x_ActorsRelations_double_vecAssembler_0ad2c484cfbd:double,auditweights_likersSvd_spark_hyper:double,auditweights_source_PROMO:double,timeDelta_double_vecAssembler_0ad2c484cfbd:double,createdHour_double_vecAssembler_0ad2c484cfbd:double,auditedHour_double_vecAssembler_0ad2c484cfbd:double,instanceId_objectType_num_double_vecAssembler_0ad2c484cfbd:double,audit_resourceType_num_double_vecAssembler_0ad2c484cfbd:double,metadata_ownerType_num_double_vecAssembler_0ad2c484cfbd:double,membership_status_num_double_vecAssembler_0ad2c484cfbd:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:151)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.07 23:20:33 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.07 23:20:33 ERROR Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider[0m
[0m2021.07.07 23:20:33 ERROR removing nulls from dataset or using handleInvalid = "keep" or "skip".[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)[0m
[0m2021.07.07 23:20:33 ERROR 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)[0m
[0m2021.07.07 23:20:33 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)[0m
[0m2021.07.07 23:20:33 ERROR 	... 25 more[0m
Jul 07, 2021 11:20:41 PM org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer fireError
SEVERE: java.net.SocketException: Connection reset by peer (Write failed)
org.eclipse.lsp4j.jsonrpc.JsonRpcException: java.net.SocketException: Connection reset by peer (Write failed)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:72)
	at scala.meta.internal.metals.debug.SocketEndpoint.consume(SocketEndpoint.scala:22)
	at scala.meta.internal.metals.debug.MessageIdAdapter.consume(MessageIdAdapter.scala:43)
	at scala.meta.internal.metals.debug.ServerAdapter.send(ServerAdapter.scala:30)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleClientMessage$1(DebugProxy.scala:89)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToClient$1(DebugProxy.scala:54)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketException: Connection reset by peer (Write failed)
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)
	at java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:69)
	... 21 more

Jul 07, 2021 11:20:41 PM org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer fireError
SEVERE: java.net.SocketException: Broken pipe (Write failed)
org.eclipse.lsp4j.jsonrpc.JsonRpcException: java.net.SocketException: Broken pipe (Write failed)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:72)
	at scala.meta.internal.metals.debug.SocketEndpoint.consume(SocketEndpoint.scala:22)
	at scala.meta.internal.metals.debug.MessageIdAdapter.consume(MessageIdAdapter.scala:43)
	at scala.meta.internal.metals.debug.ServerAdapter.send(ServerAdapter.scala:30)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleClientMessage$1(DebugProxy.scala:89)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToClient$1(DebugProxy.scala:54)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketException: Broken pipe (Write failed)
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)
	at java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:69)
	... 21 more

[0m2021.07.07 23:20:49 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 23:24:50 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 23:24:50 INFO  time: compiled ht3 in 0.68s[0m
[0m2021.07.07 23:25:08 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 23:25:08 INFO  time: compiled ht3 in 0.49s[0m
[0m2021.07.07 23:25:14 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.07 23:25:16 INFO  time: compiled ht3 in 1.61s[0m
[0m2021.07.07 23:25:17 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 8h 28m 37.553s)[0m
[0m2021.07.07 23:25:17 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.07 23:25:17 INFO  time: compiled ht3-test in 0.23s[0m
[0m2021.07.07 23:25:18 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.07 23:25:17 INFO  Listening for transport dt_socket at address: 40207[0m
[0m2021.07.07 23:25:18 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.07 23:25:18 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 23:25:17 INFO  Trying to attach to remote debuggee VM localhost:40207 .[0m
[0m2021.07.07 23:25:17 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.07 23:25:19 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.07 23:25:19 ERROR 21/07/07 23:25:19 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.87.27 instead (on interface eth0)[0m
[0m2021.07.07 23:25:19 ERROR 21/07/07 23:25:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.07 23:25:20 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.07 23:25:20 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.07 23:25:20 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.07 23:25:20 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.07 23:25:20 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.07 23:25:20 ERROR 21/07/07 23:25:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.07 23:25:29 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.07 23:25:34 ERROR 21/07/07 23:25:34 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.07 23:25:39 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.07 23:25:39 INFO  |     features_scaled|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.07 23:25:39 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.07 23:25:39 INFO  |[0.01449275362318...|                (2,[0],[1.0])|             (4,[3],[1.0])|             (1,[0],[1.0])|            (8,[1],[1.0])|      (5,[],[])|       (6,[],[])|[0m
[0m2021.07.07 23:25:39 INFO  |[0.11231884057971...|                (2,[0],[1.0])|             (4,[3],[1.0])|                 (1,[],[])|            (8,[1],[1.0])|  (5,[4],[1.0])|       (6,[],[])|[0m
[0m2021.07.07 23:25:39 INFO  |[0.00724637681159...|                (2,[0],[1.0])|             (4,[3],[1.0])|             (1,[0],[1.0])|            (8,[1],[1.0])|  (5,[4],[1.0])|       (6,[],[])|[0m
[0m2021.07.07 23:25:39 INFO  |[0.00362318840579...|                (2,[0],[1.0])|             (4,[2],[1.0])|             (1,[0],[1.0])|                (8,[],[])|      (5,[],[])|       (6,[],[])|[0m
[0m2021.07.07 23:25:39 INFO  |[0.15217391304347...|                (2,[0],[1.0])|                 (4,[],[])|             (1,[0],[1.0])|                (8,[],[])|  (5,[4],[1.0])|       (6,[],[])|[0m
[0m2021.07.07 23:25:39 INFO  |[0.04347826086956...|                (2,[0],[1.0])|                 (4,[],[])|             (1,[0],[1.0])|                (8,[],[])|      (5,[],[])|       (6,[],[])|[0m
[0m2021.07.07 23:25:39 INFO  |[0.01449275362318...|                (2,[1],[1.0])|             (4,[0],[1.0])|                 (1,[],[])|                (8,[],[])|      (5,[],[])|       (6,[],[])|[0m
[0m2021.07.07 23:25:39 INFO  |[0.00724637681159...|                (2,[0],[1.0])|             (4,[3],[1.0])|             (1,[0],[1.0])|                (8,[],[])|      (5,[],[])|       (6,[],[])|[0m
[0m2021.07.07 23:25:39 INFO  |[0.0,0.0,0.075471...|                (2,[0],[1.0])|             (4,[3],[1.0])|             (1,[0],[1.0])|            (8,[1],[1.0])|      (5,[],[])|       (6,[],[])|[0m
[0m2021.07.07 23:25:39 INFO  |[0.00724637681159...|                (2,[0],[1.0])|             (4,[2],[1.0])|             (1,[0],[1.0])|            (8,[1],[1.0])|      (5,[],[])|       (6,[],[])|[0m
[0m2021.07.07 23:25:39 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.07 23:25:39 INFO  only showing top 10 rows[0m
[0m2021.07.07 23:25:39 INFO  [0m
[0m2021.07.07 23:25:40 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.07 23:26:47 INFO  shutting down Metals[0m
[0m2021.07.07 23:26:47 INFO  Shut down connection with build server.[0m
[0m2021.07.07 23:26:47 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.07.10 20:29:03 INFO  Started: Metals version 0.10.4 in workspace '/home/hwait/dev/ht3' for client Visual Studio Code 1.57.1.[0m
[0m2021.07.10 20:29:06 INFO  time: initialize in 3.02s[0m
[0m2021.07.10 20:29:06 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4354609801280490638/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.07.10 20:29:06 WARN  no build target for: /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala[0m
[0m2021.07.10 20:29:06 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.07.10 20:29:10 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8-19-4d9f966b...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.12.10.
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.6/jline-2.14.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.12/jansi-1.12.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.10/scala-compiler-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.10/scala-reflect-2.12.10.jar
[0m[32m[D][0m Configured SemanticDB in projects 'ht3-test', 'ht3'
[0m[32m[D][0m Loading previous analysis for 'ht3-test' from '/home/hwait/dev/ht3/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Loading previous analysis for 'ht3' from '/home/hwait/dev/ht3/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4354609801280490638/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4354609801280490638/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.10 20:29:11 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.10 20:29:11 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher11481685792089680885/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher11481685792089680885/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher11481685792089680885/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.10 20:29:11 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.10 20:29:11 INFO  time: Connected to build server in 5.18s[0m
[0m2021.07.10 20:29:11 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
[0m2021.07.10 20:29:12 INFO  time: Imported build in 0.3s[0m
[0m2021.07.10 20:29:15 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
[0m2021.07.10 20:29:15 INFO  time: code lens generation in 9.02s[0m
[0m2021.07.10 20:29:16 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.07.10 20:29:16 INFO  time: indexed workspace in 5.49s[0m
[0m2021.07.10 20:29:19 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.10 20:29:22 INFO  time: compiled ht3-test in 2.83s[0m
Jul 10, 2021 9:35:14 PM scala.meta.internal.pc.CompletionProvider expected$1
WARNING: offset 5322, count -1, length 10163
[0m2021.07.10 21:36:56 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.10 21:37:00 INFO  time: compiled ht3 in 3.75s[0m
[0m2021.07.10 21:37:07 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.10 21:37:07 INFO  time: compiled ht3 in 0.94s[0m
[0m2021.07.10 21:37:51 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.10 21:37:52 INFO  time: compiled ht3 in 1.03s[0m
[0m2021.07.10 21:38:05 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.10 21:38:09 INFO  time: compiled ht3 in 4.34s[0m
[0m2021.07.10 21:38:46 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.10 21:38:48 INFO  time: compiled ht3 in 2.29s[0m
[0m2021.07.10 21:38:54 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.10 21:38:54 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 1h 9m 42.482s)[0m
[0m2021.07.10 21:38:54 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.10 21:38:56 INFO  time: compiled ht3 in 1.99s[0m
[0m2021.07.10 21:38:56 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.10 21:38:56 INFO  time: compiled ht3-test in 0.17s[0m
[0m2021.07.10 21:38:56 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.10 21:38:56 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.10 21:38:56 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.10 21:38:56 INFO  Listening for transport dt_socket at address: 59083[0m
[0m2021.07.10 21:38:56 INFO  Trying to attach to remote debuggee VM localhost:59083 .[0m
[0m2021.07.10 21:38:56 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.10 21:38:58 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.10 21:38:58 ERROR 21/07/10 21:38:58 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.30.1 instead (on interface eth0)[0m
[0m2021.07.10 21:38:58 ERROR 21/07/10 21:38:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.10 21:38:59 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.10 21:38:59 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.10 21:38:59 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.10 21:38:59 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.10 21:38:59 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.10 21:38:59 ERROR 21/07/10 21:38:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.10 21:39:07 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.10 21:39:13 ERROR 21/07/10 21:39:13 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.10 21:39:18 INFO  +--------------------+[0m
[0m2021.07.10 21:39:18 INFO  |            features|[0m
[0m2021.07.10 21:39:18 INFO  +--------------------+[0m
[0m2021.07.10 21:39:18 INFO  |(177,[0,5,6,8,26,...|[0m
[0m2021.07.10 21:39:18 INFO  |(177,[0,5,8,19,26...|[0m
[0m2021.07.10 21:39:18 INFO  |(177,[0,5,6,8,19,...|[0m
[0m2021.07.10 21:39:18 INFO  |(177,[0,4,6,26,29...|[0m
[0m2021.07.10 21:39:18 INFO  |(177,[0,6,19,26,9...|[0m
[0m2021.07.10 21:39:18 INFO  |(177,[0,6,26,99,1...|[0m
[0m2021.07.10 21:39:18 INFO  |(177,[1,2,26,99,1...|[0m
[0m2021.07.10 21:39:18 INFO  |(177,[0,5,6,26,32...|[0m
[0m2021.07.10 21:39:18 INFO  |(177,[0,5,6,8,28,...|[0m
[0m2021.07.10 21:39:18 INFO  |(177,[0,4,6,8,26,...|[0m
[0m2021.07.10 21:39:18 INFO  +--------------------+[0m
[0m2021.07.10 21:39:18 INFO  only showing top 10 rows[0m
[0m2021.07.10 21:39:18 INFO  [0m
[0m2021.07.10 21:39:18 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.10 21:46:53 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.10 21:46:53 INFO  time: compiled ht3-test in 0.25s[0m
[0m2021.07.10 21:47:07 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.10 21:47:07 INFO  time: compiled ht3 in 0.42s[0m
[0m2021.07.10 21:47:19 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.10 21:47:21 INFO  time: compiled ht3 in 1.9s[0m
[0m2021.07.10 21:47:24 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 1h 18m 12.982s)[0m
[0m2021.07.10 21:47:24 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.10 21:47:24 INFO  time: compiled ht3-test in 0.22s[0m
[0m2021.07.10 21:47:24 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.10 21:47:24 INFO  Listening for transport dt_socket at address: 49785[0m
[0m2021.07.10 21:47:24 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.10 21:47:25 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.10 21:47:24 INFO  Trying to attach to remote debuggee VM localhost:49785 .[0m
[0m2021.07.10 21:47:24 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.10 21:47:26 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.10 21:47:26 ERROR 21/07/10 21:47:26 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.30.1 instead (on interface eth0)[0m
[0m2021.07.10 21:47:26 ERROR 21/07/10 21:47:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.10 21:47:27 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.10 21:47:27 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.10 21:47:27 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.10 21:47:27 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.10 21:47:27 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.10 21:47:27 ERROR 21/07/10 21:47:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.10 21:47:36 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.10 21:47:41 ERROR 21/07/10 21:47:41 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.10 21:47:46 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.10 21:47:46 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.10 21:47:46 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.10 21:47:46 INFO  |(177,[0,5,6,8,26,...|                (2,[0],[1.0])|             (4,[3],[1.0])|             (1,[0],[1.0])|            (8,[1],[1.0])|      (5,[],[])|       (6,[],[])|[0m
[0m2021.07.10 21:47:46 INFO  |(177,[0,5,8,19,26...|                (2,[0],[1.0])|             (4,[3],[1.0])|                 (1,[],[])|            (8,[1],[1.0])|  (5,[4],[1.0])|       (6,[],[])|[0m
[0m2021.07.10 21:47:46 INFO  |(177,[0,5,6,8,19,...|                (2,[0],[1.0])|             (4,[3],[1.0])|             (1,[0],[1.0])|            (8,[1],[1.0])|  (5,[4],[1.0])|       (6,[],[])|[0m
[0m2021.07.10 21:47:46 INFO  |(177,[0,4,6,26,29...|                (2,[0],[1.0])|             (4,[2],[1.0])|             (1,[0],[1.0])|                (8,[],[])|      (5,[],[])|       (6,[],[])|[0m
[0m2021.07.10 21:47:46 INFO  |(177,[0,6,19,26,9...|                (2,[0],[1.0])|                 (4,[],[])|             (1,[0],[1.0])|                (8,[],[])|  (5,[4],[1.0])|       (6,[],[])|[0m
[0m2021.07.10 21:47:46 INFO  |(177,[0,6,26,99,1...|                (2,[0],[1.0])|                 (4,[],[])|             (1,[0],[1.0])|                (8,[],[])|      (5,[],[])|       (6,[],[])|[0m
[0m2021.07.10 21:47:46 INFO  |(177,[1,2,26,99,1...|                (2,[1],[1.0])|             (4,[0],[1.0])|                 (1,[],[])|                (8,[],[])|      (5,[],[])|       (6,[],[])|[0m
[0m2021.07.10 21:47:46 INFO  |(177,[0,5,6,26,32...|                (2,[0],[1.0])|             (4,[3],[1.0])|             (1,[0],[1.0])|                (8,[],[])|      (5,[],[])|       (6,[],[])|[0m
[0m2021.07.10 21:47:46 INFO  |(177,[0,5,6,8,28,...|                (2,[0],[1.0])|             (4,[3],[1.0])|             (1,[0],[1.0])|            (8,[1],[1.0])|      (5,[],[])|       (6,[],[])|[0m
[0m2021.07.10 21:47:46 INFO  |(177,[0,4,6,8,26,...|                (2,[0],[1.0])|             (4,[2],[1.0])|             (1,[0],[1.0])|            (8,[1],[1.0])|      (5,[],[])|       (6,[],[])|[0m
[0m2021.07.10 21:47:46 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.10 21:47:46 INFO  only showing top 10 rows[0m
[0m2021.07.10 21:47:46 INFO  [0m
[0m2021.07.10 21:47:46 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.10 23:23:19 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 2h 54m 7.86s)[0m
[0m2021.07.10 23:23:19 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.10 23:23:19 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.10 23:23:24 INFO  time: compiled ht3 in 4.44s[0m
[0m2021.07.10 23:23:24 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.10 23:23:24 INFO  time: compiled ht3-test in 0.33s[0m
[0m2021.07.10 23:23:24 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.10 23:23:24 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.10 23:23:24 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.10 23:23:24 INFO  Listening for transport dt_socket at address: 34679[0m
[0m2021.07.10 23:23:24 INFO  Trying to attach to remote debuggee VM localhost:34679 .[0m
[0m2021.07.10 23:23:24 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.10 23:23:26 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.10 23:23:26 ERROR 21/07/10 23:23:26 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.30.1 instead (on interface eth0)[0m
[0m2021.07.10 23:23:26 ERROR 21/07/10 23:23:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in Array[String]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 1005, 1005, 1018)
[0m2021.07.10 23:23:27 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.10 23:23:27 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.10 23:23:27 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.10 23:23:27 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.10 23:23:27 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.10 23:23:27 ERROR 21/07/10 23:23:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.10 23:23:37 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.10 23:23:42 ERROR 21/07/10 23:23:42 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.10 23:23:47 INFO  +--------------------+-------------------------+----------------------+----------------------+---------------------+-----------+------------+[0m
[0m2021.07.10 23:23:47 INFO  |            features|instanceId_objectType_num|audit_resourceType_num|metadata_ownerType_num|membership_status_num|log_ownerId|log_authorId|[0m
[0m2021.07.10 23:23:47 INFO  +--------------------+-------------------------+----------------------+----------------------+---------------------+-----------+------------+[0m
[0m2021.07.10 23:23:47 INFO  |(177,[0,5,6,8,26,...|                        0|                     3|                     0|                    1|        5.0|         6.0|[0m
[0m2021.07.10 23:23:47 INFO  |(177,[0,5,8,19,26...|                        0|                     3|                     1|                    1|        4.0|         6.0|[0m
[0m2021.07.10 23:23:47 INFO  |(177,[0,5,6,8,19,...|                        0|                     3|                     0|                    1|        4.0|         6.0|[0m
[0m2021.07.10 23:23:47 INFO  |(177,[0,4,6,26,29...|                        0|                     2|                     0|                    8|        5.0|         6.0|[0m
[0m2021.07.10 23:23:47 INFO  |(177,[0,6,19,26,9...|                        0|                     4|                     0|                    8|        4.0|         6.0|[0m
[0m2021.07.10 23:23:47 INFO  |(177,[0,6,26,99,1...|                        0|                     4|                     0|                    8|        5.0|         6.0|[0m
[0m2021.07.10 23:23:47 INFO  |(177,[1,2,26,99,1...|                        1|                     0|                     1|                    8|        5.0|         6.0|[0m
[0m2021.07.10 23:23:47 INFO  |(177,[0,5,6,26,32...|                        0|                     3|                     0|                    8|        5.0|         6.0|[0m
[0m2021.07.10 23:23:47 INFO  |(177,[0,5,6,8,28,...|                        0|                     3|                     0|                    1|        5.0|         6.0|[0m
[0m2021.07.10 23:23:47 INFO  |(177,[0,4,6,8,26,...|                        0|                     2|                     0|                    1|        5.0|         6.0|[0m
[0m2021.07.10 23:23:47 INFO  +--------------------+-------------------------+----------------------+----------------------+---------------------+-----------+------------+[0m
[0m2021.07.10 23:23:47 INFO  only showing top 10 rows[0m
[0m2021.07.10 23:23:47 INFO  [0m
[0m2021.07.10 23:23:48 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
Exception in thread "pool-5-thread-1" java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[0m2021.07.10 23:35:07 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.10 23:35:10 INFO  time: compiled ht3 in 3.16s[0m
[0m2021.07.10 23:35:13 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 3h 6m 1.974s)[0m
[0m2021.07.10 23:35:13 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.10 23:35:13 INFO  time: compiled ht3-test in 0.4s[0m
[0m2021.07.10 23:35:14 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.10 23:35:13 INFO  Listening for transport dt_socket at address: 59033[0m
[0m2021.07.10 23:35:14 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.10 23:35:14 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.10 23:35:13 INFO  Trying to attach to remote debuggee VM localhost:59033 .[0m
[0m2021.07.10 23:35:13 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.10 23:35:15 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.10 23:35:15 ERROR 21/07/10 23:35:15 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.30.1 instead (on interface eth0)[0m
[0m2021.07.10 23:35:15 ERROR 21/07/10 23:35:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.10 23:35:16 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.10 23:35:16 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.10 23:35:16 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.10 23:35:16 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.10 23:35:16 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.10 23:35:16 ERROR 21/07/10 23:35:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.10 23:35:25 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.10 23:35:27 ERROR 21/07/10 23:35:27 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.10 23:35:32 INFO  +--------------------+[0m
[0m2021.07.10 23:35:32 INFO  |  features_numerical|[0m
[0m2021.07.10 23:35:32 INFO  +--------------------+[0m
[0m2021.07.10 23:35:32 INFO  |(151,[0,2,4,5,26,...|[0m
[0m2021.07.10 23:35:32 INFO  |(151,[0,2,4,5,8,2...|[0m
[0m2021.07.10 23:35:32 INFO  |(151,[0,4,5,6,32,...|[0m
[0m2021.07.10 23:35:32 INFO  |(151,[0,3,4,5,73,...|[0m
[0m2021.07.10 23:35:32 INFO  |(151,[0,73,74,75,...|[0m
[0m2021.07.10 23:35:32 INFO  |(151,[0,73,74,75,...|[0m
[0m2021.07.10 23:35:32 INFO  |(151,[0,73,74,75,...|[0m
[0m2021.07.10 23:35:32 INFO  |(151,[0,6,31,73,7...|[0m
[0m2021.07.10 23:35:32 INFO  |(151,[2,4,5,73,74...|[0m
[0m2021.07.10 23:35:32 INFO  |(151,[0,4,5,73,74...|[0m
[0m2021.07.10 23:35:32 INFO  +--------------------+[0m
[0m2021.07.10 23:35:32 INFO  only showing top 10 rows[0m
[0m2021.07.10 23:35:32 INFO  [0m
[0m2021.07.10 23:37:02 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.10 23:37:04 INFO  time: compiled ht3 in 1.67s[0m
[0m2021.07.10 23:37:07 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 3h 7m 55.777s)[0m
[0m2021.07.10 23:37:07 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.10 23:37:07 INFO  time: compiled ht3-test in 0.45s[0m
[0m2021.07.10 23:37:07 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.10 23:37:07 INFO  Listening for transport dt_socket at address: 53545[0m
[0m2021.07.10 23:37:07 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.10 23:37:08 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.10 23:37:07 INFO  Trying to attach to remote debuggee VM localhost:53545 .[0m
[0m2021.07.10 23:37:07 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.10 23:37:09 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.10 23:37:09 ERROR 21/07/10 23:37:09 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.30.1 instead (on interface eth0)[0m
[0m2021.07.10 23:37:09 ERROR 21/07/10 23:37:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.10 23:37:10 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.10 23:37:10 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.10 23:37:10 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.10 23:37:10 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.10 23:37:10 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.10 23:37:10 ERROR 21/07/10 23:37:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.10 23:37:17 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.10 23:37:19 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.10 23:37:19 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 3h 8m 7.489s)[0m
[0m2021.07.10 23:37:19 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.10 23:37:19 INFO  time: compiled ht3-test in 0.33s[0m
[0m2021.07.10 23:37:19 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.10 23:37:19 INFO  Listening for transport dt_socket at address: 56609[0m
[0m2021.07.10 23:37:19 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.10 23:37:19 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.10 23:37:19 INFO  Trying to attach to remote debuggee VM localhost:56609 .[0m
[0m2021.07.10 23:37:19 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.10 23:37:21 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.10 23:37:21 ERROR 21/07/10 23:37:21 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.30.1 instead (on interface eth0)[0m
[0m2021.07.10 23:37:21 ERROR 21/07/10 23:37:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.10 23:37:21 ERROR 21/07/10 23:37:21 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.10 23:37:22 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.10 23:37:22 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.10 23:37:22 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.10 23:37:22 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.10 23:37:22 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.10 23:37:22 ERROR 21/07/10 23:37:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.10 23:37:24 ERROR 21/07/10 23:37:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.[0m
[0m2021.07.10 23:37:27 INFO  +--------------------+[0m
[0m2021.07.10 23:37:27 INFO  |     features_scaled|[0m
[0m2021.07.10 23:37:27 INFO  +--------------------+[0m
[0m2021.07.10 23:37:27 INFO  |[0.01449275362318...|[0m
[0m2021.07.10 23:37:27 INFO  |[0.11231884057971...|[0m
[0m2021.07.10 23:37:27 INFO  |[0.00724637681159...|[0m
[0m2021.07.10 23:37:27 INFO  |[0.00362318840579...|[0m
[0m2021.07.10 23:37:27 INFO  |[0.15217391304347...|[0m
[0m2021.07.10 23:37:27 INFO  |[0.04347826086956...|[0m
[0m2021.07.10 23:37:27 INFO  |[0.01449275362318...|[0m
[0m2021.07.10 23:37:27 INFO  |[0.00724637681159...|[0m
[0m2021.07.10 23:37:27 INFO  |[0.0,0.0,0.075471...|[0m
[0m2021.07.10 23:37:27 INFO  |[0.00724637681159...|[0m
[0m2021.07.10 23:37:27 INFO  +--------------------+[0m
[0m2021.07.10 23:37:27 INFO  only showing top 10 rows[0m
[0m2021.07.10 23:37:27 INFO  [0m
[0m2021.07.10 23:37:28 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.10 23:37:27 ERROR Read data from io exception: java.net.SocketException: Socket closed[0m
[0m2021.07.10 23:37:32 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.10 23:37:34 ERROR 21/07/10 23:37:34 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.10 23:37:39 INFO  +--------------------+[0m
[0m2021.07.10 23:37:39 INFO  |     features_scaled|[0m
[0m2021.07.10 23:37:39 INFO  +--------------------+[0m
[0m2021.07.10 23:37:39 INFO  |[0.01449275362318...|[0m
[0m2021.07.10 23:37:39 INFO  |[0.11231884057971...|[0m
[0m2021.07.10 23:37:39 INFO  |[0.00724637681159...|[0m
[0m2021.07.10 23:37:39 INFO  |[0.00362318840579...|[0m
[0m2021.07.10 23:37:39 INFO  |[0.15217391304347...|[0m
[0m2021.07.10 23:37:39 INFO  |[0.04347826086956...|[0m
[0m2021.07.10 23:37:39 INFO  |[0.01449275362318...|[0m
[0m2021.07.10 23:37:39 INFO  |[0.00724637681159...|[0m
[0m2021.07.10 23:37:39 INFO  |[0.0,0.0,0.075471...|[0m
[0m2021.07.10 23:37:39 INFO  |[0.00724637681159...|[0m
[0m2021.07.10 23:37:39 INFO  +--------------------+[0m
[0m2021.07.10 23:37:39 INFO  only showing top 10 rows[0m
[0m2021.07.10 23:37:39 INFO  [0m
[0m2021.07.10 23:51:06 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.10 23:51:06 INFO  shutting down Metals[0m
[0m2021.07.10 23:51:06 INFO  Shut down connection with build server.[0m
[0m2021.07.10 23:51:06 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.07.12 12:58:07 INFO  Started: Metals version 0.10.4 in workspace '/home/hwait/dev/ht3' for client Visual Studio Code 1.58.0.[0m
[0m2021.07.12 12:58:10 INFO  time: initialize in 2.58s[0m
[0m2021.07.12 12:58:10 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher10113657968884606461/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.07.12 12:58:10 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8-19-4d9f966b...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.12.10.
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.6/jline-2.14.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.12/jansi-1.12.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.10/scala-compiler-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.10/scala-reflect-2.12.10.jar
[0m[32m[D][0m Configured SemanticDB in projects 'ht3', 'ht3-test'
[0m[32m[D][0m Loading previous analysis for 'ht3' from '/home/hwait/dev/ht3/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Loading previous analysis for 'ht3-test' from '/home/hwait/dev/ht3/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher10113657968884606461/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher10113657968884606461/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.12 12:58:14 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.12 12:58:14 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher3437184107166936491/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher3437184107166936491/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher3437184107166936491/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.12 12:58:15 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.12 12:58:15 INFO  time: Connected to build server in 5.07s[0m
[0m2021.07.12 12:58:15 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
[0m2021.07.12 12:58:15 INFO  time: Imported build in 0.16s[0m
[0m2021.07.12 12:58:19 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.07.12 12:58:19 INFO  time: indexed workspace in 4.53s[0m
[0m2021.07.12 12:58:33 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 12:58:37 INFO  time: compiled ht3 in 3.69s[0m
[0m2021.07.12 12:59:07 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 12:59:09 INFO  time: compiled ht3 in 1.12s[0m
[0m2021.07.12 12:59:19 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 12:59:22 INFO  time: compiled ht3 in 3.65s[0m
[0m2021.07.12 12:59:26 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 12:59:26 INFO  time: compiled ht3-test in 0.38s[0m
[0m2021.07.12 12:59:27 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 12:59:27 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 12:59:27 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.12 12:59:26 INFO  Listening for transport dt_socket at address: 46649[0m
[0m2021.07.12 12:59:26 INFO  Trying to attach to remote debuggee VM localhost:46649 .[0m
[0m2021.07.12 12:59:26 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 12:59:28 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.12 12:59:28 ERROR 21/07/12 12:59:28 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.149.194 instead (on interface eth0)[0m
[0m2021.07.12 12:59:28 ERROR 21/07/12 12:59:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.12 12:59:29 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.12 12:59:29 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.12 12:59:29 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.12 12:59:29 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.12 12:59:29 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.12 12:59:29 ERROR 21/07/12 12:59:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.12 12:59:38 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.12 12:59:40 ERROR 21/07/12 12:59:40 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.12 12:59:46 INFO  +--------------------+[0m
[0m2021.07.12 12:59:46 INFO  |     features_scaled|[0m
[0m2021.07.12 12:59:46 INFO  +--------------------+[0m
[0m2021.07.12 12:59:46 INFO  |[1.74656375425114...|[0m
[0m2021.07.12 12:59:46 INFO  |[1.35356662711564...|[0m
[0m2021.07.12 12:59:46 INFO  |[8.51792129331685...|[0m
[0m2021.07.12 12:59:46 INFO  |[4.38614234402984...|[0m
[0m2021.07.12 12:59:46 INFO  |[1.84050811607748...|[0m
[0m2021.07.12 12:59:46 INFO  |[5.25859461863939...|[0m
[0m2021.07.12 12:59:46 INFO  |[1.75286487176416...|[0m
[0m2021.07.12 12:59:46 INFO  |[8.76432436512448...|[0m
[0m2021.07.12 12:59:46 INFO  |[0.0,0.0,5.488506...|[0m
[0m2021.07.12 12:59:46 INFO  |[8.73242970041492...|[0m
[0m2021.07.12 12:59:46 INFO  +--------------------+[0m
[0m2021.07.12 12:59:46 INFO  only showing top 10 rows[0m
[0m2021.07.12 12:59:46 INFO  [0m
[0m2021.07.12 12:59:46 ERROR Exception in thread "main" java.lang.IllegalArgumentException: output does not exist. Available: instanceId_userId, instanceId_objectType, instanceId_objectId, audit_pos, audit_clientType, audit_timestamp, audit_timePassed, audit_experiment, audit_resourceType, metadata_ownerId, metadata_ownerType, metadata_createdAt, metadata_authorId, metadata_applicationId, metadata_numCompanions, metadata_numPhotos, metadata_numPolls, metadata_numSymbols, metadata_numTokens, metadata_numVideos, metadata_platform, metadata_totalVideoLength, metadata_options, relationsMask, userOwnerCounters_USER_FEED_REMOVE, userOwnerCounters_USER_PROFILE_VIEW, userOwnerCounters_VOTE_POLL, userOwnerCounters_USER_SEND_MESSAGE, userOwnerCounters_USER_DELETE_MESSAGE, userOwnerCounters_USER_INTERNAL_LIKE, userOwnerCounters_USER_INTERNAL_UNLIKE, userOwnerCounters_USER_STATUS_COMMENT_CREATE, userOwnerCounters_PHOTO_COMMENT_CREATE, userOwnerCounters_MOVIE_COMMENT_CREATE, userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE, userOwnerCounters_COMMENT_INTERNAL_LIKE, userOwnerCounters_USER_FORUM_MESSAGE_CREATE, userOwnerCounters_PHOTO_MARK_CREATE, userOwnerCounters_PHOTO_VIEW, userOwnerCounters_PHOTO_PIN_BATCH_CREATE, userOwnerCounters_PHOTO_PIN_UPDATE, userOwnerCounters_USER_PRESENT_SEND, userOwnerCounters_UNKNOWN, userOwnerCounters_CREATE_TOPIC, userOwnerCounters_CREATE_IMAGE, userOwnerCounters_CREATE_MOVIE, userOwnerCounters_CREATE_COMMENT, userOwnerCounters_CREATE_LIKE, userOwnerCounters_TEXT, userOwnerCounters_IMAGE, userOwnerCounters_VIDEO, ownerUserCounters_USER_FEED_REMOVE, ownerUserCounters_USER_PROFILE_VIEW, ownerUserCounters_VOTE_POLL, ownerUserCounters_USER_SEND_MESSAGE, ownerUserCounters_USER_DELETE_MESSAGE, ownerUserCounters_USER_INTERNAL_LIKE, ownerUserCounters_USER_INTERNAL_UNLIKE, ownerUserCounters_USER_STATUS_COMMENT_CREATE, ownerUserCounters_PHOTO_COMMENT_CREATE, ownerUserCounters_MOVIE_COMMENT_CREATE, ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE, ownerUserCounters_COMMENT_INTERNAL_LIKE, ownerUserCounters_USER_FORUM_MESSAGE_CREATE, ownerUserCounters_PHOTO_MARK_CREATE, ownerUserCounters_PHOTO_VIEW, ownerUserCounters_PHOTO_PIN_BATCH_CREATE, ownerUserCounters_PHOTO_PIN_UPDATE, ownerUserCounters_USER_PRESENT_SEND, ownerUserCounters_UNKNOWN, ownerUserCounters_CREATE_TOPIC, ownerUserCounters_CREATE_IMAGE, ownerUserCounters_CREATE_MOVIE, ownerUserCounters_CREATE_COMMENT, ownerUserCounters_CREATE_LIKE, ownerUserCounters_TEXT, ownerUserCounters_IMAGE, ownerUserCounters_VIDEO, membership_status, membership_statusUpdateDate, membership_joinDate, membership_joinRequestDate, owner_create_date, owner_birth_date, owner_gender, owner_status, owner_ID_country, owner_ID_Location, owner_is_active, owner_is_deleted, owner_is_abused, owner_is_activated, owner_change_datime, owner_is_semiactivated, owner_region, user_create_date, user_birth_date, user_gender, user_status, user_ID_country, user_ID_Location, user_is_active, user_is_deleted, user_is_abused, user_is_activated, user_change_datime, user_is_semiactivated, user_region, feedback, objectId, auditweights_ageMs, auditweights_closed, auditweights_ctr_gender, auditweights_ctr_high, auditweights_ctr_negative, auditweights_dailyRecency, auditweights_feedOwner_RECOMMENDED_GROUP, auditweights_feedStats, auditweights_friendCommentFeeds, auditweights_friendCommenters, auditweights_friendLikes, auditweights_friendLikes_actors, auditweights_hasDetectedText, auditweights_hasText, auditweights_isPymk, auditweights_isRandom, auditweights_likersFeedStats_hyper, auditweights_likersSvd_prelaunch_hyper, auditweights_matrix, auditweights_notOriginalPhoto, auditweights_numDislikes, auditweights_numLikes, auditweights_numShows, auditweights_onlineVideo, auditweights_partAge, auditweights_partCtr, auditweights_partSvd, auditweights_processedVideo, auditweights_relationMasks, auditweights_source_LIVE_TOP, auditweights_source_MOVIE_TOP, auditweights_svd_prelaunch, auditweights_svd_spark, auditweights_userAge, auditweights_userOwner_CREATE_COMMENT, auditweights_userOwner_CREATE_IMAGE, auditweights_userOwner_CREATE_LIKE, auditweights_userOwner_IMAGE, auditweights_userOwner_MOVIE_COMMENT_CREATE, auditweights_userOwner_PHOTO_COMMENT_CREATE, auditweights_userOwner_PHOTO_MARK_CREATE, auditweights_userOwner_PHOTO_VIEW, auditweights_userOwner_TEXT, auditweights_userOwner_UNKNOWN, auditweights_userOwner_USER_DELETE_MESSAGE, auditweights_userOwner_USER_FEED_REMOVE, auditweights_userOwner_USER_FORUM_MESSAGE_CREATE, auditweights_userOwner_USER_INTERNAL_LIKE, auditweights_userOwner_USER_INTERNAL_UNLIKE, auditweights_userOwner_USER_PRESENT_SEND, auditweights_userOwner_USER_PROFILE_VIEW, auditweights_userOwner_USER_SEND_MESSAGE, auditweights_userOwner_USER_STATUS_COMMENT_CREATE, auditweights_userOwner_VIDEO, auditweights_userOwner_VOTE_POLL, auditweights_x_ActorsRelations, auditweights_likersSvd_spark_hyper, auditweights_source_PROMO, date, target, createdTime, auditedTime, timeDelta, createdHour, auditedHour, log_ownerId, log_authorId, instanceId_objectType_num, audit_resourceType_num, metadata_ownerType_num, membership_status_num, features_numerical, features_norm, features_scaled[0m
[0m2021.07.12 12:59:46 ERROR 	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)[0m
[0m2021.07.12 12:59:46 ERROR 	at scala.collection.MapLike.getOrElse(MapLike.scala:131)[0m
[0m2021.07.12 12:59:46 ERROR 	at scala.collection.MapLike.getOrElse$(MapLike.scala:129)[0m
[0m2021.07.12 12:59:46 ERROR 	at scala.collection.AbstractMap.getOrElse(Map.scala:63)[0m
[0m2021.07.12 12:59:46 ERROR 	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)[0m
[0m2021.07.12 12:59:46 ERROR 	at org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:75)[0m
[0m2021.07.12 12:59:46 ERROR 	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:54)[0m
[0m2021.07.12 12:59:46 ERROR 	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:47)[0m
[0m2021.07.12 12:59:46 ERROR 	at org.apache.spark.ml.regression.LinearRegression.org$apache$spark$ml$regression$LinearRegressionParams$$super$validateAndTransformSchema(LinearRegression.scala:185)[0m
[0m2021.07.12 12:59:46 ERROR 	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema(LinearRegression.scala:124)[0m
[0m2021.07.12 12:59:46 ERROR 	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema$(LinearRegression.scala:112)[0m
[0m2021.07.12 12:59:46 ERROR 	at org.apache.spark.ml.regression.LinearRegression.validateAndTransformSchema(LinearRegression.scala:185)[0m
[0m2021.07.12 12:59:46 ERROR 	at org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:177)[0m
[0m2021.07.12 12:59:46 ERROR 	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)[0m
[0m2021.07.12 12:59:46 ERROR 	at org.apache.spark.ml.Predictor.fit(Predictor.scala:133)[0m
[0m2021.07.12 12:59:46 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:123)[0m
[0m2021.07.12 12:59:46 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
[0m2021.07.12 12:59:49 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.12 13:02:56 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 4m 41.634s)[0m
[0m2021.07.12 13:02:56 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:02:56 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:02:58 INFO  time: compiled ht3 in 2.21s[0m
[0m2021.07.12 13:02:58 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 13:02:58 INFO  time: compiled ht3-test in 0.21s[0m
[0m2021.07.12 13:02:59 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 13:02:58 INFO  Listening for transport dt_socket at address: 35321[0m
[0m2021.07.12 13:02:59 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 13:02:59 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.12 13:02:58 INFO  Trying to attach to remote debuggee VM localhost:35321 .[0m
[0m2021.07.12 13:02:58 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 13:03:00 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.12 13:03:00 ERROR 21/07/12 13:03:00 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.149.194 instead (on interface eth0)[0m
[0m2021.07.12 13:03:00 ERROR 21/07/12 13:03:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.12 13:03:01 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.12 13:03:01 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.12 13:03:01 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.12 13:03:01 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.12 13:03:01 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.12 13:03:01 ERROR 21/07/12 13:03:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.12 13:03:10 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.12 13:03:12 ERROR 21/07/12 13:03:12 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.12 13:03:18 INFO  +--------------------+[0m
[0m2021.07.12 13:03:18 INFO  |     features_scaled|[0m
[0m2021.07.12 13:03:18 INFO  +--------------------+[0m
[0m2021.07.12 13:03:18 INFO  |[1.74656375425114...|[0m
[0m2021.07.12 13:03:18 INFO  |[1.35356662711564...|[0m
[0m2021.07.12 13:03:18 INFO  |[8.51792129331685...|[0m
[0m2021.07.12 13:03:18 INFO  |[4.38614234402984...|[0m
[0m2021.07.12 13:03:18 INFO  |[1.84050811607748...|[0m
[0m2021.07.12 13:03:18 INFO  |[5.25859461863939...|[0m
[0m2021.07.12 13:03:18 INFO  |[1.75286487176416...|[0m
[0m2021.07.12 13:03:18 INFO  |[8.76432436512448...|[0m
[0m2021.07.12 13:03:18 INFO  |[0.0,0.0,5.488506...|[0m
[0m2021.07.12 13:03:18 INFO  |[8.73242970041492...|[0m
[0m2021.07.12 13:03:18 INFO  +--------------------+[0m
[0m2021.07.12 13:03:18 INFO  only showing top 10 rows[0m
[0m2021.07.12 13:03:18 INFO  [0m
[0m2021.07.12 13:03:20 ERROR 21/07/12 13:03:20 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.12 13:03:20 ERROR 21/07/12 13:03:20 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.12 13:03:26 ERROR 21/07/12 13:03:26 INFO OWLQN: Converged because gradient converged[0m
[0m2021.07.12 13:03:31 INFO  Coefficients: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] Intercept: 0.18015161191791854[0m
[0m2021.07.12 13:03:31 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.12 13:11:36 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:11:36 INFO  time: compiled ht3 in 0.51s[0m
[0m2021.07.12 13:16:16 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:16:18 INFO  time: compiled ht3 in 1.75s[0m
[0m2021.07.12 13:16:20 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 18m 4.904s)[0m
[0m2021.07.12 13:16:20 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 13:16:20 INFO  time: compiled ht3-test in 0.23s[0m
[0m2021.07.12 13:16:20 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 13:16:20 INFO  Listening for transport dt_socket at address: 53479[0m
[0m2021.07.12 13:16:20 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 13:16:20 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.12 13:16:20 INFO  Trying to attach to remote debuggee VM localhost:53479 .[0m
[0m2021.07.12 13:16:20 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 13:16:22 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.12 13:16:22 ERROR 21/07/12 13:16:22 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.149.194 instead (on interface eth0)[0m
[0m2021.07.12 13:16:22 ERROR 21/07/12 13:16:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.12 13:16:23 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.12 13:16:23 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.12 13:16:23 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.12 13:16:23 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.12 13:16:23 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.12 13:16:23 ERROR 21/07/12 13:16:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.12 13:16:31 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.12 13:16:33 ERROR 21/07/12 13:16:33 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.12 13:16:39 INFO  +--------------------+[0m
[0m2021.07.12 13:16:39 INFO  |     features_scaled|[0m
[0m2021.07.12 13:16:39 INFO  +--------------------+[0m
[0m2021.07.12 13:16:39 INFO  |[1.74656375425114...|[0m
[0m2021.07.12 13:16:39 INFO  |[1.35356662711564...|[0m
[0m2021.07.12 13:16:39 INFO  |[8.51792129331685...|[0m
[0m2021.07.12 13:16:39 INFO  |[4.38614234402984...|[0m
[0m2021.07.12 13:16:39 INFO  |[1.84050811607748...|[0m
[0m2021.07.12 13:16:39 INFO  |[5.25859461863939...|[0m
[0m2021.07.12 13:16:39 INFO  |[1.75286487176416...|[0m
[0m2021.07.12 13:16:39 INFO  |[8.76432436512448...|[0m
[0m2021.07.12 13:16:39 INFO  |[0.0,0.0,5.488506...|[0m
[0m2021.07.12 13:16:39 INFO  |[8.73242970041492...|[0m
[0m2021.07.12 13:16:39 INFO  +--------------------+[0m
[0m2021.07.12 13:16:39 INFO  only showing top 10 rows[0m
[0m2021.07.12 13:16:39 INFO  [0m
[0m2021.07.12 13:16:40 ERROR 21/07/12 13:16:40 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.12 13:16:40 ERROR 21/07/12 13:16:40 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.12 13:16:45 ERROR 21/07/12 13:16:45 INFO OWLQN: Converged because gradient converged[0m
[0m2021.07.12 13:16:50 INFO  Coefficients: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] Intercept: 0.18015161191791854[0m
[0m2021.07.12 13:16:50 INFO  org.apache.spark.ml.regression.LinearRegressionTrainingSummary@42ff6af7[0m
[0m2021.07.12 13:17:43 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:17:45 INFO  time: compiled ht3 in 1.87s[0m
[0m2021.07.12 13:17:51 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 19m 35.92s)[0m
[0m2021.07.12 13:17:51 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 13:17:51 INFO  time: compiled ht3-test in 0.23s[0m
[0m2021.07.12 13:17:51 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 13:17:51 INFO  Listening for transport dt_socket at address: 58113[0m
[0m2021.07.12 13:17:51 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 13:17:51 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.12 13:17:51 INFO  Trying to attach to remote debuggee VM localhost:58113 .[0m
[0m2021.07.12 13:17:51 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 13:17:52 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.12 13:17:52 ERROR 21/07/12 13:17:52 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.149.194 instead (on interface eth0)[0m
[0m2021.07.12 13:17:52 ERROR 21/07/12 13:17:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.12 13:17:53 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.12 13:17:53 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.12 13:17:53 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.12 13:17:53 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.12 13:17:53 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.12 13:17:53 ERROR 21/07/12 13:17:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.12 13:17:59 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.12 13:18:02 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.12 13:18:04 ERROR 21/07/12 13:18:04 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.12 13:18:10 INFO  +--------------------+[0m
[0m2021.07.12 13:18:10 INFO  |     features_scaled|[0m
[0m2021.07.12 13:18:10 INFO  +--------------------+[0m
[0m2021.07.12 13:18:10 INFO  |[1.74656375425114...|[0m
[0m2021.07.12 13:18:10 INFO  |[1.35356662711564...|[0m
[0m2021.07.12 13:18:10 INFO  |[8.51792129331685...|[0m
[0m2021.07.12 13:18:10 INFO  |[4.38614234402984...|[0m
[0m2021.07.12 13:18:10 INFO  |[1.84050811607748...|[0m
[0m2021.07.12 13:18:10 INFO  |[5.25859461863939...|[0m
[0m2021.07.12 13:18:10 INFO  |[1.75286487176416...|[0m
[0m2021.07.12 13:18:10 INFO  |[8.76432436512448...|[0m
[0m2021.07.12 13:18:10 INFO  |[0.0,0.0,5.488506...|[0m
[0m2021.07.12 13:18:10 INFO  |[8.73242970041492...|[0m
[0m2021.07.12 13:18:10 INFO  +--------------------+[0m
[0m2021.07.12 13:18:10 INFO  only showing top 10 rows[0m
[0m2021.07.12 13:18:10 INFO  [0m
[0m2021.07.12 13:18:11 ERROR 21/07/12 13:18:11 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.12 13:18:11 ERROR 21/07/12 13:18:11 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.12 13:18:16 ERROR 21/07/12 13:18:16 INFO OWLQN: Converged because gradient converged[0m
[0m2021.07.12 13:18:21 INFO  Coefficients: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] Intercept: 0.18015161191791854[0m
[0m2021.07.12 13:18:21 INFO  numIterations: 0[0m
[0m2021.07.12 13:18:21 INFO  objectiveHistory: [0.5000000000000001][0m
[0m2021.07.12 13:18:21 INFO  +--------------------+[0m
[0m2021.07.12 13:18:21 INFO  |           residuals|[0m
[0m2021.07.12 13:18:21 INFO  +--------------------+[0m
[0m2021.07.12 13:18:21 INFO  |  0.8198483880820815|[0m
[0m2021.07.12 13:18:21 INFO  |-0.18015161191791854|[0m
[0m2021.07.12 13:18:21 INFO  |-0.18015161191791854|[0m
[0m2021.07.12 13:18:21 INFO  |  0.8198483880820815|[0m
[0m2021.07.12 13:18:21 INFO  |-0.18015161191791854|[0m
[0m2021.07.12 13:18:21 INFO  |-0.18015161191791854|[0m
[0m2021.07.12 13:18:21 INFO  |  0.8198483880820815|[0m
[0m2021.07.12 13:18:21 INFO  |-0.18015161191791854|[0m
[0m2021.07.12 13:18:21 INFO  |-0.18015161191791854|[0m
[0m2021.07.12 13:18:21 INFO  |-0.18015161191791854|[0m
[0m2021.07.12 13:18:21 INFO  |-0.18015161191791854|[0m
[0m2021.07.12 13:18:21 INFO  |-0.18015161191791854|[0m
[0m2021.07.12 13:18:21 INFO  |-0.18015161191791854|[0m
[0m2021.07.12 13:18:21 INFO  |-0.18015161191791854|[0m
[0m2021.07.12 13:18:21 INFO  |-0.18015161191791854|[0m
[0m2021.07.12 13:18:21 INFO  |-0.18015161191791854|[0m
[0m2021.07.12 13:18:21 INFO  |-0.18015161191791854|[0m
[0m2021.07.12 13:18:21 INFO  |-0.18015161191791854|[0m
[0m2021.07.12 13:18:21 INFO  |-0.18015161191791854|[0m
[0m2021.07.12 13:18:21 INFO  |-0.18015161191791854|[0m
[0m2021.07.12 13:18:21 INFO  +--------------------+[0m
[0m2021.07.12 13:18:21 INFO  only showing top 20 rows[0m
[0m2021.07.12 13:18:21 INFO  [0m
[0m2021.07.12 13:18:21 INFO  RMSE: 0.38431368521231757[0m
[0m2021.07.12 13:18:21 INFO  r2: -1.205924249347845E-12[0m
[0m2021.07.12 13:18:22 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.12 13:19:47 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:19:47 INFO  time: compiled ht3 in 0.47s[0m
[0m2021.07.12 13:21:00 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:21:00 INFO  time: compiled ht3 in 0.41s[0m
Exception in thread "pool-3-thread-1" java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[0m2021.07.12 13:21:15 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:21:17 INFO  time: compiled ht3 in 1.65s[0m
[0m2021.07.12 13:21:24 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 23m 9.641s)[0m
[0m2021.07.12 13:21:24 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 13:21:24 INFO  time: compiled ht3-test in 0.23s[0m
[0m2021.07.12 13:21:25 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 13:21:24 INFO  Listening for transport dt_socket at address: 58571[0m
[0m2021.07.12 13:21:25 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 13:21:25 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.12 13:21:24 INFO  Trying to attach to remote debuggee VM localhost:58571 .[0m
[0m2021.07.12 13:21:24 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 13:21:26 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.12 13:21:26 ERROR 21/07/12 13:21:26 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.149.194 instead (on interface eth0)[0m
[0m2021.07.12 13:21:26 ERROR 21/07/12 13:21:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.12 13:21:27 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.12 13:21:27 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.12 13:21:27 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.12 13:21:27 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.12 13:21:27 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.12 13:21:27 ERROR 21/07/12 13:21:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.12 13:21:36 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.12 13:21:37 ERROR 21/07/12 13:21:37 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.12 13:21:43 INFO  +--------------------+[0m
[0m2021.07.12 13:21:43 INFO  |     features_scaled|[0m
[0m2021.07.12 13:21:43 INFO  +--------------------+[0m
[0m2021.07.12 13:21:43 INFO  |[1.74656375425114...|[0m
[0m2021.07.12 13:21:43 INFO  |[1.35356662711564...|[0m
[0m2021.07.12 13:21:43 INFO  |[8.51792129331685...|[0m
[0m2021.07.12 13:21:43 INFO  |[4.38614234402984...|[0m
[0m2021.07.12 13:21:43 INFO  |[1.84050811607748...|[0m
[0m2021.07.12 13:21:43 INFO  |[5.25859461863939...|[0m
[0m2021.07.12 13:21:43 INFO  |[1.75286487176416...|[0m
[0m2021.07.12 13:21:43 INFO  |[8.76432436512448...|[0m
[0m2021.07.12 13:21:43 INFO  |[0.0,0.0,5.488506...|[0m
[0m2021.07.12 13:21:43 INFO  |[8.73242970041492...|[0m
[0m2021.07.12 13:21:43 INFO  +--------------------+[0m
[0m2021.07.12 13:21:43 INFO  only showing top 10 rows[0m
[0m2021.07.12 13:21:43 INFO  [0m
[0m2021.07.12 13:21:54 ERROR 21/07/12 13:21:54 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.12 13:21:54 ERROR 21/07/12 13:21:54 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.12 13:21:54 ERROR 21/07/12 13:21:55 INFO OWLQN: Converged because gradient converged[0m
[0m2021.07.12 13:21:54 INFO  Coefficients: (151,[],[]) Intercept: -1.5153206457834802[0m
[0m2021.07.12 13:21:54 INFO  numIterations: 0[0m
[0m2021.07.12 13:21:54 INFO  objectiveHistory: [0.47162330531180735][0m
[0m2021.07.12 13:22:00 INFO  accuracy: 0.8198483880820815[0m
[0m2021.07.12 13:22:00 INFO  precision: [D@87ef6de[0m
[0m2021.07.12 13:22:00 INFO  recall: [D@249b2dd4[0m
[0m2021.07.12 13:25:35 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.12 13:25:59 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 27m 44.845s)[0m
[0m2021.07.12 13:25:59 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:25:59 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:25:59 INFO  time: compiled ht3 in 0.45s[0m
[0m2021.07.12 13:25:59 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:25:59 INFO  time: compiled ht3 in 0.36s[0m
Jul 12, 2021 1:26:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
java.util.concurrent.CompletionException: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:29)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:26)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:237)
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:229)
	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417)
	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
	... 4 more

Jul 12, 2021 1:26:21 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 620
[0m2021.07.12 13:26:47 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:26:49 INFO  time: compiled ht3 in 1.55s[0m
[0m2021.07.12 13:28:37 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:28:37 INFO  time: compiled ht3 in 0.35s[0m
[0m2021.07.12 13:28:44 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:28:45 INFO  time: compiled ht3 in 1.58s[0m
[0m2021.07.12 13:28:48 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 30m 33.379s)[0m
[0m2021.07.12 13:28:48 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 13:28:48 INFO  time: compiled ht3-test in 0.24s[0m
[0m2021.07.12 13:28:48 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 13:28:48 INFO  Listening for transport dt_socket at address: 41195[0m
[0m2021.07.12 13:28:48 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 13:28:49 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.12 13:28:48 INFO  Trying to attach to remote debuggee VM localhost:41195 .[0m
[0m2021.07.12 13:28:48 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 13:28:50 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.12 13:28:50 ERROR 21/07/12 13:28:50 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.149.194 instead (on interface eth0)[0m
[0m2021.07.12 13:28:50 ERROR 21/07/12 13:28:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.12 13:28:51 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.12 13:28:51 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.12 13:28:51 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.12 13:28:51 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.12 13:28:51 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.12 13:28:51 ERROR 21/07/12 13:28:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.12 13:29:00 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.12 13:29:02 ERROR 21/07/12 13:29:02 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.12 13:29:08 INFO  +--------------------+[0m
[0m2021.07.12 13:29:08 INFO  |     features_scaled|[0m
[0m2021.07.12 13:29:08 INFO  +--------------------+[0m
[0m2021.07.12 13:29:08 INFO  |[1.74656375425114...|[0m
[0m2021.07.12 13:29:08 INFO  |[1.35356662711564...|[0m
[0m2021.07.12 13:29:08 INFO  |[8.51792129331685...|[0m
[0m2021.07.12 13:29:08 INFO  |[4.38614234402984...|[0m
[0m2021.07.12 13:29:08 INFO  |[1.84050811607748...|[0m
[0m2021.07.12 13:29:08 INFO  |[5.25859461863939...|[0m
[0m2021.07.12 13:29:08 INFO  |[1.75286487176416...|[0m
[0m2021.07.12 13:29:08 INFO  |[8.76432436512448...|[0m
[0m2021.07.12 13:29:08 INFO  |[0.0,0.0,5.488506...|[0m
[0m2021.07.12 13:29:08 INFO  |[8.73242970041492...|[0m
[0m2021.07.12 13:29:08 INFO  +--------------------+[0m
[0m2021.07.12 13:29:08 INFO  only showing top 10 rows[0m
[0m2021.07.12 13:29:08 INFO  [0m
[0m2021.07.12 13:29:18 ERROR 21/07/12 13:29:18 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.12 13:29:18 ERROR 21/07/12 13:29:18 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.12 13:29:18 ERROR 21/07/12 13:29:19 INFO OWLQN: Converged because gradient converged[0m
[0m2021.07.12 13:29:19 INFO  Coefficients: 1 x 151 CSCMatrix Intercept: [-1.5153206457834802][0m
[0m2021.07.12 13:29:19 INFO  numIterations: 0[0m
[0m2021.07.12 13:29:19 INFO  objectiveHistory: [0.47162330531180724][0m
[0m2021.07.12 13:29:24 INFO  accuracy: 0.8198483880820815[0m
[0m2021.07.12 13:29:24 INFO  precision: 0.8198483880820815,0.0[0m
[0m2021.07.12 13:29:24 INFO  recall: 1.0,0.0[0m
[0m2021.07.12 13:29:24 INFO  F1: 0.9010073514377874,0.0[0m
[0m2021.07.12 13:29:24 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.12 13:30:45 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 32m 30.229s)[0m
[0m2021.07.12 13:30:45 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:30:45 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:30:46 INFO  time: compiled ht3 in 1.54s[0m
[0m2021.07.12 13:30:46 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 13:30:46 INFO  time: compiled ht3-test in 0.42s[0m
[0m2021.07.12 13:30:47 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 13:30:46 INFO  Listening for transport dt_socket at address: 38123[0m
[0m2021.07.12 13:30:47 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 13:30:47 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.12 13:30:46 INFO  Trying to attach to remote debuggee VM localhost:38123 .[0m
[0m2021.07.12 13:30:46 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 13:30:48 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.12 13:30:48 ERROR 21/07/12 13:30:48 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.149.194 instead (on interface eth0)[0m
[0m2021.07.12 13:30:48 ERROR 21/07/12 13:30:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.12 13:30:49 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.12 13:30:49 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.12 13:30:49 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.12 13:30:49 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.12 13:30:49 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.12 13:30:49 ERROR 21/07/12 13:30:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.12 13:30:58 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.12 13:31:00 ERROR 21/07/12 13:31:00 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.12 13:31:06 INFO  +--------------------+[0m
[0m2021.07.12 13:31:06 INFO  |     features_scaled|[0m
[0m2021.07.12 13:31:06 INFO  +--------------------+[0m
[0m2021.07.12 13:31:06 INFO  |[1.74656375425114...|[0m
[0m2021.07.12 13:31:06 INFO  |[1.35356662711564...|[0m
[0m2021.07.12 13:31:06 INFO  |[8.51792129331685...|[0m
[0m2021.07.12 13:31:06 INFO  |[4.38614234402984...|[0m
[0m2021.07.12 13:31:06 INFO  |[1.84050811607748...|[0m
[0m2021.07.12 13:31:06 INFO  |[5.25859461863939...|[0m
[0m2021.07.12 13:31:06 INFO  |[1.75286487176416...|[0m
[0m2021.07.12 13:31:06 INFO  |[8.76432436512448...|[0m
[0m2021.07.12 13:31:06 INFO  |[0.0,0.0,5.488506...|[0m
[0m2021.07.12 13:31:06 INFO  |[8.73242970041492...|[0m
[0m2021.07.12 13:31:06 INFO  +--------------------+[0m
[0m2021.07.12 13:31:06 INFO  only showing top 10 rows[0m
[0m2021.07.12 13:31:06 INFO  [0m
[0m2021.07.12 13:31:18 ERROR 21/07/12 13:31:18 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.12 13:31:18 ERROR 21/07/12 13:31:18 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.12 13:31:18 ERROR 21/07/12 13:31:18 INFO OWLQN: Converged because gradient converged[0m
[0m2021.07.12 13:31:18 INFO  Coefficients: (151,[],[]) Intercept: [-1.5153206457834802][0m
[0m2021.07.12 13:31:18 INFO  numIterations: 0[0m
[0m2021.07.12 13:31:18 INFO  objectiveHistory: [0.47162330531180724][0m
[0m2021.07.12 13:31:23 INFO  accuracy: 0.8198483880820815[0m
[0m2021.07.12 13:31:23 INFO  precision: 0.8198483880820815,0.0[0m
[0m2021.07.12 13:31:23 INFO  recall: 1.0,0.0[0m
[0m2021.07.12 13:31:23 INFO  F1: 0.9010073514377874,0.0[0m
[0m2021.07.12 13:31:23 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.12 13:31:56 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:31:58 INFO  time: compiled ht3 in 1.46s[0m
[0m2021.07.12 13:32:02 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 33m 47.245s)[0m
[0m2021.07.12 13:32:02 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 13:32:02 INFO  time: compiled ht3-test in 0.23s[0m
[0m2021.07.12 13:32:02 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 13:32:02 INFO  Listening for transport dt_socket at address: 50271[0m
[0m2021.07.12 13:32:02 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 13:32:02 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.12 13:32:02 INFO  Trying to attach to remote debuggee VM localhost:50271 .[0m
[0m2021.07.12 13:32:02 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 13:32:03 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.12 13:32:03 ERROR 21/07/12 13:32:03 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.149.194 instead (on interface eth0)[0m
[0m2021.07.12 13:32:03 ERROR 21/07/12 13:32:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.12 13:32:04 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.12 13:32:04 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.12 13:32:04 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.12 13:32:04 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.12 13:32:04 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.12 13:32:04 ERROR 21/07/12 13:32:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.12 13:32:13 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.12 13:32:15 ERROR 21/07/12 13:32:15 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.12 13:32:21 INFO  +--------------------+[0m
[0m2021.07.12 13:32:21 INFO  |     features_scaled|[0m
[0m2021.07.12 13:32:21 INFO  +--------------------+[0m
[0m2021.07.12 13:32:21 INFO  |[1.74656375425114...|[0m
[0m2021.07.12 13:32:21 INFO  |[1.35356662711564...|[0m
[0m2021.07.12 13:32:21 INFO  |[8.51792129331685...|[0m
[0m2021.07.12 13:32:21 INFO  |[4.38614234402984...|[0m
[0m2021.07.12 13:32:21 INFO  |[1.84050811607748...|[0m
[0m2021.07.12 13:32:21 INFO  |[5.25859461863939...|[0m
[0m2021.07.12 13:32:21 INFO  |[1.75286487176416...|[0m
[0m2021.07.12 13:32:21 INFO  |[8.76432436512448...|[0m
[0m2021.07.12 13:32:21 INFO  |[0.0,0.0,5.488506...|[0m
[0m2021.07.12 13:32:21 INFO  |[8.73242970041492...|[0m
[0m2021.07.12 13:32:21 INFO  +--------------------+[0m
[0m2021.07.12 13:32:21 INFO  only showing top 10 rows[0m
[0m2021.07.12 13:32:21 INFO  [0m
[0m2021.07.12 13:32:33 ERROR 21/07/12 13:32:33 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.12 13:32:33 ERROR 21/07/12 13:32:33 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.12 13:32:33 ERROR 21/07/12 13:32:34 INFO StrongWolfeLineSearch: Line search t: 1.551353572307765 fval: 0.4728615278966894 rhs: 0.47162266071342696 cdd: 0.005646827936823737[0m
[0m2021.07.12 13:32:33 ERROR 21/07/12 13:32:34 INFO StrongWolfeLineSearch: Line search t: 0.6455642138422915 fval: 0.47028617384062793 rhs: 0.471623037075311 cdd: -2.564024485217928E-7[0m
[0m2021.07.12 13:32:33 ERROR 21/07/12 13:32:34 INFO LBFGS: Step Size: 0.6456[0m
[0m2021.07.12 13:32:33 ERROR 21/07/12 13:32:34 INFO LBFGS: Val and Grad Norm: 0.470286 (rel: 0.00284) 0.187414[0m
[0m2021.07.12 13:32:34 ERROR 21/07/12 13:32:34 INFO StrongWolfeLineSearch: Line search t: 0.337495658669837 fval: 0.46987907630382264 rhs: 0.47028609288233264 cdd: -5.043074262170895E-8[0m
[0m2021.07.12 13:32:34 ERROR 21/07/12 13:32:34 INFO LBFGS: Step Size: 0.3375[0m
[0m2021.07.12 13:32:34 ERROR 21/07/12 13:32:34 INFO LBFGS: Val and Grad Norm: 0.469879 (rel: 0.000866) 0.00860304[0m
[0m2021.07.12 13:32:34 ERROR 21/07/12 13:32:34 INFO LBFGS: Step Size: 2.250[0m
[0m2021.07.12 13:32:34 ERROR 21/07/12 13:32:34 INFO LBFGS: Val and Grad Norm: 0.469875 (rel: 7.63e-06) 0.00751953[0m
[0m2021.07.12 13:32:34 ERROR 21/07/12 13:32:34 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:34 ERROR 21/07/12 13:32:34 INFO LBFGS: Val and Grad Norm: 0.469862 (rel: 2.79e-05) 0.00274100[0m
[0m2021.07.12 13:32:34 ERROR 21/07/12 13:32:35 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:34 ERROR 21/07/12 13:32:35 INFO LBFGS: Val and Grad Norm: 0.469861 (rel: 3.48e-06) 0.00126627[0m
[0m2021.07.12 13:32:34 ERROR 21/07/12 13:32:35 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:34 ERROR 21/07/12 13:32:35 INFO LBFGS: Val and Grad Norm: 0.469860 (rel: 5.43e-07) 0.000160388[0m
[0m2021.07.12 13:32:34 ERROR 21/07/12 13:32:35 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:34 ERROR 21/07/12 13:32:35 INFO LBFGS: Val and Grad Norm: 0.469860 (rel: 1.48e-08) 0.000142334[0m
[0m2021.07.12 13:32:34 ERROR 21/07/12 13:32:35 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:34 ERROR 21/07/12 13:32:35 INFO LBFGS: Val and Grad Norm: 0.469860 (rel: 6.87e-08) 0.000201216[0m
[0m2021.07.12 13:32:34 ERROR 21/07/12 13:32:35 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:34 ERROR 21/07/12 13:32:35 INFO LBFGS: Val and Grad Norm: 0.469860 (rel: 1.04e-07) 0.000328506[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:35 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:35 INFO LBFGS: Val and Grad Norm: 0.469860 (rel: 3.95e-07) 0.000665135[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:35 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:35 INFO LBFGS: Val and Grad Norm: 0.469860 (rel: 7.66e-07) 0.00103751[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:35 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:35 INFO LBFGS: Val and Grad Norm: 0.469859 (rel: 1.41e-06) 0.00135489[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:35 INFO StrongWolfeLineSearch: Line search t: 0.1 fval: 0.46985915534370204 rhs: 0.4698591878278798 cdd: 1.242385304685118E-6[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:35 INFO LBFGS: Step Size: 0.1000[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:35 INFO LBFGS: Val and Grad Norm: 0.469859 (rel: 6.92e-08) 0.00380528[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:36 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:36 INFO LBFGS: Val and Grad Norm: 0.469859 (rel: 1.27e-06) 0.00128044[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:36 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:36 INFO LBFGS: Val and Grad Norm: 0.469858 (rel: 7.55e-07) 0.000960725[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:36 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:36 INFO LBFGS: Val and Grad Norm: 0.469858 (rel: 2.88e-07) 0.000643057[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:36 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:36 INFO LBFGS: Val and Grad Norm: 0.469858 (rel: 1.56e-07) 0.000157732[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:36 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:36 INFO LBFGS: Val and Grad Norm: 0.469858 (rel: 2.94e-09) 5.74468e-05[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:36 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:36 INFO LBFGS: Val and Grad Norm: 0.469858 (rel: 9.19e-10) 1.56290e-05[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:36 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:35 ERROR 21/07/12 13:32:36 INFO LBFGS: Val and Grad Norm: 0.469858 (rel: 8.73e-11) 6.35078e-06[0m
[0m2021.07.12 13:32:36 ERROR 21/07/12 13:32:36 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:36 ERROR 21/07/12 13:32:36 INFO LBFGS: Val and Grad Norm: 0.469858 (rel: 1.88e-11) 8.44529e-07[0m
[0m2021.07.12 13:32:36 ERROR 21/07/12 13:32:36 INFO LBFGS: Step Size: 1.000[0m
[0m2021.07.12 13:32:36 ERROR 21/07/12 13:32:36 INFO LBFGS: Val and Grad Norm: 0.469858 (rel: 4.26e-13) 4.49720e-07[0m
[0m2021.07.12 13:32:36 ERROR 21/07/12 13:32:36 INFO LBFGS: Converged because gradient converged[0m
[0m2021.07.12 13:32:36 INFO  Coefficients: (151,[0,1,2,3,4,5,6,7,8,26,27,28,29,30,31,32,33,34,73,74,75,76,77,78,79,80,81,82,83,84,85,86,88,89,90,91,92,93,94,95,96,97,99,101,102,103,104,105,106,107,108,109,113,116,117,118,119,120,121,122,123,128,129,131,139,141,144,145,146,147,148,149,150],[-0.2544414807756188,-0.26057914810071486,-0.015906409902065995,0.2786541370861337,-0.08721180521632468,-0.0843648722800149,-0.06795598097844378,-0.5530333408434841,-1.015130680366455,-0.26979823397819447,-0.24937194168958945,-0.14958811046015127,0.3626807520267445,-0.045672560012033264,3.320387301987919,-0.7482192149774465,-0.8470052530113743,-0.7711082211372707,0.003983929381182148,0.12718944453075595,0.00730143742658551,-0.08584796997510066,-1.320056364192725,0.00555194848855368,-0.08989121692514787,-0.1753641134582955,0.028193904650560125,-0.09096366051536976,0.005788169221350111,-0.03350275777805448,-0.045444260997496704,0.0268748030641775,0.2402043659166343,0.12513458655379284,0.07486977801905874,0.1228958013940877,-0.1295917222347525,0.12752931108418353,-0.2265142379761164,-0.2594261128206312,-0.2333981101694427,-0.2333929937940369,-0.1041274133915485,0.13757199936225822,-0.22390464241850472,-0.23739432647807993,0.3669207312104844,-0.16707166126989198,-0.18389064692194027,-0.1548998267075719,-0.17099925362145463,-0.12932328186014744,-0.0910184567830802,-0.14110582130892457,0.19496383268959386,-0.11611167085500682,0.14304810989320907,0.009959170294930651,-0.13323832580084516,0.11851252938430966,-0.025520911934689812,-0.02410643644059763,-0.14479140207977445,-0.12907820924204763,-0.1326312755100694,-0.04156411757440379,-0.19553401824016015,0.09528034203250672,0.1822853325064906,-0.1677298313911685,0.15213604453145999,-0.03733764085841287,-0.16758575555374716]) Intercept: [-1.4754681317227256][0m
[0m2021.07.12 13:32:36 INFO  numIterations: 22[0m
[0m2021.07.12 13:32:36 INFO  objectiveHistory: [0.47162330531180724,0.47028617384062793,0.46987907630382264,0.4698754913474785,0.4698623732654658,0.46986073999281963,0.4698604849574742,0.4698604779899286,0.4698604457001706,0.4698603969901865,0.46986021121541804,0.4698598514167776,0.46985918784679964,0.46985915534370204,0.46985855930945275,0.46985820463245415,0.4698580691050594,0.4698579956794385,0.4698579942962457,0.4698579938645739,0.4698579938235479,0.46985799381469606,0.4698579938144957][0m
[0m2021.07.12 13:32:41 INFO  accuracy: 0.8198872629328298[0m
[0m2021.07.12 13:32:41 INFO  precision: 0.8198820373548377,0.9375[0m
[0m2021.07.12 13:32:41 INFO  recall: 0.9999966130627397,2.3120318135577545E-4[0m
[0m2021.07.12 13:32:41 INFO  F1: 0.9010262967563164,4.6229235368447006E-4[0m
[0m2021.07.12 13:33:22 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 13:33:22 INFO  time: compiled ht3-test in 0.22s[0m
[0m2021.07.12 13:33:57 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 13:33:58 INFO  time: compiled ht3 in 1.38s[0m
[0m2021.07.12 13:34:00 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 35m 45.661s)[0m
[0m2021.07.12 13:34:00 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 13:34:00 INFO  time: compiled ht3-test in 0.19s[0m
[0m2021.07.12 13:34:01 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 13:34:00 INFO  Listening for transport dt_socket at address: 55163[0m
[0m2021.07.12 13:34:01 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 13:34:01 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.12 13:34:00 INFO  Trying to attach to remote debuggee VM localhost:55163 .[0m
[0m2021.07.12 13:34:00 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 13:34:02 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.12 13:34:02 ERROR 21/07/12 13:34:02 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.149.194 instead (on interface eth0)[0m
[0m2021.07.12 13:34:02 ERROR 21/07/12 13:34:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.12 13:34:03 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.12 13:34:03 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.12 13:34:03 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.12 13:34:03 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.12 13:34:03 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.12 13:34:03 ERROR 21/07/12 13:34:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.12 13:34:11 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.12 13:34:13 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.12 13:34:13 ERROR 21/07/12 13:34:13 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.12 13:34:20 INFO  +--------------------+[0m
[0m2021.07.12 13:34:20 INFO  |     features_scaled|[0m
[0m2021.07.12 13:34:20 INFO  +--------------------+[0m
[0m2021.07.12 13:34:20 INFO  |[1.74656375425114...|[0m
[0m2021.07.12 13:34:20 INFO  |[1.35356662711564...|[0m
[0m2021.07.12 13:34:20 INFO  |[8.51792129331685...|[0m
[0m2021.07.12 13:34:20 INFO  |[4.38614234402984...|[0m
[0m2021.07.12 13:34:20 INFO  |[1.84050811607748...|[0m
[0m2021.07.12 13:34:20 INFO  |[5.25859461863939...|[0m
[0m2021.07.12 13:34:20 INFO  |[1.75286487176416...|[0m
[0m2021.07.12 13:34:20 INFO  |[8.76432436512448...|[0m
[0m2021.07.12 13:34:20 INFO  |[0.0,0.0,5.488506...|[0m
[0m2021.07.12 13:34:20 INFO  |[8.73242970041492...|[0m
[0m2021.07.12 13:34:20 INFO  +--------------------+[0m
[0m2021.07.12 13:34:20 INFO  only showing top 10 rows[0m
[0m2021.07.12 13:34:20 INFO  [0m
[0m2021.07.12 13:34:21 ERROR 21/07/12 13:34:21 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.12 13:34:21 ERROR 21/07/12 13:34:21 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.12 13:34:27 ERROR 21/07/12 13:34:27 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.12 13:34:27 ERROR 21/07/12 13:34:27 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.12 13:34:32 INFO  Coefficients: [-0.08494333369123494,-0.0787028862985535,-0.021570247267194774,0.08965643706137838,-0.035260724414880686,-0.03336139972936502,-0.01667187600566854,-0.15260668515265638,-0.3185736179068389,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0955893820479998,-0.14672406082655126,-0.034133488092262555,0.08438951967142035,0.03526034191501986,1.0895634148344209,-0.24220644742235023,-0.25522186644950595,-0.19910163699496827,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007625877204562843,0.03885050030846868,0.003518047860299327,-0.02332391185123668,-0.31396345261313885,0.0016032894950289327,-0.024732340111843595,-0.058613628313616814,0.006058484508710727,-0.025077897038130955,-4.3401426296823457E-4,-0.01085339884048516,-0.013356369039273015,0.006236852717879654,0.0,0.07533663900727695,0.03397596232213763,0.012274691545147091,0.029475721299301098,-0.03293049182097022,0.031025026197432134,-0.06050419410357969,-0.06886601997591603,-0.06387160802234054,-0.0638695832127087,0.0,-0.02598364726605634,0.0,0.03571626956436442,-0.06009332048235471,-0.06407588432349959,0.11024710463308277,-0.05376798930811628,-0.051415616962792114,-0.03524156000098765,-0.04902333947553933,-0.040221330315642796,0.0,0.0,0.0,-0.027380906399337074,0.0,0.0,-0.03759571745164566,0.056405149579805636,-0.029232184115919022,0.04112015317071111,0.0026661050903382224,-0.03790695595226767,0.03916513524780699,-0.010263552001773832,0.0,0.0,0.0,0.0,-0.010651558419666148,-0.04451798828251661,0.0,-0.04195437233014068,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.035525707781619086,0.0,-0.01087194993938892,0.0,0.0,-0.05461581121048458,0.021380176973030362,0.04628018175973996,-0.05397957260138357,0.03912435357781949,-0.017333379686025597,-0.05039454435934958] Intercept: 0.18467473029955964[0m
[0m2021.07.12 13:34:32 INFO  numIterations: 0[0m
[0m2021.07.12 13:34:32 INFO  objectiveHistory: [0.0][0m
[0m2021.07.12 13:34:32 INFO  +--------------------+[0m
[0m2021.07.12 13:34:32 INFO  |           residuals|[0m
[0m2021.07.12 13:34:32 INFO  +--------------------+[0m
[0m2021.07.12 13:34:32 INFO  |  0.8069982679672076|[0m
[0m2021.07.12 13:34:32 INFO  |-0.12503977531963442|[0m
[0m2021.07.12 13:34:32 INFO  |-0.18301090581785545|[0m
[0m2021.07.12 13:34:32 INFO  |  0.8338566527599085|[0m
[0m2021.07.12 13:34:32 INFO  |-0.17615325197852633|[0m
[0m2021.07.12 13:34:32 INFO  | -0.1761533581606129|[0m
[0m2021.07.12 13:34:32 INFO  |  0.8238464858929835|[0m
[0m2021.07.12 13:34:32 INFO  |-0.17664323261640788|[0m
[0m2021.07.12 13:34:32 INFO  | -0.1727434467879862|[0m
[0m2021.07.12 13:34:32 INFO  | -0.1709386295202866|[0m
[0m2021.07.12 13:34:32 INFO  |-0.17733782059797232|[0m
[0m2021.07.12 13:34:32 INFO  |-0.13816377312914324|[0m
[0m2021.07.12 13:34:32 INFO  |-0.16954457471850476|[0m
[0m2021.07.12 13:34:32 INFO  |-0.15708238885562326|[0m
[0m2021.07.12 13:34:32 INFO  |-0.18519647255900118|[0m
[0m2021.07.12 13:34:32 INFO  |-0.19930421536598397|[0m
[0m2021.07.12 13:34:32 INFO  |-0.17474346586918982|[0m
[0m2021.07.12 13:34:32 INFO  |-0.17644616309908215|[0m
[0m2021.07.12 13:34:32 INFO  |-0.21226368332229328|[0m
[0m2021.07.12 13:34:32 INFO  |-0.15650672115526026|[0m
[0m2021.07.12 13:34:32 INFO  +--------------------+[0m
[0m2021.07.12 13:34:32 INFO  only showing top 20 rows[0m
[0m2021.07.12 13:34:32 INFO  [0m
[0m2021.07.12 13:34:32 INFO  RMSE: 0.3817728207315983[0m
[0m2021.07.12 13:34:32 INFO  r2: 0.013179156503166256[0m
[0m2021.07.12 13:34:32 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
Jul 12, 2021 5:11:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1132
Jul 12, 2021 5:14:50 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1141
Jul 12, 2021 5:14:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1148
Jul 12, 2021 5:15:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1154
[0m2021.07.12 17:15:02 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 4h 16m 47.149s)[0m
[0m2021.07.12 17:15:02 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 17:15:04 INFO  time: compiled ht3-test in 1.32s[0m
[0m2021.07.12 17:15:04 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 17:15:04 INFO  Listening for transport dt_socket at address: 59489[0m
[0m2021.07.12 17:15:04 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 17:15:04 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 17:15:04 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.12 17:15:04 INFO  Trying to attach to remote debuggee VM localhost:59489 .[0m
[0m2021.07.12 17:15:04 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 17:15:05 INFO  Listening for transport dt_socket at address: 38529[0m
[0m2021.07.12 17:15:05 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.12 17:15:06 INFO  time: compiled ht3 in 2.6s[0m
[0m2021.07.12 17:15:11 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 4h 16m 56.834s)[0m
[0m2021.07.12 17:15:11 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 17:15:11 INFO  time: compiled ht3-test in 0.26s[0m
[0m2021.07.12 17:15:12 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 17:15:11 INFO  Listening for transport dt_socket at address: 52107[0m
[0m2021.07.12 17:15:12 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 17:15:12 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.12 17:15:11 INFO  Trying to attach to remote debuggee VM localhost:52107 .[0m
[0m2021.07.12 17:15:11 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 17:15:14 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.12 17:15:14 ERROR 21/07/12 17:15:14 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.149.194 instead (on interface eth0)[0m
[0m2021.07.12 17:15:14 ERROR 21/07/12 17:15:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.12 17:15:15 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.12 17:15:15 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.12 17:15:15 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.12 17:15:15 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.12 17:15:15 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.12 17:15:15 ERROR 21/07/12 17:15:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.12 17:15:24 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.12 17:15:28 INFO  Pushed Record for 2018-02-01 03:00:00: [{"metadata_ownerId":49367,"date":"2018-02-01","createdTime":"2018-02-01 00:25:14","auditedTime":"2018-02-01 03:00:00","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Clicked","Unliked","Liked"],"target":1},{"metadata_ownerId":76851,"date":"2018-02-01","createdTime":"2018-01-30 10:30:14","auditedTime":"2018-02-01 03:00:00","timeDelta":1,"createdHour":10,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":19357,"date":"2018-02-01","createdTime":"2018-02-01 02:36:21","auditedTime":"2018-02-01 03:00:00","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 17:15:30 INFO  Pushed Record for 2018-02-01 03:00:01: [{"metadata_ownerId":77159,"date":"2018-02-01","createdTime":"2018-02-01 01:50:18","auditedTime":"2018-02-01 03:00:01","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 17:15:32 INFO  Pushed Record for 2018-02-01 03:00:02: [{"metadata_ownerId":25161,"date":"2018-02-01","createdTime":"2018-02-01 00:03:09","auditedTime":"2018-02-01 03:00:02","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":80203,"date":"2018-02-01","createdTime":"2018-01-29 20:21:50","auditedTime":"2018-02-01 03:00:02","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":65675,"date":"2018-02-01","createdTime":"2018-01-23 15:15:04","auditedTime":"2018-02-01 03:00:02","timeDelta":8,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
Jul 12, 2021 5:15:33 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1171
[0m2021.07.12 17:15:34 INFO  Pushed Record for 2018-02-01 03:00:03: [{"metadata_ownerId":75062,"date":"2018-02-01","createdTime":"2018-02-01 01:00:21","auditedTime":"2018-02-01 03:00:03","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":68682,"date":"2018-02-01","createdTime":"2018-01-31 15:06:46","auditedTime":"2018-02-01 03:00:03","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":49570,"date":"2018-02-01","createdTime":"2018-01-31 13:00:48","auditedTime":"2018-02-01 03:00:03","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 17:15:36 INFO  Pushed Record for 2018-02-01 03:00:04: [{"metadata_ownerId":40612,"date":"2018-02-01","createdTime":"2018-02-01 02:50:09","auditedTime":"2018-02-01 03:00:04","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":27554,"date":"2018-02-01","createdTime":"2018-01-15 14:22:34","auditedTime":"2018-02-01 03:00:04","timeDelta":16,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 17:15:36 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 4h 17m 22.162s)[0m
[0m2021.07.12 17:15:36 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 17:15:36 INFO  time: compiled ht3-test in 0.22s[0m
[0m2021.07.12 17:15:37 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 17:15:36 INFO  Listening for transport dt_socket at address: 51165[0m
[0m2021.07.12 17:15:37 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 17:15:37 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.12 17:15:37 INFO  Trying to attach to remote debuggee VM localhost:51165 .[0m
[0m2021.07.12 17:15:37 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 17:15:37 ERROR Exception in thread "main" com.typesafe.config.ConfigException$Missing: application.json @ file:/home/hwait/dev/ht3/src/main/resources/application.json: 2: No configuration setting found for key 'spark.app_name'[0m
[0m2021.07.12 17:15:37 ERROR 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:157)[0m
[0m2021.07.12 17:15:37 ERROR 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:175)[0m
[0m2021.07.12 17:15:37 ERROR 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:181)[0m
[0m2021.07.12 17:15:37 ERROR 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189)[0m
[0m2021.07.12 17:15:37 ERROR 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:194)[0m
[0m2021.07.12 17:15:37 ERROR 	at com.typesafe.config.impl.SimpleConfig.getString(SimpleConfig.java:251)[0m
[0m2021.07.12 17:15:37 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka$.main(SparkMLRegressionFromKafka.scala:27)[0m
[0m2021.07.12 17:15:37 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka.main(SparkMLRegressionFromKafka.scala)[0m
[0m2021.07.12 17:15:37 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.12 17:15:37 INFO  Pushed Record for 2018-02-01 03:00:05: [{"metadata_ownerId":64597,"date":"2018-02-01","createdTime":"2018-01-31 06:00:05","auditedTime":"2018-02-01 03:00:05","timeDelta":0,"createdHour":6,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":33827,"date":"2018-02-01","createdTime":"2018-01-31 03:04:19","auditedTime":"2018-02-01 03:00:05","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 17:15:40 INFO  Pushed Record for 2018-02-01 03:00:06: [{"metadata_ownerId":24079,"date":"2018-02-01","createdTime":"2018-02-01 00:26:49","auditedTime":"2018-02-01 03:00:06","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":22580,"date":"2018-02-01","createdTime":"2018-01-31 20:01:09","auditedTime":"2018-02-01 03:00:06","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":35049,"date":"2018-02-01","createdTime":"2017-10-05 15:45:48","auditedTime":"2018-02-01 03:00:06","timeDelta":118,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":5862,"date":"2018-02-01","createdTime":"2018-01-30 19:36:22","auditedTime":"2018-02-01 03:00:06","timeDelta":1,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 17:15:42 INFO  Pushed Record for 2018-02-01 03:00:07: [{"metadata_ownerId":5049,"date":"2018-02-01","createdTime":"2018-01-31 22:16:22","auditedTime":"2018-02-01 03:00:07","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 17:15:44 INFO  Pushed Record for 2018-02-01 03:00:08: [{"metadata_ownerId":45299,"date":"2018-02-01","createdTime":"2018-02-01 01:01:15","auditedTime":"2018-02-01 03:00:08","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":80229,"date":"2018-02-01","createdTime":"2018-01-29 10:16:19","auditedTime":"2018-02-01 03:00:08","timeDelta":2,"createdHour":10,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":13290,"date":"2018-02-01","createdTime":"2018-01-31 02:02:07","auditedTime":"2018-02-01 03:00:08","timeDelta":1,"createdHour":2,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":1846,"date":"2018-02-01","createdTime":"2018-02-01 02:20:42","auditedTime":"2018-02-01 03:00:08","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Liked","Ignored"],"target":1}][0m
[0m2021.07.12 17:15:45 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 4h 17m 30.829s)[0m
[0m2021.07.12 17:15:45 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 17:15:45 INFO  time: compiled ht3-test in 0.32s[0m
[0m2021.07.12 17:15:46 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 17:15:45 INFO  Listening for transport dt_socket at address: 53639[0m
[0m2021.07.12 17:15:46 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 17:15:46 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.12 17:15:45 INFO  Trying to attach to remote debuggee VM localhost:53639 .[0m
[0m2021.07.12 17:15:45 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 17:15:46 INFO  Pushed Record for 2018-02-01 03:00:09: [{"metadata_ownerId":25161,"date":"2018-02-01","createdTime":"2018-02-01 00:03:09","auditedTime":"2018-02-01 03:00:09","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 17:15:46 ERROR Exception in thread "main" com.typesafe.config.ConfigException$Missing: application.json @ file:/home/hwait/dev/ht3/src/main/resources/application.json: 2: No configuration setting found for key 'spark.app_name'[0m
[0m2021.07.12 17:15:46 ERROR 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:157)[0m
[0m2021.07.12 17:15:46 ERROR 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:175)[0m
[0m2021.07.12 17:15:46 ERROR 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:181)[0m
[0m2021.07.12 17:15:46 ERROR 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189)[0m
[0m2021.07.12 17:15:46 ERROR 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:194)[0m
[0m2021.07.12 17:15:46 ERROR 	at com.typesafe.config.impl.SimpleConfig.getString(SimpleConfig.java:251)[0m
[0m2021.07.12 17:15:46 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka$.main(SparkMLRegressionFromKafka.scala:27)[0m
[0m2021.07.12 17:15:46 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka.main(SparkMLRegressionFromKafka.scala)[0m
[0m2021.07.12 17:15:47 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.12 17:15:48 INFO  Pushed Record for 2018-02-01 03:00:10: [{"metadata_ownerId":20606,"date":"2018-02-01","createdTime":"2018-01-31 13:10:42","auditedTime":"2018-02-01 03:00:10","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":39412,"date":"2018-02-01","createdTime":"2018-01-30 16:47:35","auditedTime":"2018-02-01 03:00:10","timeDelta":1,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 17:15:50 INFO  Pushed Record for 2018-02-01 03:00:11: [{"metadata_ownerId":80581,"date":"2018-02-01","createdTime":"2018-01-31 01:08:07","auditedTime":"2018-02-01 03:00:11","timeDelta":1,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":7195,"date":"2018-02-01","createdTime":"2018-01-29 17:22:03","auditedTime":"2018-02-01 03:00:11","timeDelta":2,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":52022,"date":"2018-02-01","createdTime":"2016-02-06 16:11:39","auditedTime":"2018-02-01 03:00:11","timeDelta":725,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 17:15:52 INFO  Pushed Record for 2018-02-01 03:00:12: [{"metadata_ownerId":15031,"date":"2018-02-01","createdTime":"2018-01-29 23:06:02","auditedTime":"2018-02-01 03:00:12","timeDelta":2,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":13917,"date":"2018-02-01","createdTime":"2018-01-31 15:36:44","auditedTime":"2018-02-01 03:00:12","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":36559,"date":"2018-02-01","createdTime":"2018-01-30 05:44:20","auditedTime":"2018-02-01 03:00:12","timeDelta":1,"createdHour":5,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 17:15:54 INFO  Pushed Record for 2018-02-01 03:00:13: [{"metadata_ownerId":24072,"date":"2018-02-01","createdTime":"2018-01-30 18:49:09","auditedTime":"2018-02-01 03:00:13","timeDelta":1,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":31314,"date":"2018-02-01","createdTime":"2018-01-29 04:19:24","auditedTime":"2018-02-01 03:00:13","timeDelta":2,"createdHour":4,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 17:15:56 INFO  Pushed Record for 2018-02-01 03:00:14: [{"metadata_ownerId":61558,"date":"2018-02-01","createdTime":"2018-01-31 21:00:22","auditedTime":"2018-02-01 03:00:14","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0}][0m
[0m2021.07.12 17:15:58 INFO  Pushed Record for 2018-02-01 03:00:15: [{"metadata_ownerId":11925,"date":"2018-02-01","createdTime":"2018-01-31 10:39:38","auditedTime":"2018-02-01 03:00:15","timeDelta":0,"createdHour":10,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":20251,"date":"2018-02-01","createdTime":"2018-02-01 00:52:18","auditedTime":"2018-02-01 03:00:15","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":23289,"date":"2018-02-01","createdTime":"2018-01-28 13:46:07","auditedTime":"2018-02-01 03:00:15","timeDelta":3,"createdHour":13,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 17:16:00 INFO  Pushed Record for 2018-02-01 03:00:16: [{"metadata_ownerId":63735,"date":"2018-02-01","createdTime":"2018-02-01 01:58:27","auditedTime":"2018-02-01 03:00:16","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Clicked","Ignored"],"target":0},{"metadata_ownerId":74173,"date":"2018-02-01","createdTime":"2018-01-31 14:45:05","auditedTime":"2018-02-01 03:00:16","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":51455,"date":"2018-02-01","createdTime":"2018-01-31 09:30:28","auditedTime":"2018-02-01 03:00:16","timeDelta":0,"createdHour":9,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":46163,"date":"2018-02-01","createdTime":"2018-01-26 08:56:52","auditedTime":"2018-02-01 03:00:16","timeDelta":5,"createdHour":8,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":81171,"date":"2018-02-01","createdTime":"2018-01-31 15:29:36","auditedTime":"2018-02-01 03:00:16","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Clicked"],"target":0}][0m
[0m2021.07.12 17:16:01 INFO  Pushed Record for 2018-02-01 03:00:17: [{"metadata_ownerId":50547,"date":"2018-02-01","createdTime":"2018-01-31 21:30:38","auditedTime":"2018-02-01 03:00:17","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 17:16:04 INFO  Pushed Record for 2018-02-01 03:00:18: [{"metadata_ownerId":18706,"date":"2018-02-01","createdTime":"2018-01-31 14:10:52","auditedTime":"2018-02-01 03:00:18","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":3641,"date":"2018-02-01","createdTime":"2018-02-01 02:40:16","auditedTime":"2018-02-01 03:00:18","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 17:16:05 INFO  Pushed Record for 2018-02-01 03:00:19: [{"metadata_ownerId":3163,"date":"2018-02-01","createdTime":"2018-01-31 18:20:16","auditedTime":"2018-02-01 03:00:19","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":32749,"date":"2018-02-01","createdTime":"2018-01-29 20:50:33","auditedTime":"2018-02-01 03:00:19","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 17:16:07 INFO  Pushed Record for 2018-02-01 03:00:20: [{"metadata_ownerId":77398,"date":"2018-02-01","createdTime":"2018-01-31 19:20:14","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":26735,"date":"2018-02-01","createdTime":"2018-01-28 00:01:26","auditedTime":"2018-02-01 03:00:20","timeDelta":4,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":24469,"date":"2018-02-01","createdTime":"2018-01-31 20:01:01","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":80588,"date":"2018-02-01","createdTime":"2018-01-31 12:10:13","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":12,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":81081,"date":"2018-02-01","createdTime":"2018-02-01 00:10:22","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":29613,"date":"2018-02-01","createdTime":"2018-01-31 23:05:15","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0},{"metadata_ownerId":81019,"date":"2018-02-01","createdTime":"2018-01-31 22:10:23","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 17:16:09 INFO  Pushed Record for 2018-02-01 03:00:21: [{"metadata_ownerId":77398,"date":"2018-02-01","createdTime":"2018-01-31 19:05:14","auditedTime":"2018-02-01 03:00:21","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":39329,"date":"2018-02-01","createdTime":"2018-01-29 18:30:16","auditedTime":"2018-02-01 03:00:21","timeDelta":2,"createdHour":18,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 17:16:11 INFO  Pushed Record for 2018-02-01 03:00:22: [{"metadata_ownerId":9828,"date":"2018-02-01","createdTime":"2018-01-31 14:16:19","auditedTime":"2018-02-01 03:00:22","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":53994,"date":"2018-02-01","createdTime":"2018-02-01 00:14:29","auditedTime":"2018-02-01 03:00:22","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":39939,"date":"2018-02-01","createdTime":"2018-01-31 16:01:07","auditedTime":"2018-02-01 03:00:22","timeDelta":0,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 17:16:13 INFO  Pushed Record for 2018-02-01 03:00:23: [{"metadata_ownerId":4983,"date":"2018-02-01","createdTime":"2018-01-30 17:02:32","auditedTime":"2018-02-01 03:00:23","timeDelta":1,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":21228,"date":"2018-02-01","createdTime":"2018-01-31 23:04:16","auditedTime":"2018-02-01 03:00:23","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 17:16:15 INFO  Pushed Record for 2018-02-01 03:00:24: [{"metadata_ownerId":29093,"date":"2018-02-01","createdTime":"2018-01-29 08:42:49","auditedTime":"2018-02-01 03:00:24","timeDelta":2,"createdHour":8,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":62578,"date":"2018-02-01","createdTime":"2018-02-01 00:01:42","auditedTime":"2018-02-01 03:00:24","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 17:16:16 INFO  Pushed Record for 2018-02-01 03:00:25: [{"metadata_ownerId":24518,"date":"2018-02-01","createdTime":"2018-02-01 02:52:43","auditedTime":"2018-02-01 03:00:25","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":76068,"date":"2018-02-01","createdTime":"2018-01-31 20:40:10","auditedTime":"2018-02-01 03:00:25","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 17:16:18 INFO  Pushed Record for 2018-02-01 03:00:26: [{"metadata_ownerId":42758,"date":"2018-02-01","createdTime":"2018-01-31 09:35:43","auditedTime":"2018-02-01 03:00:26","timeDelta":0,"createdHour":9,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":66943,"date":"2018-02-01","createdTime":"2018-01-29 23:55:21","auditedTime":"2018-02-01 03:00:26","timeDelta":2,"createdHour":23,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":5298,"date":"2018-02-01","createdTime":"2018-01-31 16:17:35","auditedTime":"2018-02-01 03:00:26","timeDelta":0,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 17:16:18 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.12 17:16:18 INFO  Listening for transport dt_socket at address: 33635[0m
[0m2021.07.12 17:16:31 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 4h 18m 16.187s)[0m
[0m2021.07.12 17:16:31 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 17:16:33 INFO  time: compiled ht3-test in 1.79s[0m
[0m2021.07.12 17:16:33 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 17:16:33 INFO  Listening for transport dt_socket at address: 33331[0m
[0m2021.07.12 17:16:33 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 17:16:33 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.12 17:16:33 INFO  Trying to attach to remote debuggee VM localhost:33331 .[0m
[0m2021.07.12 17:16:33 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 17:16:33 ERROR Exception in thread "main" com.typesafe.config.ConfigException$Missing: application.json @ file:/home/hwait/dev/ht3/src/main/resources/application.json: 2: No configuration setting found for key 'spark.app_name'[0m
[0m2021.07.12 17:16:33 ERROR 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:157)[0m
[0m2021.07.12 17:16:33 ERROR 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:175)[0m
[0m2021.07.12 17:16:33 ERROR 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:181)[0m
[0m2021.07.12 17:16:33 ERROR 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189)[0m
[0m2021.07.12 17:16:33 ERROR 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:194)[0m
[0m2021.07.12 17:16:33 ERROR 	at com.typesafe.config.impl.SimpleConfig.getString(SimpleConfig.java:251)[0m
[0m2021.07.12 17:16:33 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka$.main(SparkMLRegressionFromKafka.scala:27)[0m
[0m2021.07.12 17:16:33 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka.main(SparkMLRegressionFromKafka.scala)[0m
[0m2021.07.12 17:16:34 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.12 17:16:59 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 4h 18m 43.989s)[0m
[0m2021.07.12 17:16:59 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 17:16:59 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 17:17:00 INFO  time: compiled ht3 in 1.37s[0m
[0m2021.07.12 17:17:00 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 17:17:00 INFO  time: compiled ht3-test in 0.12s[0m
[0m2021.07.12 17:17:00 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 17:17:00 INFO  Listening for transport dt_socket at address: 37389[0m
[0m2021.07.12 17:17:00 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 17:17:00 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.12 17:17:00 INFO  Trying to attach to remote debuggee VM localhost:37389 .[0m
[0m2021.07.12 17:17:00 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 17:17:02 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.12 17:17:02 ERROR 21/07/12 17:17:02 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.149.194 instead (on interface eth0)[0m
[0m2021.07.12 17:17:02 ERROR 21/07/12 17:17:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.12 17:17:03 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.12 17:17:03 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.12 17:17:03 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.12 17:17:03 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.12 17:17:03 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.12 17:17:03 ERROR 21/07/12 17:17:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.12 17:17:05 ERROR 21/07/12 17:17:05 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.[0m
[0m2021.07.12 17:17:05 ERROR 21/07/12 17:17:05 WARN KafkaUtils: overriding enable.auto.commit to false for executor[0m
[0m2021.07.12 17:17:05 ERROR 21/07/12 17:17:05 WARN KafkaUtils: overriding auto.offset.reset to none for executor[0m
[0m2021.07.12 17:17:05 ERROR 21/07/12 17:17:05 WARN KafkaUtils: overriding executor group.id to spark-executor-spark_ml_testing[0m
[0m2021.07.12 17:17:05 ERROR 21/07/12 17:17:05 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135[0m
[0m2021.07.12 17:17:34 INFO  root[0m
[0m2021.07.12 17:17:34 INFO   |-- auditedHour: long (nullable = true)[0m
[0m2021.07.12 17:17:34 INFO   |-- auditedTime: string (nullable = true)[0m
[0m2021.07.12 17:17:34 INFO   |-- createdHour: long (nullable = true)[0m
[0m2021.07.12 17:17:34 INFO   |-- createdTime: string (nullable = true)[0m
[0m2021.07.12 17:17:34 INFO   |-- date: string (nullable = true)[0m
[0m2021.07.12 17:17:34 INFO   |-- feedback: array (nullable = true)[0m
[0m2021.07.12 17:17:34 INFO   |    |-- element: string (containsNull = true)[0m
[0m2021.07.12 17:17:34 INFO   |-- metadata_ownerId: long (nullable = true)[0m
[0m2021.07.12 17:17:34 INFO   |-- target: long (nullable = true)[0m
[0m2021.07.12 17:17:34 INFO   |-- timeDelta: long (nullable = true)[0m
[0m2021.07.12 17:17:34 INFO  [0m
[0m2021.07.12 17:17:34 INFO  +-----------+-------------------+-----------+-------------------+----------+--------------------+----------------+------+---------+[0m
[0m2021.07.12 17:17:34 INFO  |auditedHour|        auditedTime|createdHour|        createdTime|      date|            feedback|metadata_ownerId|target|timeDelta|[0m
[0m2021.07.12 17:17:34 INFO  +-----------+-------------------+-----------+-------------------+----------+--------------------+----------------+------+---------+[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:00|          0|2018-02-01 00:25:14|2018-02-01|[Clicked, Unliked...|           49367|     1|        0|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:00|         10|2018-01-30 10:30:14|2018-02-01|           [Ignored]|           76851|     0|        1|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:00|          2|2018-02-01 02:36:21|2018-02-01|           [Ignored]|           19357|     0|        0|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:01|          1|2018-02-01 01:50:18|2018-02-01|           [Ignored]|           77159|     0|        0|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:02|          0|2018-02-01 00:03:09|2018-02-01|          [Disliked]|           25161|     0|        0|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:02|         20|2018-01-29 20:21:50|2018-02-01|           [Clicked]|           80203|     0|        2|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:02|         15|2018-01-23 15:15:04|2018-02-01|           [Ignored]|           65675|     0|        8|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:03|          1|2018-02-01 01:00:21|2018-02-01|           [Ignored]|           75062|     0|        0|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:03|         15|2018-01-31 15:06:46|2018-02-01|          [Disliked]|           68682|     0|        0|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:03|         13|2018-01-31 13:00:48|2018-02-01|           [Ignored]|           49570|     0|        0|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:04|          2|2018-02-01 02:50:09|2018-02-01|           [Clicked]|           40612|     0|        0|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:04|         14|2018-01-15 14:22:34|2018-02-01|           [Ignored]|           27554|     0|       16|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:05|          6|2018-01-31 06:00:05|2018-02-01|           [Ignored]|           64597|     0|        0|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:05|          3|2018-01-31 03:04:19|2018-02-01|           [Ignored]|           33827|     0|        0|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:06|          0|2018-02-01 00:26:49|2018-02-01|           [Clicked]|           24079|     0|        0|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:06|         20|2018-01-31 20:01:09|2018-02-01|           [Ignored]|           22580|     0|        0|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:06|         15|2017-10-05 15:45:48|2018-02-01|           [Ignored]|           35049|     0|      118|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:06|         19|2018-01-30 19:36:22|2018-02-01|           [Ignored]|            5862|     0|        1|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:07|         22|2018-01-31 22:16:22|2018-02-01|           [Ignored]|            5049|     0|        0|[0m
[0m2021.07.12 17:17:34 INFO  |          3|2018-02-01 03:00:08|          1|2018-02-01 01:01:15|2018-02-01|           [Clicked]|           45299|     0|        0|[0m
[0m2021.07.12 17:17:34 INFO  +-----------+-------------------+-----------+-------------------+----------+--------------------+----------------+------+---------+[0m
[0m2021.07.12 17:17:34 INFO  only showing top 20 rows[0m
[0m2021.07.12 17:17:34 INFO  [0m
[0m2021.07.12 17:18:43 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.12 17:18:43 INFO  Listening for transport dt_socket at address: 43501[0m
Jul 12, 2021 5:59:14 PM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTrace
[0m2021.07.12 18:00:02 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 18:00:02 INFO  time: compiled ht3-test in 0.28s[0m
Jul 12, 2021 6:01:39 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1284
[0m2021.07.12 18:01:53 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 18:01:53 INFO  time: compiled ht3-test in 0.23s[0m
Jul 12, 2021 6:02:16 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1317
Jul 12, 2021 6:02:26 PM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTrace
[0m2021.07.12 18:02:40 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 18:02:40 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 18:02:41 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.12 18:02:41 INFO  Listening for transport dt_socket at address: 52587[0m
[0m2021.07.12 18:02:41 INFO  Trying to attach to remote debuggee VM localhost:52587 .[0m
[0m2021.07.12 18:02:41 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 18:02:42 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.12 18:02:42 ERROR 21/07/12 18:02:42 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.149.194 instead (on interface eth0)[0m
[0m2021.07.12 18:02:42 ERROR 21/07/12 18:02:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.12 18:02:43 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.12 18:02:43 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.12 18:02:43 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.12 18:02:43 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.12 18:02:43 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.12 18:02:43 ERROR 21/07/12 18:02:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.12 18:02:52 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.12 18:02:55 INFO  Pushed Record for 2018-02-01 03:00:00: [{"metadata_ownerId":49367,"date":"2018-02-01","createdTime":"2018-02-01 00:25:14","auditedTime":"2018-02-01 03:00:00","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Clicked","Unliked","Liked"],"target":1},{"metadata_ownerId":76851,"date":"2018-02-01","createdTime":"2018-01-30 10:30:14","auditedTime":"2018-02-01 03:00:00","timeDelta":1,"createdHour":10,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":19357,"date":"2018-02-01","createdTime":"2018-02-01 02:36:21","auditedTime":"2018-02-01 03:00:00","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:02:58 INFO  Pushed Record for 2018-02-01 03:00:01: [{"metadata_ownerId":77159,"date":"2018-02-01","createdTime":"2018-02-01 01:50:18","auditedTime":"2018-02-01 03:00:01","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:03:00 INFO  Pushed Record for 2018-02-01 03:00:02: [{"metadata_ownerId":25161,"date":"2018-02-01","createdTime":"2018-02-01 00:03:09","auditedTime":"2018-02-01 03:00:02","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":80203,"date":"2018-02-01","createdTime":"2018-01-29 20:21:50","auditedTime":"2018-02-01 03:00:02","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":65675,"date":"2018-02-01","createdTime":"2018-01-23 15:15:04","auditedTime":"2018-02-01 03:00:02","timeDelta":8,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:03:02 INFO  Pushed Record for 2018-02-01 03:00:03: [{"metadata_ownerId":75062,"date":"2018-02-01","createdTime":"2018-02-01 01:00:21","auditedTime":"2018-02-01 03:00:03","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":68682,"date":"2018-02-01","createdTime":"2018-01-31 15:06:46","auditedTime":"2018-02-01 03:00:03","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":49570,"date":"2018-02-01","createdTime":"2018-01-31 13:00:48","auditedTime":"2018-02-01 03:00:03","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:03:04 INFO  Pushed Record for 2018-02-01 03:00:04: [{"metadata_ownerId":40612,"date":"2018-02-01","createdTime":"2018-02-01 02:50:09","auditedTime":"2018-02-01 03:00:04","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":27554,"date":"2018-02-01","createdTime":"2018-01-15 14:22:34","auditedTime":"2018-02-01 03:00:04","timeDelta":16,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:03:06 INFO  Pushed Record for 2018-02-01 03:00:05: [{"metadata_ownerId":64597,"date":"2018-02-01","createdTime":"2018-01-31 06:00:05","auditedTime":"2018-02-01 03:00:05","timeDelta":0,"createdHour":6,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":33827,"date":"2018-02-01","createdTime":"2018-01-31 03:04:19","auditedTime":"2018-02-01 03:00:05","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:03:06 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.12 18:03:06 INFO  Listening for transport dt_socket at address: 33875[0m
Jul 12, 2021 6:05:37 PM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTrace
Jul 12, 2021 6:06:16 PM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTrace
[0m2021.07.12 18:08:17 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 5h 10m 2.737s)[0m
[0m2021.07.12 18:08:17 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 18:08:17 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 18:08:17 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.12 18:08:17 INFO  Listening for transport dt_socket at address: 49085[0m
[0m2021.07.12 18:08:17 INFO  Trying to attach to remote debuggee VM localhost:49085 .[0m
[0m2021.07.12 18:08:17 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 18:08:17 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 18:08:18 INFO  compiling ht3-build (1 scala source)[0m
[0m2021.07.12 18:08:17 INFO  time: compiled ht3-test in 0.27s[0m
[0m2021.07.12 18:08:19 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.12 18:08:19 ERROR 21/07/12 18:08:19 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.149.194 instead (on interface eth0)[0m
[0m2021.07.12 18:08:19 ERROR 21/07/12 18:08:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.12 18:08:20 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.12 18:08:20 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.12 18:08:20 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.12 18:08:20 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.12 18:08:20 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.12 18:08:20 ERROR 21/07/12 18:08:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.12 18:08:22 INFO  time: compiled ht3-build in 4.37s[0m
Jul 12, 2021 6:08:22 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.ClassNotFoundException: ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka
java.util.concurrent.CompletionException: java.lang.ClassNotFoundException: ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:29)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:26)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:42)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka
	at scala.meta.internal.metals.debug.BuildTargetClassesFinder.$anonfun$findClassAndBuildTarget$1(BuildTargetClassesFinder.scala:107)
	at scala.Option.fold(Option.scala:251)
	at scala.meta.internal.metals.debug.BuildTargetClassesFinder.findClassAndBuildTarget(BuildTargetClassesFinder.scala:108)
	at scala.meta.internal.metals.debug.BuildTargetClassesFinder.findMainClassAndItsBuildTarget(BuildTargetClassesFinder.scala:36)
	at scala.meta.internal.metals.debug.DebugProvider.$anonfun$resolveMainClassParams$1(DebugProvider.scala:325)
	at scala.meta.internal.metals.debug.DebugProvider$$anonfun$withRebuildRetry$1.$anonfun$applyOrElse$5(DebugProvider.scala:557)
	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
	... 4 more

[0m2021.07.12 18:08:28 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.12 18:08:32 INFO  Pushed Record for 2018-02-01 03:00:00: [{"metadata_ownerId":49367,"date":"2018-02-01","createdTime":"2018-02-01 00:25:14","auditedTime":"2018-02-01 03:00:00","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Clicked","Unliked","Liked"],"target":1},{"metadata_ownerId":76851,"date":"2018-02-01","createdTime":"2018-01-30 10:30:14","auditedTime":"2018-02-01 03:00:00","timeDelta":1,"createdHour":10,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":19357,"date":"2018-02-01","createdTime":"2018-02-01 02:36:21","auditedTime":"2018-02-01 03:00:00","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:08:35 INFO  Pushed Record for 2018-02-01 03:00:01: [{"metadata_ownerId":77159,"date":"2018-02-01","createdTime":"2018-02-01 01:50:18","auditedTime":"2018-02-01 03:00:01","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:08:37 INFO  Pushed Record for 2018-02-01 03:00:02: [{"metadata_ownerId":25161,"date":"2018-02-01","createdTime":"2018-02-01 00:03:09","auditedTime":"2018-02-01 03:00:02","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":80203,"date":"2018-02-01","createdTime":"2018-01-29 20:21:50","auditedTime":"2018-02-01 03:00:02","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":65675,"date":"2018-02-01","createdTime":"2018-01-23 15:15:04","auditedTime":"2018-02-01 03:00:02","timeDelta":8,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:08:39 INFO  Pushed Record for 2018-02-01 03:00:03: [{"metadata_ownerId":75062,"date":"2018-02-01","createdTime":"2018-02-01 01:00:21","auditedTime":"2018-02-01 03:00:03","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":68682,"date":"2018-02-01","createdTime":"2018-01-31 15:06:46","auditedTime":"2018-02-01 03:00:03","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":49570,"date":"2018-02-01","createdTime":"2018-01-31 13:00:48","auditedTime":"2018-02-01 03:00:03","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:08:41 INFO  Pushed Record for 2018-02-01 03:00:04: [{"metadata_ownerId":40612,"date":"2018-02-01","createdTime":"2018-02-01 02:50:09","auditedTime":"2018-02-01 03:00:04","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":27554,"date":"2018-02-01","createdTime":"2018-01-15 14:22:34","auditedTime":"2018-02-01 03:00:04","timeDelta":16,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:08:43 INFO  Pushed Record for 2018-02-01 03:00:05: [{"metadata_ownerId":64597,"date":"2018-02-01","createdTime":"2018-01-31 06:00:05","auditedTime":"2018-02-01 03:00:05","timeDelta":0,"createdHour":6,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":33827,"date":"2018-02-01","createdTime":"2018-01-31 03:04:19","auditedTime":"2018-02-01 03:00:05","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:08:45 INFO  Pushed Record for 2018-02-01 03:00:06: [{"metadata_ownerId":24079,"date":"2018-02-01","createdTime":"2018-02-01 00:26:49","auditedTime":"2018-02-01 03:00:06","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":22580,"date":"2018-02-01","createdTime":"2018-01-31 20:01:09","auditedTime":"2018-02-01 03:00:06","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":35049,"date":"2018-02-01","createdTime":"2017-10-05 15:45:48","auditedTime":"2018-02-01 03:00:06","timeDelta":118,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":5862,"date":"2018-02-01","createdTime":"2018-01-30 19:36:22","auditedTime":"2018-02-01 03:00:06","timeDelta":1,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:08:47 INFO  Pushed Record for 2018-02-01 03:00:07: [{"metadata_ownerId":5049,"date":"2018-02-01","createdTime":"2018-01-31 22:16:22","auditedTime":"2018-02-01 03:00:07","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:08:49 INFO  Pushed Record for 2018-02-01 03:00:08: [{"metadata_ownerId":45299,"date":"2018-02-01","createdTime":"2018-02-01 01:01:15","auditedTime":"2018-02-01 03:00:08","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":80229,"date":"2018-02-01","createdTime":"2018-01-29 10:16:19","auditedTime":"2018-02-01 03:00:08","timeDelta":2,"createdHour":10,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":13290,"date":"2018-02-01","createdTime":"2018-01-31 02:02:07","auditedTime":"2018-02-01 03:00:08","timeDelta":1,"createdHour":2,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":1846,"date":"2018-02-01","createdTime":"2018-02-01 02:20:42","auditedTime":"2018-02-01 03:00:08","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Liked","Ignored"],"target":1}][0m
[0m2021.07.12 18:08:51 INFO  Pushed Record for 2018-02-01 03:00:09: [{"metadata_ownerId":25161,"date":"2018-02-01","createdTime":"2018-02-01 00:03:09","auditedTime":"2018-02-01 03:00:09","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:08:53 INFO  Pushed Record for 2018-02-01 03:00:10: [{"metadata_ownerId":20606,"date":"2018-02-01","createdTime":"2018-01-31 13:10:42","auditedTime":"2018-02-01 03:00:10","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":39412,"date":"2018-02-01","createdTime":"2018-01-30 16:47:35","auditedTime":"2018-02-01 03:00:10","timeDelta":1,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:08:55 INFO  Pushed Record for 2018-02-01 03:00:11: [{"metadata_ownerId":80581,"date":"2018-02-01","createdTime":"2018-01-31 01:08:07","auditedTime":"2018-02-01 03:00:11","timeDelta":1,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":7195,"date":"2018-02-01","createdTime":"2018-01-29 17:22:03","auditedTime":"2018-02-01 03:00:11","timeDelta":2,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":52022,"date":"2018-02-01","createdTime":"2016-02-06 16:11:39","auditedTime":"2018-02-01 03:00:11","timeDelta":725,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:08:57 INFO  Pushed Record for 2018-02-01 03:00:12: [{"metadata_ownerId":15031,"date":"2018-02-01","createdTime":"2018-01-29 23:06:02","auditedTime":"2018-02-01 03:00:12","timeDelta":2,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":13917,"date":"2018-02-01","createdTime":"2018-01-31 15:36:44","auditedTime":"2018-02-01 03:00:12","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":36559,"date":"2018-02-01","createdTime":"2018-01-30 05:44:20","auditedTime":"2018-02-01 03:00:12","timeDelta":1,"createdHour":5,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:08:58 INFO  Pushed Record for 2018-02-01 03:00:13: [{"metadata_ownerId":24072,"date":"2018-02-01","createdTime":"2018-01-30 18:49:09","auditedTime":"2018-02-01 03:00:13","timeDelta":1,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":31314,"date":"2018-02-01","createdTime":"2018-01-29 04:19:24","auditedTime":"2018-02-01 03:00:13","timeDelta":2,"createdHour":4,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:00 INFO  Pushed Record for 2018-02-01 03:00:14: [{"metadata_ownerId":61558,"date":"2018-02-01","createdTime":"2018-01-31 21:00:22","auditedTime":"2018-02-01 03:00:14","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0}][0m
[0m2021.07.12 18:09:02 INFO  Pushed Record for 2018-02-01 03:00:15: [{"metadata_ownerId":11925,"date":"2018-02-01","createdTime":"2018-01-31 10:39:38","auditedTime":"2018-02-01 03:00:15","timeDelta":0,"createdHour":10,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":20251,"date":"2018-02-01","createdTime":"2018-02-01 00:52:18","auditedTime":"2018-02-01 03:00:15","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":23289,"date":"2018-02-01","createdTime":"2018-01-28 13:46:07","auditedTime":"2018-02-01 03:00:15","timeDelta":3,"createdHour":13,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:09:04 INFO  Pushed Record for 2018-02-01 03:00:16: [{"metadata_ownerId":63735,"date":"2018-02-01","createdTime":"2018-02-01 01:58:27","auditedTime":"2018-02-01 03:00:16","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Clicked","Ignored"],"target":0},{"metadata_ownerId":74173,"date":"2018-02-01","createdTime":"2018-01-31 14:45:05","auditedTime":"2018-02-01 03:00:16","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":51455,"date":"2018-02-01","createdTime":"2018-01-31 09:30:28","auditedTime":"2018-02-01 03:00:16","timeDelta":0,"createdHour":9,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":46163,"date":"2018-02-01","createdTime":"2018-01-26 08:56:52","auditedTime":"2018-02-01 03:00:16","timeDelta":5,"createdHour":8,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":81171,"date":"2018-02-01","createdTime":"2018-01-31 15:29:36","auditedTime":"2018-02-01 03:00:16","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Clicked"],"target":0}][0m
[0m2021.07.12 18:09:06 INFO  Pushed Record for 2018-02-01 03:00:17: [{"metadata_ownerId":50547,"date":"2018-02-01","createdTime":"2018-01-31 21:30:38","auditedTime":"2018-02-01 03:00:17","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:08 INFO  Pushed Record for 2018-02-01 03:00:18: [{"metadata_ownerId":18706,"date":"2018-02-01","createdTime":"2018-01-31 14:10:52","auditedTime":"2018-02-01 03:00:18","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":3641,"date":"2018-02-01","createdTime":"2018-02-01 02:40:16","auditedTime":"2018-02-01 03:00:18","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:09:10 INFO  Pushed Record for 2018-02-01 03:00:19: [{"metadata_ownerId":3163,"date":"2018-02-01","createdTime":"2018-01-31 18:20:16","auditedTime":"2018-02-01 03:00:19","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":32749,"date":"2018-02-01","createdTime":"2018-01-29 20:50:33","auditedTime":"2018-02-01 03:00:19","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:12 INFO  Pushed Record for 2018-02-01 03:00:20: [{"metadata_ownerId":77398,"date":"2018-02-01","createdTime":"2018-01-31 19:20:14","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":26735,"date":"2018-02-01","createdTime":"2018-01-28 00:01:26","auditedTime":"2018-02-01 03:00:20","timeDelta":4,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":24469,"date":"2018-02-01","createdTime":"2018-01-31 20:01:01","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":80588,"date":"2018-02-01","createdTime":"2018-01-31 12:10:13","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":12,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":81081,"date":"2018-02-01","createdTime":"2018-02-01 00:10:22","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":29613,"date":"2018-02-01","createdTime":"2018-01-31 23:05:15","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0},{"metadata_ownerId":81019,"date":"2018-02-01","createdTime":"2018-01-31 22:10:23","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:09:13 INFO  Pushed Record for 2018-02-01 03:00:21: [{"metadata_ownerId":77398,"date":"2018-02-01","createdTime":"2018-01-31 19:05:14","auditedTime":"2018-02-01 03:00:21","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":39329,"date":"2018-02-01","createdTime":"2018-01-29 18:30:16","auditedTime":"2018-02-01 03:00:21","timeDelta":2,"createdHour":18,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:09:15 INFO  Pushed Record for 2018-02-01 03:00:22: [{"metadata_ownerId":9828,"date":"2018-02-01","createdTime":"2018-01-31 14:16:19","auditedTime":"2018-02-01 03:00:22","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":53994,"date":"2018-02-01","createdTime":"2018-02-01 00:14:29","auditedTime":"2018-02-01 03:00:22","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":39939,"date":"2018-02-01","createdTime":"2018-01-31 16:01:07","auditedTime":"2018-02-01 03:00:22","timeDelta":0,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:17 INFO  Pushed Record for 2018-02-01 03:00:23: [{"metadata_ownerId":4983,"date":"2018-02-01","createdTime":"2018-01-30 17:02:32","auditedTime":"2018-02-01 03:00:23","timeDelta":1,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":21228,"date":"2018-02-01","createdTime":"2018-01-31 23:04:16","auditedTime":"2018-02-01 03:00:23","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:19 INFO  Pushed Record for 2018-02-01 03:00:24: [{"metadata_ownerId":29093,"date":"2018-02-01","createdTime":"2018-01-29 08:42:49","auditedTime":"2018-02-01 03:00:24","timeDelta":2,"createdHour":8,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":62578,"date":"2018-02-01","createdTime":"2018-02-01 00:01:42","auditedTime":"2018-02-01 03:00:24","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:21 INFO  Pushed Record for 2018-02-01 03:00:25: [{"metadata_ownerId":24518,"date":"2018-02-01","createdTime":"2018-02-01 02:52:43","auditedTime":"2018-02-01 03:00:25","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":76068,"date":"2018-02-01","createdTime":"2018-01-31 20:40:10","auditedTime":"2018-02-01 03:00:25","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:23 INFO  Pushed Record for 2018-02-01 03:00:26: [{"metadata_ownerId":42758,"date":"2018-02-01","createdTime":"2018-01-31 09:35:43","auditedTime":"2018-02-01 03:00:26","timeDelta":0,"createdHour":9,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":66943,"date":"2018-02-01","createdTime":"2018-01-29 23:55:21","auditedTime":"2018-02-01 03:00:26","timeDelta":2,"createdHour":23,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":5298,"date":"2018-02-01","createdTime":"2018-01-31 16:17:35","auditedTime":"2018-02-01 03:00:26","timeDelta":0,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:25 INFO  Pushed Record for 2018-02-01 03:00:27: [{"metadata_ownerId":44830,"date":"2018-02-01","createdTime":"2018-01-31 18:05:45","auditedTime":"2018-02-01 03:00:27","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":12931,"date":"2018-02-01","createdTime":"2018-01-31 18:35:15","auditedTime":"2018-02-01 03:00:27","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:27 INFO  Pushed Record for 2018-02-01 03:00:28: [{"metadata_ownerId":80036,"date":"2018-02-01","createdTime":"2018-01-30 21:01:03","auditedTime":"2018-02-01 03:00:28","timeDelta":1,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":10524,"date":"2018-02-01","createdTime":"2018-01-31 11:57:59","auditedTime":"2018-02-01 03:00:28","timeDelta":0,"createdHour":11,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":48215,"date":"2018-02-01","createdTime":"2018-01-29 22:30:52","auditedTime":"2018-02-01 03:00:28","timeDelta":2,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:28 INFO  Pushed Record for 2018-02-01 03:00:29: [{"metadata_ownerId":3370,"date":"2018-02-01","createdTime":"2018-01-30 19:50:10","auditedTime":"2018-02-01 03:00:29","timeDelta":1,"createdHour":19,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":30094,"date":"2018-02-01","createdTime":"2018-01-30 17:11:56","auditedTime":"2018-02-01 03:00:29","timeDelta":1,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:32 INFO  Pushed Record for 2018-02-01 03:00:31: [{"metadata_ownerId":25334,"date":"2018-02-01","createdTime":"2018-02-01 01:11:52","auditedTime":"2018-02-01 03:00:31","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:33 INFO  Pushed Record for 2018-02-01 03:00:32: [{"metadata_ownerId":33596,"date":"2018-02-01","createdTime":"2018-01-30 11:03:26","auditedTime":"2018-02-01 03:00:32","timeDelta":1,"createdHour":11,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":38648,"date":"2018-02-01","createdTime":"2018-01-30 01:05:17","auditedTime":"2018-02-01 03:00:32","timeDelta":2,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:35 INFO  Pushed Record for 2018-02-01 03:00:33: [{"metadata_ownerId":34617,"date":"2018-02-01","createdTime":"2018-01-31 23:30:34","auditedTime":"2018-02-01 03:00:33","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":26098,"date":"2018-02-01","createdTime":"2018-01-31 08:00:34","auditedTime":"2018-02-01 03:00:33","timeDelta":0,"createdHour":8,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":9486,"date":"2018-02-01","createdTime":"2018-02-01 02:00:53","auditedTime":"2018-02-01 03:00:33","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:37 INFO  Pushed Record for 2018-02-01 03:00:34: [{"metadata_ownerId":78936,"date":"2018-02-01","createdTime":"2017-12-13 23:52:44","auditedTime":"2018-02-01 03:00:34","timeDelta":49,"createdHour":23,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":50598,"date":"2018-02-01","createdTime":"2018-02-01 01:57:12","auditedTime":"2018-02-01 03:00:34","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:39 INFO  Pushed Record for 2018-02-01 03:00:35: [{"metadata_ownerId":4159,"date":"2018-02-01","createdTime":"2018-01-31 00:46:25","auditedTime":"2018-02-01 03:00:35","timeDelta":1,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":61553,"date":"2018-02-01","createdTime":"2018-01-31 19:46:54","auditedTime":"2018-02-01 03:00:35","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":25657,"date":"2018-02-01","createdTime":"2018-01-31 21:09:11","auditedTime":"2018-02-01 03:00:35","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":13451,"date":"2018-02-01","createdTime":"2018-01-30 03:06:12","auditedTime":"2018-02-01 03:00:35","timeDelta":1,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:41 INFO  Pushed Record for 2018-02-01 03:00:36: [{"metadata_ownerId":48644,"date":"2018-02-01","createdTime":"2018-01-31 16:58:20","auditedTime":"2018-02-01 03:00:36","timeDelta":0,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":80855,"date":"2018-02-01","createdTime":"2018-01-31 08:16:09","auditedTime":"2018-02-01 03:00:36","timeDelta":0,"createdHour":8,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":38665,"date":"2018-02-01","createdTime":"2018-01-31 09:16:04","auditedTime":"2018-02-01 03:00:36","timeDelta":0,"createdHour":9,"auditedHour":3,"feedback":["Commented","Liked"],"target":1}][0m
[0m2021.07.12 18:09:43 INFO  Pushed Record for 2018-02-01 03:00:37: [{"metadata_ownerId":69046,"date":"2018-02-01","createdTime":"2018-01-30 15:37:14","auditedTime":"2018-02-01 03:00:37","timeDelta":1,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:45 INFO  Pushed Record for 2018-02-01 03:00:38: [{"metadata_ownerId":42180,"date":"2018-02-01","createdTime":"2018-01-31 11:40:13","auditedTime":"2018-02-01 03:00:38","timeDelta":0,"createdHour":11,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":37057,"date":"2018-02-01","createdTime":"2018-01-30 23:45:50","auditedTime":"2018-02-01 03:00:38","timeDelta":1,"createdHour":23,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":73917,"date":"2018-02-01","createdTime":"2018-01-31 18:18:57","auditedTime":"2018-02-01 03:00:38","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":16674,"date":"2018-02-01","createdTime":"2018-02-01 02:42:56","auditedTime":"2018-02-01 03:00:38","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:49 INFO  Pushed Record for 2018-02-01 03:00:41: [{"metadata_ownerId":44383,"date":"2018-02-01","createdTime":"2017-09-01 09:40:55","auditedTime":"2018-02-01 03:00:41","timeDelta":152,"createdHour":9,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":18164,"date":"2018-02-01","createdTime":"2018-02-01 00:09:51","auditedTime":"2018-02-01 03:00:41","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":80942,"date":"2018-02-01","createdTime":"2018-01-29 23:35:08","auditedTime":"2018-02-01 03:00:41","timeDelta":2,"createdHour":23,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":17882,"date":"2018-02-01","createdTime":"2018-02-01 02:01:50","auditedTime":"2018-02-01 03:00:41","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":71923,"date":"2018-02-01","createdTime":"2018-02-01 03:00:18","auditedTime":"2018-02-01 03:00:41","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:51 INFO  Pushed Record for 2018-02-01 03:00:42: [{"metadata_ownerId":68434,"date":"2018-02-01","createdTime":"2018-02-01 01:05:05","auditedTime":"2018-02-01 03:00:42","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":56160,"date":"2018-02-01","createdTime":"2018-02-01 00:00:50","auditedTime":"2018-02-01 03:00:42","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":60930,"date":"2018-02-01","createdTime":"2018-02-01 01:00:12","auditedTime":"2018-02-01 03:00:42","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:53 INFO  Pushed Record for 2018-02-01 03:00:43: [{"metadata_ownerId":19278,"date":"2018-02-01","createdTime":"2018-01-29 21:50:14","auditedTime":"2018-02-01 03:00:43","timeDelta":2,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":34355,"date":"2018-02-01","createdTime":"2018-02-01 02:35:08","auditedTime":"2018-02-01 03:00:43","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:09:54 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.12 18:09:54 INFO  Listening for transport dt_socket at address: 56179[0m
[0m2021.07.12 18:09:54 ERROR Exception in thread "main" org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:[0m
[0m2021.07.12 18:09:54 ERROR Exchange rangepartitioning(auditedTime#855 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#3271][0m
[0m2021.07.12 18:09:54 ERROR +- *(1) Project [metadata_ownerId#9, date#168, from_unixtime(cast((cast(metadata_createdAt#11L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(Asia/Almaty)) AS createdTime#683, from_unixtime(cast((cast(audit_timestamp#5L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(Asia/Almaty)) AS auditedTime#855, cast(((cast((audit_timestamp#5L - metadata_createdAt#11L) as double) / 3600000.0) / 24.0) as int) AS timeDelta#1028, hour(cast(from_unixtime(cast((cast(metadata_createdAt#11L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(Asia/Almaty)) as timestamp), Some(Asia/Almaty)) AS createdHour#1202, hour(cast(from_unixtime(cast((cast(audit_timestamp#5L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(Asia/Almaty)) as timestamp), Some(Asia/Almaty)) AS auditedHour#1377, feedback#108, CASE WHEN array_contains(feedback#108, Liked) THEN 1 ELSE 0 END AS target#512][0m
[0m2021.07.12 18:09:54 ERROR    +- *(1) Filter ((isnotnull(audit_timestamp#5L) AND (from_unixtime(cast((cast(audit_timestamp#5L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(Asia/Almaty)) >= 2018-02-01 03:00:44)) AND (from_unixtime(cast((cast(audit_timestamp#5L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(Asia/Almaty)) < 2018-02-01 03:00:45))[0m
[0m2021.07.12 18:09:54 ERROR       +- FileScan parquet [audit_timestamp#5L,metadata_ownerId#9,metadata_createdAt#11L,feedback#108,date#168] Batched: false, DataFilters: [isnotnull(audit_timestamp#5L), (from_unixtime(cast((cast(audit_timestamp#5L as double) / 1000.0)..., Format: Parquet, Location: InMemoryFileIndex[file:/home/hwait/dev/data/sna], PartitionFilters: [isnotnull(date#168), (date#168 = 17563)], PushedFilters: [IsNotNull(audit_timestamp)], ReadSchema: struct<audit_timestamp:bigint,metadata_ownerId:int,metadata_createdAt:bigint,feedback:array<string>>[0m
[0m2021.07.12 18:09:54 ERROR [0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:96)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.MapPartitionsExec.doExecute(objects.scala:192)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SerializeFromObjectExec.inputRDDs(objects.scala:117)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:387)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2965)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2965)[0m
[0m2021.07.12 18:09:54 ERROR 	at ru.otus.bigdataml.ht3.DataProvider$.formatContent$1(DataProvider.scala:77)[0m
[0m2021.07.12 18:09:54 ERROR 	at ru.otus.bigdataml.ht3.DataProvider$.iterate$1(DataProvider.scala:83)[0m
[0m2021.07.12 18:09:54 ERROR 	at ru.otus.bigdataml.ht3.DataProvider$.main(DataProvider.scala:94)[0m
[0m2021.07.12 18:09:54 ERROR 	at ru.otus.bigdataml.ht3.DataProvider.main(DataProvider.scala)[0m
[0m2021.07.12 18:09:54 ERROR Caused by: java.lang.IllegalStateException: SparkContext has been shutdown[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2194)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:267)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:155)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)[0m
[0m2021.07.12 18:09:54 ERROR 	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)[0m
[0m2021.07.12 18:09:54 ERROR 	... 56 more[0m
[0m2021.07.12 18:10:00 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 18:10:00 INFO  time: compiled ht3-test in 0.28s[0m
Jul 12, 2021 6:10:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1337
[0m2021.07.12 18:10:33 INFO  compiling ht3-test (1 scala source)[0m
[0m2021.07.12 18:10:33 INFO  time: compiled ht3-test in 0.23s[0m
[0m2021.07.12 18:10:59 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 18:10:59 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 18:10:59 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.12 18:10:59 INFO  Listening for transport dt_socket at address: 42427[0m
[0m2021.07.12 18:10:59 INFO  Trying to attach to remote debuggee VM localhost:42427 .[0m
[0m2021.07.12 18:10:59 INFO  Attaching to debuggee VM succeeded.[0m
Jul 12, 2021 6:10:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.ClassNotFoundException: ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka
java.util.concurrent.CompletionException: java.lang.ClassNotFoundException: ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:29)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:26)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:42)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka
	at scala.meta.internal.metals.debug.BuildTargetClassesFinder.$anonfun$findClassAndBuildTarget$1(BuildTargetClassesFinder.scala:107)
	at scala.Option.fold(Option.scala:251)
	at scala.meta.internal.metals.debug.BuildTargetClassesFinder.findClassAndBuildTarget(BuildTargetClassesFinder.scala:108)
	at scala.meta.internal.metals.debug.BuildTargetClassesFinder.findMainClassAndItsBuildTarget(BuildTargetClassesFinder.scala:36)
	at scala.meta.internal.metals.debug.DebugProvider.$anonfun$resolveMainClassParams$1(DebugProvider.scala:325)
	at scala.meta.internal.metals.debug.DebugProvider$$anonfun$withRebuildRetry$1.$anonfun$applyOrElse$5(DebugProvider.scala:557)
	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
	... 4 more

[0m2021.07.12 18:11:00 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.12 18:11:00 ERROR 21/07/12 18:11:00 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.149.194 instead (on interface eth0)[0m
[0m2021.07.12 18:11:00 ERROR 21/07/12 18:11:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.12 18:11:01 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.12 18:11:01 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.12 18:11:01 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.12 18:11:01 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.12 18:11:01 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.12 18:11:01 ERROR 21/07/12 18:11:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.12 18:11:10 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.12 18:11:13 INFO  Pushed Record for 2018-02-01 03:00:00: [{"metadata_ownerId":49367,"date":"2018-02-01","createdTime":"2018-02-01 00:25:14","auditedTime":"2018-02-01 03:00:00","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Clicked","Unliked","Liked"],"target":1},{"metadata_ownerId":76851,"date":"2018-02-01","createdTime":"2018-01-30 10:30:14","auditedTime":"2018-02-01 03:00:00","timeDelta":1,"createdHour":10,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":19357,"date":"2018-02-01","createdTime":"2018-02-01 02:36:21","auditedTime":"2018-02-01 03:00:00","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:11:16 INFO  Pushed Record for 2018-02-01 03:00:01: [{"metadata_ownerId":77159,"date":"2018-02-01","createdTime":"2018-02-01 01:50:18","auditedTime":"2018-02-01 03:00:01","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:11:18 INFO  Pushed Record for 2018-02-01 03:00:02: [{"metadata_ownerId":25161,"date":"2018-02-01","createdTime":"2018-02-01 00:03:09","auditedTime":"2018-02-01 03:00:02","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":80203,"date":"2018-02-01","createdTime":"2018-01-29 20:21:50","auditedTime":"2018-02-01 03:00:02","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":65675,"date":"2018-02-01","createdTime":"2018-01-23 15:15:04","auditedTime":"2018-02-01 03:00:02","timeDelta":8,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:11:20 INFO  Pushed Record for 2018-02-01 03:00:03: [{"metadata_ownerId":75062,"date":"2018-02-01","createdTime":"2018-02-01 01:00:21","auditedTime":"2018-02-01 03:00:03","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":68682,"date":"2018-02-01","createdTime":"2018-01-31 15:06:46","auditedTime":"2018-02-01 03:00:03","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":49570,"date":"2018-02-01","createdTime":"2018-01-31 13:00:48","auditedTime":"2018-02-01 03:00:03","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:11:22 INFO  Pushed Record for 2018-02-01 03:00:04: [{"metadata_ownerId":40612,"date":"2018-02-01","createdTime":"2018-02-01 02:50:09","auditedTime":"2018-02-01 03:00:04","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":27554,"date":"2018-02-01","createdTime":"2018-01-15 14:22:34","auditedTime":"2018-02-01 03:00:04","timeDelta":16,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:11:24 INFO  Pushed Record for 2018-02-01 03:00:05: [{"metadata_ownerId":64597,"date":"2018-02-01","createdTime":"2018-01-31 06:00:05","auditedTime":"2018-02-01 03:00:05","timeDelta":0,"createdHour":6,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":33827,"date":"2018-02-01","createdTime":"2018-01-31 03:04:19","auditedTime":"2018-02-01 03:00:05","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:11:26 INFO  Pushed Record for 2018-02-01 03:00:06: [{"metadata_ownerId":24079,"date":"2018-02-01","createdTime":"2018-02-01 00:26:49","auditedTime":"2018-02-01 03:00:06","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":22580,"date":"2018-02-01","createdTime":"2018-01-31 20:01:09","auditedTime":"2018-02-01 03:00:06","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":35049,"date":"2018-02-01","createdTime":"2017-10-05 15:45:48","auditedTime":"2018-02-01 03:00:06","timeDelta":118,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":5862,"date":"2018-02-01","createdTime":"2018-01-30 19:36:22","auditedTime":"2018-02-01 03:00:06","timeDelta":1,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:11:28 INFO  Pushed Record for 2018-02-01 03:00:07: [{"metadata_ownerId":5049,"date":"2018-02-01","createdTime":"2018-01-31 22:16:22","auditedTime":"2018-02-01 03:00:07","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:11:30 INFO  Pushed Record for 2018-02-01 03:00:08: [{"metadata_ownerId":45299,"date":"2018-02-01","createdTime":"2018-02-01 01:01:15","auditedTime":"2018-02-01 03:00:08","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":80229,"date":"2018-02-01","createdTime":"2018-01-29 10:16:19","auditedTime":"2018-02-01 03:00:08","timeDelta":2,"createdHour":10,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":13290,"date":"2018-02-01","createdTime":"2018-01-31 02:02:07","auditedTime":"2018-02-01 03:00:08","timeDelta":1,"createdHour":2,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":1846,"date":"2018-02-01","createdTime":"2018-02-01 02:20:42","auditedTime":"2018-02-01 03:00:08","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Liked","Ignored"],"target":1}][0m
[0m2021.07.12 18:11:32 INFO  Pushed Record for 2018-02-01 03:00:09: [{"metadata_ownerId":25161,"date":"2018-02-01","createdTime":"2018-02-01 00:03:09","auditedTime":"2018-02-01 03:00:09","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:11:34 INFO  Pushed Record for 2018-02-01 03:00:10: [{"metadata_ownerId":20606,"date":"2018-02-01","createdTime":"2018-01-31 13:10:42","auditedTime":"2018-02-01 03:00:10","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":39412,"date":"2018-02-01","createdTime":"2018-01-30 16:47:35","auditedTime":"2018-02-01 03:00:10","timeDelta":1,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:11:36 INFO  Pushed Record for 2018-02-01 03:00:11: [{"metadata_ownerId":80581,"date":"2018-02-01","createdTime":"2018-01-31 01:08:07","auditedTime":"2018-02-01 03:00:11","timeDelta":1,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":7195,"date":"2018-02-01","createdTime":"2018-01-29 17:22:03","auditedTime":"2018-02-01 03:00:11","timeDelta":2,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":52022,"date":"2018-02-01","createdTime":"2016-02-06 16:11:39","auditedTime":"2018-02-01 03:00:11","timeDelta":725,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:11:37 INFO  Pushed Record for 2018-02-01 03:00:12: [{"metadata_ownerId":15031,"date":"2018-02-01","createdTime":"2018-01-29 23:06:02","auditedTime":"2018-02-01 03:00:12","timeDelta":2,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":13917,"date":"2018-02-01","createdTime":"2018-01-31 15:36:44","auditedTime":"2018-02-01 03:00:12","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":36559,"date":"2018-02-01","createdTime":"2018-01-30 05:44:20","auditedTime":"2018-02-01 03:00:12","timeDelta":1,"createdHour":5,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:11:39 INFO  Pushed Record for 2018-02-01 03:00:13: [{"metadata_ownerId":24072,"date":"2018-02-01","createdTime":"2018-01-30 18:49:09","auditedTime":"2018-02-01 03:00:13","timeDelta":1,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":31314,"date":"2018-02-01","createdTime":"2018-01-29 04:19:24","auditedTime":"2018-02-01 03:00:13","timeDelta":2,"createdHour":4,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:11:41 INFO  Pushed Record for 2018-02-01 03:00:14: [{"metadata_ownerId":61558,"date":"2018-02-01","createdTime":"2018-01-31 21:00:22","auditedTime":"2018-02-01 03:00:14","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0}][0m
[0m2021.07.12 18:11:43 INFO  Pushed Record for 2018-02-01 03:00:15: [{"metadata_ownerId":11925,"date":"2018-02-01","createdTime":"2018-01-31 10:39:38","auditedTime":"2018-02-01 03:00:15","timeDelta":0,"createdHour":10,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":20251,"date":"2018-02-01","createdTime":"2018-02-01 00:52:18","auditedTime":"2018-02-01 03:00:15","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":23289,"date":"2018-02-01","createdTime":"2018-01-28 13:46:07","auditedTime":"2018-02-01 03:00:15","timeDelta":3,"createdHour":13,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:11:45 INFO  Pushed Record for 2018-02-01 03:00:16: [{"metadata_ownerId":63735,"date":"2018-02-01","createdTime":"2018-02-01 01:58:27","auditedTime":"2018-02-01 03:00:16","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Clicked","Ignored"],"target":0},{"metadata_ownerId":74173,"date":"2018-02-01","createdTime":"2018-01-31 14:45:05","auditedTime":"2018-02-01 03:00:16","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":51455,"date":"2018-02-01","createdTime":"2018-01-31 09:30:28","auditedTime":"2018-02-01 03:00:16","timeDelta":0,"createdHour":9,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":46163,"date":"2018-02-01","createdTime":"2018-01-26 08:56:52","auditedTime":"2018-02-01 03:00:16","timeDelta":5,"createdHour":8,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":81171,"date":"2018-02-01","createdTime":"2018-01-31 15:29:36","auditedTime":"2018-02-01 03:00:16","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Clicked"],"target":0}][0m
[0m2021.07.12 18:11:47 INFO  Pushed Record for 2018-02-01 03:00:17: [{"metadata_ownerId":50547,"date":"2018-02-01","createdTime":"2018-01-31 21:30:38","auditedTime":"2018-02-01 03:00:17","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:11:49 INFO  Pushed Record for 2018-02-01 03:00:18: [{"metadata_ownerId":18706,"date":"2018-02-01","createdTime":"2018-01-31 14:10:52","auditedTime":"2018-02-01 03:00:18","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":3641,"date":"2018-02-01","createdTime":"2018-02-01 02:40:16","auditedTime":"2018-02-01 03:00:18","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:11:50 INFO  Pushed Record for 2018-02-01 03:00:19: [{"metadata_ownerId":3163,"date":"2018-02-01","createdTime":"2018-01-31 18:20:16","auditedTime":"2018-02-01 03:00:19","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":32749,"date":"2018-02-01","createdTime":"2018-01-29 20:50:33","auditedTime":"2018-02-01 03:00:19","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:11:52 INFO  Pushed Record for 2018-02-01 03:00:20: [{"metadata_ownerId":77398,"date":"2018-02-01","createdTime":"2018-01-31 19:20:14","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":26735,"date":"2018-02-01","createdTime":"2018-01-28 00:01:26","auditedTime":"2018-02-01 03:00:20","timeDelta":4,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":24469,"date":"2018-02-01","createdTime":"2018-01-31 20:01:01","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":80588,"date":"2018-02-01","createdTime":"2018-01-31 12:10:13","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":12,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":81081,"date":"2018-02-01","createdTime":"2018-02-01 00:10:22","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":29613,"date":"2018-02-01","createdTime":"2018-01-31 23:05:15","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0},{"metadata_ownerId":81019,"date":"2018-02-01","createdTime":"2018-01-31 22:10:23","auditedTime":"2018-02-01 03:00:20","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:11:54 INFO  Pushed Record for 2018-02-01 03:00:21: [{"metadata_ownerId":77398,"date":"2018-02-01","createdTime":"2018-01-31 19:05:14","auditedTime":"2018-02-01 03:00:21","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":39329,"date":"2018-02-01","createdTime":"2018-01-29 18:30:16","auditedTime":"2018-02-01 03:00:21","timeDelta":2,"createdHour":18,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:11:56 INFO  Pushed Record for 2018-02-01 03:00:22: [{"metadata_ownerId":9828,"date":"2018-02-01","createdTime":"2018-01-31 14:16:19","auditedTime":"2018-02-01 03:00:22","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":53994,"date":"2018-02-01","createdTime":"2018-02-01 00:14:29","auditedTime":"2018-02-01 03:00:22","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":39939,"date":"2018-02-01","createdTime":"2018-01-31 16:01:07","auditedTime":"2018-02-01 03:00:22","timeDelta":0,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:11:58 INFO  Pushed Record for 2018-02-01 03:00:23: [{"metadata_ownerId":4983,"date":"2018-02-01","createdTime":"2018-01-30 17:02:32","auditedTime":"2018-02-01 03:00:23","timeDelta":1,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":21228,"date":"2018-02-01","createdTime":"2018-01-31 23:04:16","auditedTime":"2018-02-01 03:00:23","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:00 INFO  Pushed Record for 2018-02-01 03:00:24: [{"metadata_ownerId":29093,"date":"2018-02-01","createdTime":"2018-01-29 08:42:49","auditedTime":"2018-02-01 03:00:24","timeDelta":2,"createdHour":8,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":62578,"date":"2018-02-01","createdTime":"2018-02-01 00:01:42","auditedTime":"2018-02-01 03:00:24","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:01 INFO  Pushed Record for 2018-02-01 03:00:25: [{"metadata_ownerId":24518,"date":"2018-02-01","createdTime":"2018-02-01 02:52:43","auditedTime":"2018-02-01 03:00:25","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":76068,"date":"2018-02-01","createdTime":"2018-01-31 20:40:10","auditedTime":"2018-02-01 03:00:25","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:03 INFO  Pushed Record for 2018-02-01 03:00:26: [{"metadata_ownerId":42758,"date":"2018-02-01","createdTime":"2018-01-31 09:35:43","auditedTime":"2018-02-01 03:00:26","timeDelta":0,"createdHour":9,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":66943,"date":"2018-02-01","createdTime":"2018-01-29 23:55:21","auditedTime":"2018-02-01 03:00:26","timeDelta":2,"createdHour":23,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":5298,"date":"2018-02-01","createdTime":"2018-01-31 16:17:35","auditedTime":"2018-02-01 03:00:26","timeDelta":0,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:05 INFO  Pushed Record for 2018-02-01 03:00:27: [{"metadata_ownerId":44830,"date":"2018-02-01","createdTime":"2018-01-31 18:05:45","auditedTime":"2018-02-01 03:00:27","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":12931,"date":"2018-02-01","createdTime":"2018-01-31 18:35:15","auditedTime":"2018-02-01 03:00:27","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:07 INFO  Pushed Record for 2018-02-01 03:00:28: [{"metadata_ownerId":80036,"date":"2018-02-01","createdTime":"2018-01-30 21:01:03","auditedTime":"2018-02-01 03:00:28","timeDelta":1,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":10524,"date":"2018-02-01","createdTime":"2018-01-31 11:57:59","auditedTime":"2018-02-01 03:00:28","timeDelta":0,"createdHour":11,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":48215,"date":"2018-02-01","createdTime":"2018-01-29 22:30:52","auditedTime":"2018-02-01 03:00:28","timeDelta":2,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:09 INFO  Pushed Record for 2018-02-01 03:00:29: [{"metadata_ownerId":3370,"date":"2018-02-01","createdTime":"2018-01-30 19:50:10","auditedTime":"2018-02-01 03:00:29","timeDelta":1,"createdHour":19,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":30094,"date":"2018-02-01","createdTime":"2018-01-30 17:11:56","auditedTime":"2018-02-01 03:00:29","timeDelta":1,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:12 INFO  Pushed Record for 2018-02-01 03:00:31: [{"metadata_ownerId":25334,"date":"2018-02-01","createdTime":"2018-02-01 01:11:52","auditedTime":"2018-02-01 03:00:31","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:14 INFO  Pushed Record for 2018-02-01 03:00:32: [{"metadata_ownerId":33596,"date":"2018-02-01","createdTime":"2018-01-30 11:03:26","auditedTime":"2018-02-01 03:00:32","timeDelta":1,"createdHour":11,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":38648,"date":"2018-02-01","createdTime":"2018-01-30 01:05:17","auditedTime":"2018-02-01 03:00:32","timeDelta":2,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:16 INFO  Pushed Record for 2018-02-01 03:00:33: [{"metadata_ownerId":34617,"date":"2018-02-01","createdTime":"2018-01-31 23:30:34","auditedTime":"2018-02-01 03:00:33","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":26098,"date":"2018-02-01","createdTime":"2018-01-31 08:00:34","auditedTime":"2018-02-01 03:00:33","timeDelta":0,"createdHour":8,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":9486,"date":"2018-02-01","createdTime":"2018-02-01 02:00:53","auditedTime":"2018-02-01 03:00:33","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:17 INFO  Pushed Record for 2018-02-01 03:00:34: [{"metadata_ownerId":78936,"date":"2018-02-01","createdTime":"2017-12-13 23:52:44","auditedTime":"2018-02-01 03:00:34","timeDelta":49,"createdHour":23,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":50598,"date":"2018-02-01","createdTime":"2018-02-01 01:57:12","auditedTime":"2018-02-01 03:00:34","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:19 INFO  Pushed Record for 2018-02-01 03:00:35: [{"metadata_ownerId":4159,"date":"2018-02-01","createdTime":"2018-01-31 00:46:25","auditedTime":"2018-02-01 03:00:35","timeDelta":1,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":61553,"date":"2018-02-01","createdTime":"2018-01-31 19:46:54","auditedTime":"2018-02-01 03:00:35","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":25657,"date":"2018-02-01","createdTime":"2018-01-31 21:09:11","auditedTime":"2018-02-01 03:00:35","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":13451,"date":"2018-02-01","createdTime":"2018-01-30 03:06:12","auditedTime":"2018-02-01 03:00:35","timeDelta":1,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:21 INFO  Pushed Record for 2018-02-01 03:00:36: [{"metadata_ownerId":48644,"date":"2018-02-01","createdTime":"2018-01-31 16:58:20","auditedTime":"2018-02-01 03:00:36","timeDelta":0,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":80855,"date":"2018-02-01","createdTime":"2018-01-31 08:16:09","auditedTime":"2018-02-01 03:00:36","timeDelta":0,"createdHour":8,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":38665,"date":"2018-02-01","createdTime":"2018-01-31 09:16:04","auditedTime":"2018-02-01 03:00:36","timeDelta":0,"createdHour":9,"auditedHour":3,"feedback":["Commented","Liked"],"target":1}][0m
[0m2021.07.12 18:12:23 INFO  Pushed Record for 2018-02-01 03:00:37: [{"metadata_ownerId":69046,"date":"2018-02-01","createdTime":"2018-01-30 15:37:14","auditedTime":"2018-02-01 03:00:37","timeDelta":1,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:25 INFO  Pushed Record for 2018-02-01 03:00:38: [{"metadata_ownerId":42180,"date":"2018-02-01","createdTime":"2018-01-31 11:40:13","auditedTime":"2018-02-01 03:00:38","timeDelta":0,"createdHour":11,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":37057,"date":"2018-02-01","createdTime":"2018-01-30 23:45:50","auditedTime":"2018-02-01 03:00:38","timeDelta":1,"createdHour":23,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":73917,"date":"2018-02-01","createdTime":"2018-01-31 18:18:57","auditedTime":"2018-02-01 03:00:38","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":16674,"date":"2018-02-01","createdTime":"2018-02-01 02:42:56","auditedTime":"2018-02-01 03:00:38","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:29 INFO  Pushed Record for 2018-02-01 03:00:41: [{"metadata_ownerId":44383,"date":"2018-02-01","createdTime":"2017-09-01 09:40:55","auditedTime":"2018-02-01 03:00:41","timeDelta":152,"createdHour":9,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":18164,"date":"2018-02-01","createdTime":"2018-02-01 00:09:51","auditedTime":"2018-02-01 03:00:41","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":80942,"date":"2018-02-01","createdTime":"2018-01-29 23:35:08","auditedTime":"2018-02-01 03:00:41","timeDelta":2,"createdHour":23,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":17882,"date":"2018-02-01","createdTime":"2018-02-01 02:01:50","auditedTime":"2018-02-01 03:00:41","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":71923,"date":"2018-02-01","createdTime":"2018-02-01 03:00:18","auditedTime":"2018-02-01 03:00:41","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:31 INFO  Pushed Record for 2018-02-01 03:00:42: [{"metadata_ownerId":68434,"date":"2018-02-01","createdTime":"2018-02-01 01:05:05","auditedTime":"2018-02-01 03:00:42","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":56160,"date":"2018-02-01","createdTime":"2018-02-01 00:00:50","auditedTime":"2018-02-01 03:00:42","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":60930,"date":"2018-02-01","createdTime":"2018-02-01 01:00:12","auditedTime":"2018-02-01 03:00:42","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:33 INFO  Pushed Record for 2018-02-01 03:00:43: [{"metadata_ownerId":19278,"date":"2018-02-01","createdTime":"2018-01-29 21:50:14","auditedTime":"2018-02-01 03:00:43","timeDelta":2,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":34355,"date":"2018-02-01","createdTime":"2018-02-01 02:35:08","auditedTime":"2018-02-01 03:00:43","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:35 INFO  Pushed Record for 2018-02-01 03:00:44: [{"metadata_ownerId":19621,"date":"2018-02-01","createdTime":"2018-01-30 22:33:02","auditedTime":"2018-02-01 03:00:44","timeDelta":1,"createdHour":22,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":22293,"date":"2018-02-01","createdTime":"2018-01-31 20:12:08","auditedTime":"2018-02-01 03:00:44","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["ReShared","Liked"],"target":1},{"metadata_ownerId":12312,"date":"2018-02-01","createdTime":"2018-01-31 13:35:16","auditedTime":"2018-02-01 03:00:44","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:37 INFO  Pushed Record for 2018-02-01 03:00:45: [{"metadata_ownerId":7716,"date":"2018-02-01","createdTime":"2018-01-30 05:10:11","auditedTime":"2018-02-01 03:00:45","timeDelta":1,"createdHour":5,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":23943,"date":"2018-02-01","createdTime":"2018-01-31 10:28:14","auditedTime":"2018-02-01 03:00:45","timeDelta":0,"createdHour":10,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":24590,"date":"2018-02-01","createdTime":"2018-01-30 22:24:23","auditedTime":"2018-02-01 03:00:45","timeDelta":1,"createdHour":22,"auditedHour":3,"feedback":["Commented"],"target":0}][0m
[0m2021.07.12 18:12:39 INFO  Pushed Record for 2018-02-01 03:00:46: [{"metadata_ownerId":74285,"date":"2018-02-01","createdTime":"2018-01-31 23:04:37","auditedTime":"2018-02-01 03:00:46","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":34655,"date":"2018-02-01","createdTime":"2018-02-01 01:55:05","auditedTime":"2018-02-01 03:00:46","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:41 INFO  Pushed Record for 2018-02-01 03:00:47: [{"metadata_ownerId":5737,"date":"2018-02-01","createdTime":"2018-02-01 01:27:34","auditedTime":"2018-02-01 03:00:47","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":45301,"date":"2018-02-01","createdTime":"2018-01-31 22:10:53","auditedTime":"2018-02-01 03:00:47","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":53288,"date":"2018-02-01","createdTime":"2018-01-31 22:05:14","auditedTime":"2018-02-01 03:00:47","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0}][0m
[0m2021.07.12 18:12:42 INFO  Pushed Record for 2018-02-01 03:00:48: [{"metadata_ownerId":20095,"date":"2018-02-01","createdTime":"2018-01-28 23:33:07","auditedTime":"2018-02-01 03:00:48","timeDelta":3,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":67997,"date":"2018-02-01","createdTime":"2018-01-31 10:54:15","auditedTime":"2018-02-01 03:00:48","timeDelta":0,"createdHour":10,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":2831,"date":"2018-02-01","createdTime":"2018-01-31 17:30:59","auditedTime":"2018-02-01 03:00:48","timeDelta":0,"createdHour":17,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0},{"metadata_ownerId":64054,"date":"2018-02-01","createdTime":"2018-01-30 16:49:18","auditedTime":"2018-02-01 03:00:48","timeDelta":1,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:44 INFO  Pushed Record for 2018-02-01 03:00:49: [{"metadata_ownerId":19089,"date":"2018-02-01","createdTime":"2018-01-31 18:00:54","auditedTime":"2018-02-01 03:00:49","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":22445,"date":"2018-02-01","createdTime":"2018-01-31 21:02:18","auditedTime":"2018-02-01 03:00:49","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":18956,"date":"2018-02-01","createdTime":"2018-01-31 22:40:36","auditedTime":"2018-02-01 03:00:49","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Clicked","ReShared","Liked"],"target":1}][0m
[0m2021.07.12 18:12:46 INFO  Pushed Record for 2018-02-01 03:00:50: [{"metadata_ownerId":13917,"date":"2018-02-01","createdTime":"2015-09-04 21:32:55","auditedTime":"2018-02-01 03:00:50","timeDelta":880,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:48 INFO  Pushed Record for 2018-02-01 03:00:51: [{"metadata_ownerId":17927,"date":"2018-02-01","createdTime":"2018-01-31 23:15:02","auditedTime":"2018-02-01 03:00:51","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":680,"date":"2018-02-01","createdTime":"2018-01-27 16:59:18","auditedTime":"2018-02-01 03:00:51","timeDelta":4,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:50 INFO  Pushed Record for 2018-02-01 03:00:52: [{"metadata_ownerId":49596,"date":"2018-02-01","createdTime":"2018-01-31 18:37:31","auditedTime":"2018-02-01 03:00:52","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":21225,"date":"2018-02-01","createdTime":"2018-01-31 23:21:06","auditedTime":"2018-02-01 03:00:52","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:52 INFO  Pushed Record for 2018-02-01 03:00:53: [{"metadata_ownerId":64370,"date":"2018-02-01","createdTime":"2018-01-29 21:00:06","auditedTime":"2018-02-01 03:00:53","timeDelta":2,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":9182,"date":"2018-02-01","createdTime":"2018-01-31 21:06:12","auditedTime":"2018-02-01 03:00:53","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:53 INFO  Pushed Record for 2018-02-01 03:00:54: [{"metadata_ownerId":6518,"date":"2018-02-01","createdTime":"2018-01-31 22:14:31","auditedTime":"2018-02-01 03:00:54","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0},{"metadata_ownerId":26953,"date":"2018-02-01","createdTime":"2018-02-01 00:32:31","auditedTime":"2018-02-01 03:00:54","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":13818,"date":"2018-02-01","createdTime":"2018-02-01 00:28:19","auditedTime":"2018-02-01 03:00:54","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":51892,"date":"2018-02-01","createdTime":"2018-02-01 02:15:12","auditedTime":"2018-02-01 03:00:54","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked","ReShared","Ignored"],"target":0}][0m
[0m2021.07.12 18:12:55 INFO  Pushed Record for 2018-02-01 03:00:55: [{"metadata_ownerId":52543,"date":"2018-02-01","createdTime":"2018-01-31 18:20:28","auditedTime":"2018-02-01 03:00:55","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":62886,"date":"2018-02-01","createdTime":"2018-02-01 02:50:58","auditedTime":"2018-02-01 03:00:55","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":27220,"date":"2018-02-01","createdTime":"2018-01-31 23:16:17","auditedTime":"2018-02-01 03:00:55","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":23016,"date":"2018-02-01","createdTime":"2018-01-31 19:40:32","auditedTime":"2018-02-01 03:00:55","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":18200,"date":"2018-02-01","createdTime":"2018-01-30 19:32:06","auditedTime":"2018-02-01 03:00:55","timeDelta":1,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":24590,"date":"2018-02-01","createdTime":"2018-01-31 17:42:14","auditedTime":"2018-02-01 03:00:55","timeDelta":0,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":54159,"date":"2018-02-01","createdTime":"2018-01-31 10:07:51","auditedTime":"2018-02-01 03:00:55","timeDelta":0,"createdHour":10,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":8142,"date":"2018-02-01","createdTime":"2018-01-30 21:53:10","auditedTime":"2018-02-01 03:00:55","timeDelta":1,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":70280,"date":"2018-02-01","createdTime":"2018-01-31 21:05:18","auditedTime":"2018-02-01 03:00:55","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:57 INFO  Pushed Record for 2018-02-01 03:00:56: [{"metadata_ownerId":183,"date":"2018-02-01","createdTime":"2018-02-01 00:15:18","auditedTime":"2018-02-01 03:00:56","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:12:59 INFO  Pushed Record for 2018-02-01 03:00:57: [{"metadata_ownerId":64054,"date":"2018-02-01","createdTime":"2018-01-30 16:49:18","auditedTime":"2018-02-01 03:00:57","timeDelta":1,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":14205,"date":"2018-02-01","createdTime":"2018-01-31 16:39:10","auditedTime":"2018-02-01 03:00:57","timeDelta":0,"createdHour":16,"auditedHour":3,"feedback":["Viewed","Ignored"],"target":0},{"metadata_ownerId":17653,"date":"2018-02-01","createdTime":"2018-02-01 02:22:12","auditedTime":"2018-02-01 03:00:57","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":71744,"date":"2018-02-01","createdTime":"2018-01-30 02:50:21","auditedTime":"2018-02-01 03:00:57","timeDelta":2,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:01 INFO  Pushed Record for 2018-02-01 03:00:58: [{"metadata_ownerId":33082,"date":"2018-02-01","createdTime":"2018-01-31 18:57:03","auditedTime":"2018-02-01 03:00:58","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":23605,"date":"2018-02-01","createdTime":"2018-01-30 14:34:05","auditedTime":"2018-02-01 03:00:58","timeDelta":1,"createdHour":14,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":21245,"date":"2018-02-01","createdTime":"2018-02-01 02:36:03","auditedTime":"2018-02-01 03:00:58","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:04 INFO  Pushed Record for 2018-02-01 03:01:00: [{"metadata_ownerId":6124,"date":"2018-02-01","createdTime":"2018-02-01 02:14:05","auditedTime":"2018-02-01 03:01:00","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:06 INFO  Pushed Record for 2018-02-01 03:01:01: [{"metadata_ownerId":79921,"date":"2018-02-01","createdTime":"2018-01-31 22:22:21","auditedTime":"2018-02-01 03:01:01","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:09 INFO  Pushed Record for 2018-02-01 03:01:03: [{"metadata_ownerId":30397,"date":"2018-02-01","createdTime":"2018-01-31 17:23:04","auditedTime":"2018-02-01 03:01:03","timeDelta":0,"createdHour":17,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":30397,"date":"2018-02-01","createdTime":"2018-01-31 19:19:48","auditedTime":"2018-02-01 03:01:03","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":48898,"date":"2018-02-01","createdTime":"2018-01-30 08:00:33","auditedTime":"2018-02-01 03:01:03","timeDelta":1,"createdHour":8,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":16035,"date":"2018-02-01","createdTime":"2018-01-31 13:35:25","auditedTime":"2018-02-01 03:01:03","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Clicked","Viewed","Liked"],"target":1},{"metadata_ownerId":18200,"date":"2018-02-01","createdTime":"2018-01-30 19:32:06","auditedTime":"2018-02-01 03:01:03","timeDelta":1,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:11 INFO  Pushed Record for 2018-02-01 03:01:04: [{"metadata_ownerId":41622,"date":"2018-02-01","createdTime":"2018-02-01 01:01:00","auditedTime":"2018-02-01 03:01:04","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:13:12 INFO  Pushed Record for 2018-02-01 03:01:05: [{"metadata_ownerId":41474,"date":"2018-02-01","createdTime":"2018-01-30 23:48:12","auditedTime":"2018-02-01 03:01:05","timeDelta":1,"createdHour":23,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":32839,"date":"2018-02-01","createdTime":"2018-01-30 11:03:26","auditedTime":"2018-02-01 03:01:05","timeDelta":1,"createdHour":11,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":60772,"date":"2018-02-01","createdTime":"2018-01-31 21:35:07","auditedTime":"2018-02-01 03:01:05","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:13:14 INFO  Pushed Record for 2018-02-01 03:01:06: [{"metadata_ownerId":18918,"date":"2018-02-01","createdTime":"2018-01-31 17:40:18","auditedTime":"2018-02-01 03:01:06","timeDelta":0,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":47420,"date":"2018-02-01","createdTime":"2018-02-01 00:46:11","auditedTime":"2018-02-01 03:01:06","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":32023,"date":"2018-02-01","createdTime":"2018-02-01 02:03:47","auditedTime":"2018-02-01 03:01:06","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked","ReShared","Liked"],"target":1}][0m
[0m2021.07.12 18:13:16 INFO  Pushed Record for 2018-02-01 03:01:07: [{"metadata_ownerId":43398,"date":"2018-02-01","createdTime":"2018-02-01 02:05:57","auditedTime":"2018-02-01 03:01:07","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":27984,"date":"2018-02-01","createdTime":"2018-02-01 00:00:17","auditedTime":"2018-02-01 03:01:07","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":39980,"date":"2018-02-01","createdTime":"2018-01-31 00:40:02","auditedTime":"2018-02-01 03:01:07","timeDelta":1,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:18 INFO  Pushed Record for 2018-02-01 03:01:08: [{"metadata_ownerId":18574,"date":"2018-02-01","createdTime":"2018-02-01 00:02:15","auditedTime":"2018-02-01 03:01:08","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["ReShared","Ignored"],"target":0},{"metadata_ownerId":35796,"date":"2018-02-01","createdTime":"2018-01-30 03:45:09","auditedTime":"2018-02-01 03:01:08","timeDelta":1,"createdHour":3,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":17969,"date":"2018-02-01","createdTime":"2018-02-01 02:00:31","auditedTime":"2018-02-01 03:01:08","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:20 INFO  Pushed Record for 2018-02-01 03:01:09: [{"metadata_ownerId":49721,"date":"2018-02-01","createdTime":"2018-02-01 03:00:30","auditedTime":"2018-02-01 03:01:09","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":84483,"date":"2018-02-01","createdTime":"2018-01-31 04:00:13","auditedTime":"2018-02-01 03:01:09","timeDelta":0,"createdHour":4,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":40607,"date":"2018-02-01","createdTime":"2018-02-01 02:20:12","auditedTime":"2018-02-01 03:01:09","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":6575,"date":"2018-02-01","createdTime":"2018-02-01 01:30:23","auditedTime":"2018-02-01 03:01:09","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":5826,"date":"2018-02-01","createdTime":"2018-01-31 23:05:12","auditedTime":"2018-02-01 03:01:09","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Commented"],"target":0},{"metadata_ownerId":19407,"date":"2018-02-01","createdTime":"2018-01-30 12:21:03","auditedTime":"2018-02-01 03:01:09","timeDelta":1,"createdHour":12,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:13:22 INFO  Pushed Record for 2018-02-01 03:01:10: [{"metadata_ownerId":24187,"date":"2018-02-01","createdTime":"2018-01-30 13:06:42","auditedTime":"2018-02-01 03:01:10","timeDelta":1,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":9236,"date":"2018-02-01","createdTime":"2018-02-01 02:18:21","auditedTime":"2018-02-01 03:01:10","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked"],"target":0}][0m
[0m2021.07.12 18:13:23 INFO  Pushed Record for 2018-02-01 03:01:11: [{"metadata_ownerId":45565,"date":"2018-02-01","createdTime":"2018-02-01 02:38:06","auditedTime":"2018-02-01 03:01:11","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":20751,"date":"2018-02-01","createdTime":"2018-01-31 21:16:30","auditedTime":"2018-02-01 03:01:11","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":70210,"date":"2018-02-01","createdTime":"2018-02-01 02:15:04","auditedTime":"2018-02-01 03:01:11","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":83461,"date":"2018-02-01","createdTime":"2018-01-31 15:40:31","auditedTime":"2018-02-01 03:01:11","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:13:25 INFO  Pushed Record for 2018-02-01 03:01:12: [{"metadata_ownerId":17118,"date":"2018-02-01","createdTime":"2018-01-29 20:57:25","auditedTime":"2018-02-01 03:01:12","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":44744,"date":"2018-02-01","createdTime":"2018-01-31 14:35:14","auditedTime":"2018-02-01 03:01:12","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":81836,"date":"2018-02-01","createdTime":"2018-02-01 02:10:10","auditedTime":"2018-02-01 03:01:12","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":16540,"date":"2018-02-01","createdTime":"2018-01-29 17:40:48","auditedTime":"2018-02-01 03:01:12","timeDelta":2,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":39709,"date":"2018-02-01","createdTime":"2018-01-29 13:01:48","auditedTime":"2018-02-01 03:01:12","timeDelta":2,"createdHour":13,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":15239,"date":"2018-02-01","createdTime":"2018-01-31 02:05:22","auditedTime":"2018-02-01 03:01:12","timeDelta":1,"createdHour":2,"auditedHour":3,"feedback":["Clicked"],"target":0}][0m
[0m2021.07.12 18:13:27 INFO  Pushed Record for 2018-02-01 03:01:13: [{"metadata_ownerId":24260,"date":"2018-02-01","createdTime":"2018-02-01 02:33:16","auditedTime":"2018-02-01 03:01:13","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:29 INFO  Pushed Record for 2018-02-01 03:01:14: [{"metadata_ownerId":48745,"date":"2018-02-01","createdTime":"2018-01-27 00:43:55","auditedTime":"2018-02-01 03:01:14","timeDelta":5,"createdHour":0,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0}][0m
[0m2021.07.12 18:13:31 INFO  Pushed Record for 2018-02-01 03:01:15: [{"metadata_ownerId":45651,"date":"2018-02-01","createdTime":"2018-02-01 01:35:15","auditedTime":"2018-02-01 03:01:15","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":18127,"date":"2018-02-01","createdTime":"2018-02-01 02:05:15","auditedTime":"2018-02-01 03:01:15","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:32 INFO  Pushed Record for 2018-02-01 03:01:16: [{"metadata_ownerId":23461,"date":"2018-02-01","createdTime":"2018-02-01 02:50:14","auditedTime":"2018-02-01 03:01:16","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked"],"target":0}][0m
[0m2021.07.12 18:13:34 INFO  Pushed Record for 2018-02-01 03:01:17: [{"metadata_ownerId":37508,"date":"2018-02-01","createdTime":"2018-01-30 23:40:39","auditedTime":"2018-02-01 03:01:17","timeDelta":1,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:36 INFO  Pushed Record for 2018-02-01 03:01:18: [{"metadata_ownerId":22412,"date":"2018-02-01","createdTime":"2018-02-01 00:01:01","auditedTime":"2018-02-01 03:01:18","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:39 INFO  Pushed Record for 2018-02-01 03:01:20: [{"metadata_ownerId":30830,"date":"2018-02-01","createdTime":"2018-01-31 01:30:19","auditedTime":"2018-02-01 03:01:20","timeDelta":1,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":32469,"date":"2018-02-01","createdTime":"2018-01-31 22:15:33","auditedTime":"2018-02-01 03:01:20","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":51486,"date":"2018-02-01","createdTime":"2018-01-30 13:03:25","auditedTime":"2018-02-01 03:01:20","timeDelta":1,"createdHour":13,"auditedHour":3,"feedback":["Clicked","ReShared","Liked"],"target":1},{"metadata_ownerId":12639,"date":"2018-02-01","createdTime":"2018-01-30 22:35:16","auditedTime":"2018-02-01 03:01:20","timeDelta":1,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":15324,"date":"2018-02-01","createdTime":"2018-02-01 02:20:35","auditedTime":"2018-02-01 03:01:20","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":10031,"date":"2018-02-01","createdTime":"2018-02-01 02:06:27","auditedTime":"2018-02-01 03:01:20","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:13:41 INFO  Pushed Record for 2018-02-01 03:01:21: [{"metadata_ownerId":76306,"date":"2018-02-01","createdTime":"2018-01-29 20:30:12","auditedTime":"2018-02-01 03:01:21","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Clicked","Liked"],"target":1},{"metadata_ownerId":16086,"date":"2018-02-01","createdTime":"2018-02-01 02:22:53","auditedTime":"2018-02-01 03:01:21","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:13:43 INFO  Pushed Record for 2018-02-01 03:01:22: [{"metadata_ownerId":46228,"date":"2018-02-01","createdTime":"2018-01-31 23:49:11","auditedTime":"2018-02-01 03:01:22","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:45 INFO  Pushed Record for 2018-02-01 03:01:23: [{"metadata_ownerId":20093,"date":"2018-02-01","createdTime":"2018-01-31 21:25:10","auditedTime":"2018-02-01 03:01:23","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:46 INFO  Pushed Record for 2018-02-01 03:01:24: [{"metadata_ownerId":37463,"date":"2018-02-01","createdTime":"2018-02-01 01:33:54","auditedTime":"2018-02-01 03:01:24","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":44742,"date":"2018-02-01","createdTime":"2018-01-30 18:05:01","auditedTime":"2018-02-01 03:01:24","timeDelta":1,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":15793,"date":"2018-02-01","createdTime":"2018-01-28 17:33:18","auditedTime":"2018-02-01 03:01:24","timeDelta":3,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":18535,"date":"2018-02-01","createdTime":"2018-01-31 13:04:16","auditedTime":"2018-02-01 03:01:24","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":37822,"date":"2018-02-01","createdTime":"2018-01-31 20:05:10","auditedTime":"2018-02-01 03:01:24","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":36405,"date":"2018-02-01","createdTime":"2018-01-31 15:02:05","auditedTime":"2018-02-01 03:01:24","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:48 INFO  Pushed Record for 2018-02-01 03:01:25: [{"metadata_ownerId":24604,"date":"2018-02-01","createdTime":"2018-02-01 02:56:12","auditedTime":"2018-02-01 03:01:25","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":3333,"date":"2018-02-01","createdTime":"2018-02-01 00:36:29","auditedTime":"2018-02-01 03:01:25","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":14959,"date":"2018-02-01","createdTime":"2018-01-31 13:02:25","auditedTime":"2018-02-01 03:01:25","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:50 INFO  Pushed Record for 2018-02-01 03:01:26: [{"metadata_ownerId":18553,"date":"2018-02-01","createdTime":"2018-01-31 00:20:31","auditedTime":"2018-02-01 03:01:26","timeDelta":1,"createdHour":0,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":31557,"date":"2018-02-01","createdTime":"2018-02-01 00:45:22","auditedTime":"2018-02-01 03:01:26","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":60195,"date":"2018-02-01","createdTime":"2018-01-31 20:30:14","auditedTime":"2018-02-01 03:01:26","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:52 INFO  Pushed Record for 2018-02-01 03:01:27: [{"metadata_ownerId":47661,"date":"2018-02-01","createdTime":"2018-01-31 13:35:16","auditedTime":"2018-02-01 03:01:27","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:54 INFO  Pushed Record for 2018-02-01 03:01:28: [{"metadata_ownerId":39606,"date":"2018-02-01","createdTime":"2018-01-29 03:45:17","auditedTime":"2018-02-01 03:01:28","timeDelta":2,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":20019,"date":"2018-02-01","createdTime":"2018-01-31 18:45:16","auditedTime":"2018-02-01 03:01:28","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":46171,"date":"2018-02-01","createdTime":"2018-01-31 08:14:42","auditedTime":"2018-02-01 03:01:28","timeDelta":0,"createdHour":8,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0}][0m
[0m2021.07.12 18:13:56 INFO  Pushed Record for 2018-02-01 03:01:29: [{"metadata_ownerId":11101,"date":"2018-02-01","createdTime":"2018-01-31 15:39:28","auditedTime":"2018-02-01 03:01:29","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":59431,"date":"2018-02-01","createdTime":"2018-01-31 09:00:12","auditedTime":"2018-02-01 03:01:29","timeDelta":0,"createdHour":9,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":20813,"date":"2018-02-01","createdTime":"2018-01-31 12:04:24","auditedTime":"2018-02-01 03:01:29","timeDelta":0,"createdHour":12,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:13:57 INFO  Pushed Record for 2018-02-01 03:01:30: [{"metadata_ownerId":11222,"date":"2018-02-01","createdTime":"2018-01-31 15:30:52","auditedTime":"2018-02-01 03:01:30","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:01 INFO  Pushed Record for 2018-02-01 03:01:32: [{"metadata_ownerId":19224,"date":"2018-02-01","createdTime":"2018-01-31 22:24:53","auditedTime":"2018-02-01 03:01:32","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":70190,"date":"2018-02-01","createdTime":"2018-01-31 00:47:51","auditedTime":"2018-02-01 03:01:32","timeDelta":1,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:02 INFO  Pushed Record for 2018-02-01 03:01:33: [{"metadata_ownerId":16416,"date":"2018-02-01","createdTime":"2018-01-31 18:30:50","auditedTime":"2018-02-01 03:01:33","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":29500,"date":"2018-02-01","createdTime":"2018-02-01 01:12:28","auditedTime":"2018-02-01 03:01:33","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":15534,"date":"2018-02-01","createdTime":"2018-02-01 00:05:34","auditedTime":"2018-02-01 03:01:33","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:04 INFO  Pushed Record for 2018-02-01 03:01:34: [{"metadata_ownerId":60805,"date":"2018-02-01","createdTime":"2018-02-01 02:00:33","auditedTime":"2018-02-01 03:01:34","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":22575,"date":"2018-02-01","createdTime":"2018-02-01 00:54:05","auditedTime":"2018-02-01 03:01:34","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":71284,"date":"2018-02-01","createdTime":"2018-01-27 14:23:29","auditedTime":"2018-02-01 03:01:34","timeDelta":4,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:06 INFO  Pushed Record for 2018-02-01 03:01:35: [{"metadata_ownerId":7021,"date":"2018-02-01","createdTime":"2018-01-18 14:48:34","auditedTime":"2018-02-01 03:01:35","timeDelta":13,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:08 INFO  Pushed Record for 2018-02-01 03:01:36: [{"metadata_ownerId":20350,"date":"2018-02-01","createdTime":"2018-01-30 09:30:34","auditedTime":"2018-02-01 03:01:36","timeDelta":1,"createdHour":9,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:10 INFO  Pushed Record for 2018-02-01 03:01:37: [{"metadata_ownerId":25555,"date":"2018-02-01","createdTime":"2018-01-27 11:47:09","auditedTime":"2018-02-01 03:01:37","timeDelta":4,"createdHour":11,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:11 INFO  Pushed Record for 2018-02-01 03:01:38: [{"metadata_ownerId":29085,"date":"2018-02-01","createdTime":"2018-01-31 23:01:08","auditedTime":"2018-02-01 03:01:38","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":35810,"date":"2018-02-01","createdTime":"2018-02-01 00:36:05","auditedTime":"2018-02-01 03:01:38","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0},{"metadata_ownerId":40124,"date":"2018-02-01","createdTime":"2018-01-29 20:07:13","auditedTime":"2018-02-01 03:01:38","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":25770,"date":"2018-02-01","createdTime":"2018-02-01 00:30:05","auditedTime":"2018-02-01 03:01:38","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:13 INFO  Pushed Record for 2018-02-01 03:01:39: [{"metadata_ownerId":38588,"date":"2018-02-01","createdTime":"2018-02-01 01:22:15","auditedTime":"2018-02-01 03:01:39","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Clicked","Viewed"],"target":0},{"metadata_ownerId":44693,"date":"2018-02-01","createdTime":"2018-02-01 01:05:28","auditedTime":"2018-02-01 03:01:39","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":79090,"date":"2018-02-01","createdTime":"2018-01-31 21:32:35","auditedTime":"2018-02-01 03:01:39","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Clicked","Liked","Ignored"],"target":1},{"metadata_ownerId":838,"date":"2018-02-01","createdTime":"2018-01-29 08:19:15","auditedTime":"2018-02-01 03:01:39","timeDelta":2,"createdHour":8,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:15 INFO  Pushed Record for 2018-02-01 03:01:40: [{"metadata_ownerId":7139,"date":"2018-02-01","createdTime":"2018-02-01 01:35:14","auditedTime":"2018-02-01 03:01:40","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":12508,"date":"2018-02-01","createdTime":"2018-01-31 18:00:13","auditedTime":"2018-02-01 03:01:40","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Clicked","Liked"],"target":1},{"metadata_ownerId":24604,"date":"2018-02-01","createdTime":"2018-01-31 21:27:29","auditedTime":"2018-02-01 03:01:40","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":83218,"date":"2018-02-01","createdTime":"2018-01-30 08:00:17","auditedTime":"2018-02-01 03:01:40","timeDelta":1,"createdHour":8,"auditedHour":3,"feedback":["Clicked"],"target":0}][0m
[0m2021.07.12 18:14:17 INFO  Pushed Record for 2018-02-01 03:01:41: [{"metadata_ownerId":13821,"date":"2018-02-01","createdTime":"2018-01-31 19:02:40","auditedTime":"2018-02-01 03:01:41","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":38703,"date":"2018-02-01","createdTime":"2018-01-29 23:20:41","auditedTime":"2018-02-01 03:01:41","timeDelta":2,"createdHour":23,"auditedHour":3,"feedback":["Clicked"],"target":0}][0m
[0m2021.07.12 18:14:19 INFO  Pushed Record for 2018-02-01 03:01:42: [{"metadata_ownerId":82293,"date":"2018-02-01","createdTime":"2018-02-01 01:27:41","auditedTime":"2018-02-01 03:01:42","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":6732,"date":"2018-02-01","createdTime":"2018-02-01 02:01:28","auditedTime":"2018-02-01 03:01:42","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":80949,"date":"2018-02-01","createdTime":"2018-02-01 01:30:11","auditedTime":"2018-02-01 03:01:42","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:20 INFO  Pushed Record for 2018-02-01 03:01:43: [{"metadata_ownerId":20289,"date":"2018-02-01","createdTime":"2018-01-31 21:01:31","auditedTime":"2018-02-01 03:01:43","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":3443,"date":"2018-02-01","createdTime":"2018-02-01 01:01:54","auditedTime":"2018-02-01 03:01:43","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:14:22 INFO  Pushed Record for 2018-02-01 03:01:44: [{"metadata_ownerId":4034,"date":"2018-02-01","createdTime":"2018-01-31 22:06:08","auditedTime":"2018-02-01 03:01:44","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":17922,"date":"2018-02-01","createdTime":"2018-01-30 10:19:12","auditedTime":"2018-02-01 03:01:44","timeDelta":1,"createdHour":10,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":24604,"date":"2018-02-01","createdTime":"2018-02-01 02:56:12","auditedTime":"2018-02-01 03:01:44","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":67481,"date":"2018-02-01","createdTime":"2018-01-31 21:08:30","auditedTime":"2018-02-01 03:01:44","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:24 INFO  Pushed Record for 2018-02-01 03:01:45: [{"metadata_ownerId":39065,"date":"2018-02-01","createdTime":"2018-02-01 00:22:42","auditedTime":"2018-02-01 03:01:45","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:26 INFO  Pushed Record for 2018-02-01 03:01:46: [{"metadata_ownerId":12021,"date":"2018-02-01","createdTime":"2018-02-01 00:10:14","auditedTime":"2018-02-01 03:01:46","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":37648,"date":"2018-02-01","createdTime":"2018-02-01 00:00:30","auditedTime":"2018-02-01 03:01:46","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:28 INFO  Pushed Record for 2018-02-01 03:01:47: [{"metadata_ownerId":49070,"date":"2018-02-01","createdTime":"2018-01-30 02:38:43","auditedTime":"2018-02-01 03:01:47","timeDelta":2,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":16000,"date":"2018-02-01","createdTime":"2018-01-30 23:38:45","auditedTime":"2018-02-01 03:01:47","timeDelta":1,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":1184,"date":"2018-02-01","createdTime":"2018-02-01 03:01:29","auditedTime":"2018-02-01 03:01:47","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":18550,"date":"2018-02-01","createdTime":"2018-01-31 22:30:50","auditedTime":"2018-02-01 03:01:47","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":4894,"date":"2018-02-01","createdTime":"2018-01-31 14:00:02","auditedTime":"2018-02-01 03:01:47","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:29 INFO  Pushed Record for 2018-02-01 03:01:48: [{"metadata_ownerId":14148,"date":"2018-02-01","createdTime":"2018-01-31 18:06:31","auditedTime":"2018-02-01 03:01:48","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0},{"metadata_ownerId":79089,"date":"2018-02-01","createdTime":"2018-01-30 15:25:08","auditedTime":"2018-02-01 03:01:48","timeDelta":1,"createdHour":15,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0}][0m
[0m2021.07.12 18:14:31 INFO  Pushed Record for 2018-02-01 03:01:49: [{"metadata_ownerId":32973,"date":"2018-02-01","createdTime":"2018-01-31 20:05:13","auditedTime":"2018-02-01 03:01:49","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":582,"date":"2018-02-01","createdTime":"2018-01-29 20:35:52","auditedTime":"2018-02-01 03:01:49","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":13578,"date":"2018-02-01","createdTime":"2018-02-01 02:26:24","auditedTime":"2018-02-01 03:01:49","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:14:33 INFO  Pushed Record for 2018-02-01 03:01:50: [{"metadata_ownerId":67500,"date":"2018-02-01","createdTime":"2018-01-31 20:00:20","auditedTime":"2018-02-01 03:01:50","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:35 INFO  Pushed Record for 2018-02-01 03:01:51: [{"metadata_ownerId":536,"date":"2018-02-01","createdTime":"2018-01-24 14:00:42","auditedTime":"2018-02-01 03:01:51","timeDelta":7,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":63553,"date":"2018-02-01","createdTime":"2018-01-31 21:49:36","auditedTime":"2018-02-01 03:01:51","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":37112,"date":"2018-02-01","createdTime":"2018-02-01 01:37:09","auditedTime":"2018-02-01 03:01:51","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":19481,"date":"2018-02-01","createdTime":"2018-01-31 17:20:26","auditedTime":"2018-02-01 03:01:51","timeDelta":0,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:37 INFO  Pushed Record for 2018-02-01 03:01:52: [{"metadata_ownerId":70888,"date":"2018-02-01","createdTime":"2018-01-30 18:27:14","auditedTime":"2018-02-01 03:01:52","timeDelta":1,"createdHour":18,"auditedHour":3,"feedback":["Clicked","Liked"],"target":1},{"metadata_ownerId":25411,"date":"2018-02-01","createdTime":"2018-01-29 10:20:08","auditedTime":"2018-02-01 03:01:52","timeDelta":2,"createdHour":10,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:14:39 INFO  Pushed Record for 2018-02-01 03:01:53: [{"metadata_ownerId":33465,"date":"2018-02-01","createdTime":"2018-01-31 18:13:30","auditedTime":"2018-02-01 03:01:53","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":61557,"date":"2018-02-01","createdTime":"2018-01-31 23:30:04","auditedTime":"2018-02-01 03:01:53","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:14:40 INFO  Pushed Record for 2018-02-01 03:01:54: [{"metadata_ownerId":30830,"date":"2018-02-01","createdTime":"2018-01-31 21:01:35","auditedTime":"2018-02-01 03:01:54","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":35205,"date":"2018-02-01","createdTime":"2018-01-31 02:01:26","auditedTime":"2018-02-01 03:01:54","timeDelta":1,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:42 INFO  Pushed Record for 2018-02-01 03:01:55: [{"metadata_ownerId":73239,"date":"2018-02-01","createdTime":"2018-01-30 09:48:39","auditedTime":"2018-02-01 03:01:55","timeDelta":1,"createdHour":9,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":3239,"date":"2018-02-01","createdTime":"2018-01-31 23:23:46","auditedTime":"2018-02-01 03:01:55","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:44 INFO  Pushed Record for 2018-02-01 03:01:56: [{"metadata_ownerId":582,"date":"2018-02-01","createdTime":"2018-01-24 03:20:07","auditedTime":"2018-02-01 03:01:56","timeDelta":7,"createdHour":3,"auditedHour":3,"feedback":["Liked","Ignored"],"target":1},{"metadata_ownerId":33357,"date":"2018-02-01","createdTime":"2018-02-01 01:57:26","auditedTime":"2018-02-01 03:01:56","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:46 INFO  Pushed Record for 2018-02-01 03:01:57: [{"metadata_ownerId":61558,"date":"2018-02-01","createdTime":"2018-02-01 02:00:34","auditedTime":"2018-02-01 03:01:57","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":41149,"date":"2018-02-01","createdTime":"2018-01-30 11:56:05","auditedTime":"2018-02-01 03:01:57","timeDelta":1,"createdHour":11,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":12164,"date":"2018-02-01","createdTime":"2018-01-30 15:22:52","auditedTime":"2018-02-01 03:01:57","timeDelta":1,"createdHour":15,"auditedHour":3,"feedback":["Clicked"],"target":0}][0m
[0m2021.07.12 18:14:48 INFO  Pushed Record for 2018-02-01 03:01:58: [{"metadata_ownerId":79442,"date":"2018-02-01","createdTime":"2018-01-30 14:10:15","auditedTime":"2018-02-01 03:01:58","timeDelta":1,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":39485,"date":"2018-02-01","createdTime":"2018-02-01 01:05:38","auditedTime":"2018-02-01 03:01:58","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:49 INFO  Pushed Record for 2018-02-01 03:01:59: [{"metadata_ownerId":9236,"date":"2018-02-01","createdTime":"2018-02-01 02:18:21","auditedTime":"2018-02-01 03:01:59","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":79861,"date":"2018-02-01","createdTime":"2018-01-28 14:04:20","auditedTime":"2018-02-01 03:01:59","timeDelta":3,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:51 INFO  Pushed Record for 2018-02-01 03:02:00: [{"metadata_ownerId":11666,"date":"2018-02-01","createdTime":"2018-02-01 02:23:24","auditedTime":"2018-02-01 03:02:00","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:53 INFO  Pushed Record for 2018-02-01 03:02:01: [{"metadata_ownerId":62199,"date":"2018-02-01","createdTime":"2018-01-31 21:35:06","auditedTime":"2018-02-01 03:02:01","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":80035,"date":"2018-02-01","createdTime":"2018-01-23 20:50:09","auditedTime":"2018-02-01 03:02:01","timeDelta":8,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":4630,"date":"2018-02-01","createdTime":"2018-01-31 23:35:10","auditedTime":"2018-02-01 03:02:01","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:14:55 INFO  Pushed Record for 2018-02-01 03:02:02: [{"metadata_ownerId":44059,"date":"2018-02-01","createdTime":"2018-01-31 22:05:22","auditedTime":"2018-02-01 03:02:02","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Clicked","Liked"],"target":1},{"metadata_ownerId":25555,"date":"2018-02-01","createdTime":"2018-01-29 13:54:15","auditedTime":"2018-02-01 03:02:02","timeDelta":2,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:14:58 INFO  Pushed Record for 2018-02-01 03:02:04: [{"metadata_ownerId":44198,"date":"2018-02-01","createdTime":"2018-01-31 22:11:38","auditedTime":"2018-02-01 03:02:04","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["ReShared"],"target":0},{"metadata_ownerId":47420,"date":"2018-02-01","createdTime":"2018-01-29 14:10:26","auditedTime":"2018-02-01 03:02:04","timeDelta":2,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:01 INFO  Pushed Record for 2018-02-01 03:02:06: [{"metadata_ownerId":74315,"date":"2018-02-01","createdTime":"2018-01-31 14:00:17","auditedTime":"2018-02-01 03:02:06","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Clicked"],"target":0}][0m
[0m2021.07.12 18:15:04 INFO  Pushed Record for 2018-02-01 03:02:08: [{"metadata_ownerId":3639,"date":"2018-02-01","createdTime":"2018-01-29 09:01:01","auditedTime":"2018-02-01 03:02:08","timeDelta":2,"createdHour":9,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":20220,"date":"2018-02-01","createdTime":"2018-02-01 02:40:14","auditedTime":"2018-02-01 03:02:08","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":15576,"date":"2018-02-01","createdTime":"2018-01-31 13:02:19","auditedTime":"2018-02-01 03:02:08","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":27891,"date":"2018-02-01","createdTime":"2018-01-31 08:04:09","auditedTime":"2018-02-01 03:02:08","timeDelta":0,"createdHour":8,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:06 INFO  Pushed Record for 2018-02-01 03:02:09: [{"metadata_ownerId":83733,"date":"2018-02-01","createdTime":"2018-01-29 22:30:12","auditedTime":"2018-02-01 03:02:09","timeDelta":2,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:08 INFO  Pushed Record for 2018-02-01 03:02:10: [{"metadata_ownerId":43516,"date":"2018-02-01","createdTime":"2018-01-31 23:01:41","auditedTime":"2018-02-01 03:02:10","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":37445,"date":"2018-02-01","createdTime":"2018-02-01 03:00:39","auditedTime":"2018-02-01 03:02:10","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:15:10 INFO  Pushed Record for 2018-02-01 03:02:11: [{"metadata_ownerId":59153,"date":"2018-02-01","createdTime":"2018-01-31 01:58:40","auditedTime":"2018-02-01 03:02:11","timeDelta":1,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":17106,"date":"2018-02-01","createdTime":"2018-01-31 13:06:10","auditedTime":"2018-02-01 03:02:11","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:12 INFO  Pushed Record for 2018-02-01 03:02:12: [{"metadata_ownerId":19131,"date":"2018-02-01","createdTime":"2018-02-01 01:12:22","auditedTime":"2018-02-01 03:02:12","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":373,"date":"2018-02-01","createdTime":"2018-01-31 13:02:29","auditedTime":"2018-02-01 03:02:12","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Clicked","ReShared","Liked"],"target":1}][0m
[0m2021.07.12 18:15:13 INFO  Pushed Record for 2018-02-01 03:02:13: [{"metadata_ownerId":59890,"date":"2018-02-01","createdTime":"2018-01-31 23:23:34","auditedTime":"2018-02-01 03:02:13","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":16571,"date":"2018-02-01","createdTime":"2018-01-31 17:33:55","auditedTime":"2018-02-01 03:02:13","timeDelta":0,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":15987,"date":"2018-02-01","createdTime":"2018-01-31 15:01:57","auditedTime":"2018-02-01 03:02:13","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:15 INFO  Pushed Record for 2018-02-01 03:02:14: [{"metadata_ownerId":77938,"date":"2018-02-01","createdTime":"2018-01-30 10:04:12","auditedTime":"2018-02-01 03:02:14","timeDelta":1,"createdHour":10,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":79057,"date":"2018-02-01","createdTime":"2018-01-30 02:22:46","auditedTime":"2018-02-01 03:02:14","timeDelta":2,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":49277,"date":"2018-02-01","createdTime":"2018-01-31 01:10:58","auditedTime":"2018-02-01 03:02:14","timeDelta":1,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":68033,"date":"2018-02-01","createdTime":"2018-01-31 18:30:08","auditedTime":"2018-02-01 03:02:14","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:15:17 INFO  Pushed Record for 2018-02-01 03:02:15: [{"metadata_ownerId":6518,"date":"2018-02-01","createdTime":"2018-01-31 22:14:31","auditedTime":"2018-02-01 03:02:15","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":74777,"date":"2018-02-01","createdTime":"2018-01-31 23:34:55","auditedTime":"2018-02-01 03:02:15","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0},{"metadata_ownerId":51577,"date":"2018-02-01","createdTime":"2018-01-30 17:05:16","auditedTime":"2018-02-01 03:02:15","timeDelta":1,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":67779,"date":"2018-02-01","createdTime":"2018-01-31 22:35:19","auditedTime":"2018-02-01 03:02:15","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":84516,"date":"2018-02-01","createdTime":"2018-01-30 19:59:07","auditedTime":"2018-02-01 03:02:15","timeDelta":1,"createdHour":19,"auditedHour":3,"feedback":["Liked","Ignored"],"target":1},{"metadata_ownerId":59334,"date":"2018-02-01","createdTime":"2018-01-31 16:01:41","auditedTime":"2018-02-01 03:02:15","timeDelta":0,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:18 INFO  Pushed Record for 2018-02-01 03:02:16: [{"metadata_ownerId":46791,"date":"2018-02-01","createdTime":"2018-02-01 01:01:49","auditedTime":"2018-02-01 03:02:16","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":66769,"date":"2018-02-01","createdTime":"2018-01-30 10:15:06","auditedTime":"2018-02-01 03:02:16","timeDelta":1,"createdHour":10,"auditedHour":3,"feedback":["Clicked","Liked"],"target":1},{"metadata_ownerId":79308,"date":"2018-02-01","createdTime":"2018-01-31 23:20:23","auditedTime":"2018-02-01 03:02:16","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:20 INFO  Pushed Record for 2018-02-01 03:02:17: [{"metadata_ownerId":32899,"date":"2018-02-01","createdTime":"2016-12-14 01:08:40","auditedTime":"2018-02-01 03:02:17","timeDelta":414,"createdHour":1,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":47273,"date":"2018-02-01","createdTime":"2018-02-01 02:22:04","auditedTime":"2018-02-01 03:02:17","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":18670,"date":"2018-02-01","createdTime":"2018-01-31 15:10:27","auditedTime":"2018-02-01 03:02:17","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":11222,"date":"2018-02-01","createdTime":"2018-01-31 15:30:52","auditedTime":"2018-02-01 03:02:17","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:22 INFO  Pushed Record for 2018-02-01 03:02:18: [{"metadata_ownerId":68109,"date":"2018-02-01","createdTime":"2018-01-31 16:00:09","auditedTime":"2018-02-01 03:02:18","timeDelta":0,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:24 INFO  Pushed Record for 2018-02-01 03:02:19: [{"metadata_ownerId":79245,"date":"2018-02-01","createdTime":"2018-01-31 06:54:29","auditedTime":"2018-02-01 03:02:19","timeDelta":0,"createdHour":6,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":12730,"date":"2018-02-01","createdTime":"2018-01-31 09:01:33","auditedTime":"2018-02-01 03:02:19","timeDelta":0,"createdHour":9,"auditedHour":3,"feedback":["Clicked","Liked","Ignored"],"target":1},{"metadata_ownerId":34048,"date":"2018-02-01","createdTime":"2018-01-31 20:41:14","auditedTime":"2018-02-01 03:02:19","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":7298,"date":"2018-02-01","createdTime":"2018-01-31 20:57:58","auditedTime":"2018-02-01 03:02:19","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":19621,"date":"2018-02-01","createdTime":"2018-01-31 23:12:03","auditedTime":"2018-02-01 03:02:19","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:15:27 INFO  Pushed Record for 2018-02-01 03:02:21: [{"metadata_ownerId":79785,"date":"2018-02-01","createdTime":"2018-02-01 03:00:30","auditedTime":"2018-02-01 03:02:21","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":57392,"date":"2018-02-01","createdTime":"2018-01-30 22:48:49","auditedTime":"2018-02-01 03:02:21","timeDelta":1,"createdHour":22,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:15:29 INFO  Pushed Record for 2018-02-01 03:02:22: [{"metadata_ownerId":6065,"date":"2018-02-01","createdTime":"2018-02-01 01:04:13","auditedTime":"2018-02-01 03:02:22","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":37535,"date":"2018-02-01","createdTime":"2018-01-31 18:19:04","auditedTime":"2018-02-01 03:02:22","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:31 INFO  Pushed Record for 2018-02-01 03:02:23: [{"metadata_ownerId":40471,"date":"2018-02-01","createdTime":"2018-02-01 00:49:08","auditedTime":"2018-02-01 03:02:23","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":38258,"date":"2018-02-01","createdTime":"2018-02-01 01:02:40","auditedTime":"2018-02-01 03:02:23","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":9182,"date":"2018-02-01","createdTime":"2018-02-01 00:20:26","auditedTime":"2018-02-01 03:02:23","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:32 INFO  Pushed Record for 2018-02-01 03:02:24: [{"metadata_ownerId":20405,"date":"2018-02-01","createdTime":"2018-01-26 13:02:48","auditedTime":"2018-02-01 03:02:24","timeDelta":5,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":15569,"date":"2018-02-01","createdTime":"2018-01-31 21:03:46","auditedTime":"2018-02-01 03:02:24","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":6319,"date":"2018-02-01","createdTime":"2018-01-31 14:01:43","auditedTime":"2018-02-01 03:02:24","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:34 INFO  Pushed Record for 2018-02-01 03:02:25: [{"metadata_ownerId":19621,"date":"2018-02-01","createdTime":"2018-01-31 21:15:12","auditedTime":"2018-02-01 03:02:25","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:15:36 INFO  Pushed Record for 2018-02-01 03:02:26: [{"metadata_ownerId":32839,"date":"2018-02-01","createdTime":"2018-01-31 23:04:57","auditedTime":"2018-02-01 03:02:26","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":22293,"date":"2018-02-01","createdTime":"2018-01-31 20:12:08","auditedTime":"2018-02-01 03:02:26","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":57362,"date":"2018-02-01","createdTime":"2018-02-01 01:45:04","auditedTime":"2018-02-01 03:02:26","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":67111,"date":"2018-02-01","createdTime":"2018-01-31 08:00:04","auditedTime":"2018-02-01 03:02:26","timeDelta":0,"createdHour":8,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:38 INFO  Pushed Record for 2018-02-01 03:02:27: [{"metadata_ownerId":35699,"date":"2018-02-01","createdTime":"2018-01-31 14:45:12","auditedTime":"2018-02-01 03:02:27","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":36134,"date":"2018-02-01","createdTime":"2018-01-30 07:38:24","auditedTime":"2018-02-01 03:02:27","timeDelta":1,"createdHour":7,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":44564,"date":"2018-02-01","createdTime":"2018-01-30 05:26:31","auditedTime":"2018-02-01 03:02:27","timeDelta":1,"createdHour":5,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":36405,"date":"2018-02-01","createdTime":"2018-01-31 21:54:04","auditedTime":"2018-02-01 03:02:27","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:15:39 INFO  Pushed Record for 2018-02-01 03:02:28: [{"metadata_ownerId":76560,"date":"2018-02-01","createdTime":"2018-01-31 13:08:02","auditedTime":"2018-02-01 03:02:28","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":18720,"date":"2018-02-01","createdTime":"2018-02-01 02:10:39","auditedTime":"2018-02-01 03:02:28","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:41 INFO  Pushed Record for 2018-02-01 03:02:29: [{"metadata_ownerId":50795,"date":"2018-02-01","createdTime":"2018-01-30 22:40:17","auditedTime":"2018-02-01 03:02:29","timeDelta":1,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":25161,"date":"2018-02-01","createdTime":"2018-02-01 00:03:09","auditedTime":"2018-02-01 03:02:29","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":46942,"date":"2018-02-01","createdTime":"2018-01-28 16:03:45","auditedTime":"2018-02-01 03:02:29","timeDelta":3,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:43 INFO  Pushed Record for 2018-02-01 03:02:30: [{"metadata_ownerId":77398,"date":"2018-02-01","createdTime":"2018-01-31 19:28:16","auditedTime":"2018-02-01 03:02:30","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":68077,"date":"2018-02-01","createdTime":"2018-01-31 19:00:26","auditedTime":"2018-02-01 03:02:30","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":70190,"date":"2018-02-01","createdTime":"2018-02-01 02:49:43","auditedTime":"2018-02-01 03:02:30","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked","Liked"],"target":1},{"metadata_ownerId":21039,"date":"2018-02-01","createdTime":"2018-01-31 21:21:06","auditedTime":"2018-02-01 03:02:30","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":80195,"date":"2018-02-01","createdTime":"2018-01-30 12:23:09","auditedTime":"2018-02-01 03:02:30","timeDelta":1,"createdHour":12,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:45 INFO  Pushed Record for 2018-02-01 03:02:31: [{"metadata_ownerId":26843,"date":"2018-02-01","createdTime":"2018-01-31 19:05:36","auditedTime":"2018-02-01 03:02:31","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:47 INFO  Pushed Record for 2018-02-01 03:02:32: [{"metadata_ownerId":18112,"date":"2018-02-01","createdTime":"2018-01-31 02:52:04","auditedTime":"2018-02-01 03:02:32","timeDelta":1,"createdHour":2,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":34407,"date":"2018-02-01","createdTime":"2018-02-01 00:25:49","auditedTime":"2018-02-01 03:02:32","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:48 INFO  Pushed Record for 2018-02-01 03:02:33: [{"metadata_ownerId":8183,"date":"2018-02-01","createdTime":"2018-01-30 16:54:35","auditedTime":"2018-02-01 03:02:33","timeDelta":1,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":46939,"date":"2018-02-01","createdTime":"2018-01-31 04:00:53","auditedTime":"2018-02-01 03:02:33","timeDelta":0,"createdHour":4,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:15:50 INFO  Pushed Record for 2018-02-01 03:02:34: [{"metadata_ownerId":21904,"date":"2018-02-01","createdTime":"2018-01-31 10:30:04","auditedTime":"2018-02-01 03:02:34","timeDelta":0,"createdHour":10,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":69102,"date":"2018-02-01","createdTime":"2018-01-31 12:35:44","auditedTime":"2018-02-01 03:02:34","timeDelta":0,"createdHour":12,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:52 INFO  Pushed Record for 2018-02-01 03:02:35: [{"metadata_ownerId":56603,"date":"2018-02-01","createdTime":"2018-01-29 09:04:53","auditedTime":"2018-02-01 03:02:35","timeDelta":2,"createdHour":9,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":10621,"date":"2018-02-01","createdTime":"2018-01-31 20:05:10","auditedTime":"2018-02-01 03:02:35","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":38703,"date":"2018-02-01","createdTime":"2018-01-31 13:10:40","auditedTime":"2018-02-01 03:02:35","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":22347,"date":"2018-02-01","createdTime":"2018-01-31 15:03:14","auditedTime":"2018-02-01 03:02:35","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:54 INFO  Pushed Record for 2018-02-01 03:02:36: [{"metadata_ownerId":14930,"date":"2018-02-01","createdTime":"2018-01-29 19:05:42","auditedTime":"2018-02-01 03:02:36","timeDelta":2,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:55 INFO  Pushed Record for 2018-02-01 03:02:37: [{"metadata_ownerId":79133,"date":"2018-02-01","createdTime":"2018-01-30 09:00:35","auditedTime":"2018-02-01 03:02:37","timeDelta":1,"createdHour":9,"auditedHour":3,"feedback":["Clicked","Liked"],"target":1},{"metadata_ownerId":36830,"date":"2018-02-01","createdTime":"2018-01-30 19:25:09","auditedTime":"2018-02-01 03:02:37","timeDelta":1,"createdHour":19,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":13617,"date":"2018-02-01","createdTime":"2018-01-31 11:27:27","auditedTime":"2018-02-01 03:02:37","timeDelta":0,"createdHour":11,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":36931,"date":"2018-02-01","createdTime":"2018-01-31 15:36:09","auditedTime":"2018-02-01 03:02:37","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:57 INFO  Pushed Record for 2018-02-01 03:02:38: [{"metadata_ownerId":16216,"date":"2018-02-01","createdTime":"2018-01-31 18:15:35","auditedTime":"2018-02-01 03:02:38","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":50738,"date":"2018-02-01","createdTime":"2018-02-01 00:05:35","auditedTime":"2018-02-01 03:02:38","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:15:59 INFO  Pushed Record for 2018-02-01 03:02:39: [{"metadata_ownerId":52312,"date":"2018-02-01","createdTime":"2018-01-31 19:30:30","auditedTime":"2018-02-01 03:02:39","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":34777,"date":"2018-02-01","createdTime":"2018-01-31 17:31:02","auditedTime":"2018-02-01 03:02:39","timeDelta":0,"createdHour":17,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":66769,"date":"2018-02-01","createdTime":"2018-02-01 02:15:07","auditedTime":"2018-02-01 03:02:39","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["ReShared"],"target":0},{"metadata_ownerId":68049,"date":"2018-02-01","createdTime":"2018-01-31 19:40:05","auditedTime":"2018-02-01 03:02:39","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:01 INFO  Pushed Record for 2018-02-01 03:02:40: [{"metadata_ownerId":37473,"date":"2018-02-01","createdTime":"2018-01-31 18:35:16","auditedTime":"2018-02-01 03:02:40","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":6734,"date":"2018-02-01","createdTime":"2018-01-31 17:35:40","auditedTime":"2018-02-01 03:02:40","timeDelta":0,"createdHour":17,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":21589,"date":"2018-02-01","createdTime":"2018-02-01 00:22:29","auditedTime":"2018-02-01 03:02:40","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Disliked","Clicked","Liked"],"target":1},{"metadata_ownerId":53119,"date":"2018-02-01","createdTime":"2018-01-31 20:41:28","auditedTime":"2018-02-01 03:02:40","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Disliked","Liked","Ignored"],"target":1},{"metadata_ownerId":39446,"date":"2018-02-01","createdTime":"2018-01-29 15:28:17","auditedTime":"2018-02-01 03:02:40","timeDelta":2,"createdHour":15,"auditedHour":3,"feedback":["Disliked","Liked"],"target":1},{"metadata_ownerId":36954,"date":"2018-02-01","createdTime":"2018-01-31 16:20:11","auditedTime":"2018-02-01 03:02:40","timeDelta":0,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":72241,"date":"2018-02-01","createdTime":"2018-01-29 20:01:40","auditedTime":"2018-02-01 03:02:40","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":83192,"date":"2018-02-01","createdTime":"2018-01-31 22:06:23","auditedTime":"2018-02-01 03:02:40","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:16:05 INFO  Pushed Record for 2018-02-01 03:02:43: [{"metadata_ownerId":5436,"date":"2018-02-01","createdTime":"2018-01-31 22:05:58","auditedTime":"2018-02-01 03:02:43","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":32010,"date":"2018-02-01","createdTime":"2018-01-31 17:15:35","auditedTime":"2018-02-01 03:02:43","timeDelta":0,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:07 INFO  Pushed Record for 2018-02-01 03:02:44: [{"metadata_ownerId":5722,"date":"2018-02-01","createdTime":"2018-02-01 02:04:05","auditedTime":"2018-02-01 03:02:44","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":46062,"date":"2018-02-01","createdTime":"2018-01-31 22:19:29","auditedTime":"2018-02-01 03:02:44","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":50524,"date":"2018-02-01","createdTime":"2018-01-30 14:15:21","auditedTime":"2018-02-01 03:02:44","timeDelta":1,"createdHour":14,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:16:09 INFO  Pushed Record for 2018-02-01 03:02:45: [{"metadata_ownerId":60195,"date":"2018-02-01","createdTime":"2018-01-31 22:00:34","auditedTime":"2018-02-01 03:02:45","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":77398,"date":"2018-02-01","createdTime":"2018-01-31 19:20:14","auditedTime":"2018-02-01 03:02:45","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":64879,"date":"2018-02-01","createdTime":"2018-01-31 19:10:32","auditedTime":"2018-02-01 03:02:45","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:11 INFO  Pushed Record for 2018-02-01 03:02:46: [{"metadata_ownerId":27636,"date":"2018-02-01","createdTime":"2017-10-19 14:15:26","auditedTime":"2018-02-01 03:02:46","timeDelta":104,"createdHour":14,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":27966,"date":"2018-02-01","createdTime":"2018-02-01 00:33:09","auditedTime":"2018-02-01 03:02:46","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":46552,"date":"2018-02-01","createdTime":"2018-01-31 23:10:22","auditedTime":"2018-02-01 03:02:46","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:12 INFO  Pushed Record for 2018-02-01 03:02:47: [{"metadata_ownerId":29613,"date":"2018-02-01","createdTime":"2018-01-31 01:06:09","auditedTime":"2018-02-01 03:02:47","timeDelta":1,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":4871,"date":"2018-02-01","createdTime":"2018-01-31 00:58:42","auditedTime":"2018-02-01 03:02:47","timeDelta":1,"createdHour":0,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":25085,"date":"2018-02-01","createdTime":"2018-01-31 22:06:25","auditedTime":"2018-02-01 03:02:47","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:14 INFO  Pushed Record for 2018-02-01 03:02:48: [{"metadata_ownerId":81707,"date":"2018-02-01","createdTime":"2018-01-31 02:00:26","auditedTime":"2018-02-01 03:02:48","timeDelta":1,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":23043,"date":"2018-02-01","createdTime":"2018-01-31 12:16:14","auditedTime":"2018-02-01 03:02:48","timeDelta":0,"createdHour":12,"auditedHour":3,"feedback":["Commented","Liked"],"target":1},{"metadata_ownerId":73910,"date":"2018-02-01","createdTime":"2018-02-01 02:18:30","auditedTime":"2018-02-01 03:02:48","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":48421,"date":"2018-02-01","createdTime":"2018-02-01 02:39:49","auditedTime":"2018-02-01 03:02:48","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":51780,"date":"2018-02-01","createdTime":"2018-02-01 02:44:39","auditedTime":"2018-02-01 03:02:48","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:16 INFO  Pushed Record for 2018-02-01 03:02:49: [{"metadata_ownerId":32072,"date":"2018-02-01","createdTime":"2018-01-31 23:30:35","auditedTime":"2018-02-01 03:02:49","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":21245,"date":"2018-02-01","createdTime":"2018-02-01 02:36:03","auditedTime":"2018-02-01 03:02:49","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":16100,"date":"2018-02-01","createdTime":"2018-02-01 02:48:58","auditedTime":"2018-02-01 03:02:49","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:18 INFO  Pushed Record for 2018-02-01 03:02:50: [{"metadata_ownerId":18375,"date":"2018-02-01","createdTime":"2018-01-30 19:40:15","auditedTime":"2018-02-01 03:02:50","timeDelta":1,"createdHour":19,"auditedHour":3,"feedback":["Clicked","Viewed","Liked"],"target":1},{"metadata_ownerId":3110,"date":"2018-02-01","createdTime":"2018-01-30 19:01:26","auditedTime":"2018-02-01 03:02:50","timeDelta":1,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":74086,"date":"2018-02-01","createdTime":"2018-01-29 13:16:05","auditedTime":"2018-02-01 03:02:50","timeDelta":2,"createdHour":13,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:16:19 INFO  Pushed Record for 2018-02-01 03:02:51: [{"metadata_ownerId":68925,"date":"2018-02-01","createdTime":"2018-01-30 09:50:05","auditedTime":"2018-02-01 03:02:51","timeDelta":1,"createdHour":9,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":42701,"date":"2018-02-01","createdTime":"2018-01-31 09:30:35","auditedTime":"2018-02-01 03:02:51","timeDelta":0,"createdHour":9,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":68049,"date":"2018-02-01","createdTime":"2018-01-30 21:15:05","auditedTime":"2018-02-01 03:02:51","timeDelta":1,"createdHour":21,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":32899,"date":"2018-02-01","createdTime":"2018-01-31 03:00:11","auditedTime":"2018-02-01 03:02:51","timeDelta":1,"createdHour":3,"auditedHour":3,"feedback":["Disliked","Liked"],"target":1},{"metadata_ownerId":17215,"date":"2018-02-01","createdTime":"2018-01-31 14:35:19","auditedTime":"2018-02-01 03:02:51","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":41293,"date":"2018-02-01","createdTime":"2018-01-31 19:05:21","auditedTime":"2018-02-01 03:02:51","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:21 INFO  Pushed Record for 2018-02-01 03:02:52: [{"metadata_ownerId":78196,"date":"2018-02-01","createdTime":"2018-01-28 22:05:24","auditedTime":"2018-02-01 03:02:52","timeDelta":3,"createdHour":22,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":6012,"date":"2018-02-01","createdTime":"2018-01-30 19:05:40","auditedTime":"2018-02-01 03:02:52","timeDelta":1,"createdHour":19,"auditedHour":3,"feedback":["Clicked","Liked"],"target":1}][0m
[0m2021.07.12 18:16:23 INFO  Pushed Record for 2018-02-01 03:02:53: [{"metadata_ownerId":39591,"date":"2018-02-01","createdTime":"2018-01-30 01:24:47","auditedTime":"2018-02-01 03:02:53","timeDelta":2,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":20648,"date":"2018-02-01","createdTime":"2018-01-31 21:10:30","auditedTime":"2018-02-01 03:02:53","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Clicked"],"target":0}][0m
[0m2021.07.12 18:16:25 INFO  Pushed Record for 2018-02-01 03:02:54: [{"metadata_ownerId":34239,"date":"2018-02-01","createdTime":"2018-01-31 08:01:03","auditedTime":"2018-02-01 03:02:54","timeDelta":0,"createdHour":8,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":18666,"date":"2018-02-01","createdTime":"2018-01-31 02:10:33","auditedTime":"2018-02-01 03:02:54","timeDelta":1,"createdHour":2,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0},{"metadata_ownerId":35734,"date":"2018-02-01","createdTime":"2018-02-01 01:50:19","auditedTime":"2018-02-01 03:02:54","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":79181,"date":"2018-02-01","createdTime":"2018-02-01 00:54:02","auditedTime":"2018-02-01 03:02:54","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:16:27 INFO  Pushed Record for 2018-02-01 03:02:55: [{"metadata_ownerId":53942,"date":"2018-02-01","createdTime":"2018-01-30 17:46:12","auditedTime":"2018-02-01 03:02:55","timeDelta":1,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":5077,"date":"2018-02-01","createdTime":"2018-02-01 00:29:52","auditedTime":"2018-02-01 03:02:55","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:28 INFO  Pushed Record for 2018-02-01 03:02:56: [{"metadata_ownerId":5049,"date":"2018-02-01","createdTime":"2018-01-31 22:16:22","auditedTime":"2018-02-01 03:02:56","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:30 INFO  Pushed Record for 2018-02-01 03:02:57: [{"metadata_ownerId":5296,"date":"2018-02-01","createdTime":"2018-01-31 20:46:10","auditedTime":"2018-02-01 03:02:57","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":33400,"date":"2018-02-01","createdTime":"2018-01-30 18:30:34","auditedTime":"2018-02-01 03:02:57","timeDelta":1,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":10617,"date":"2018-02-01","createdTime":"2018-01-31 14:35:04","auditedTime":"2018-02-01 03:02:57","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:32 INFO  Pushed Record for 2018-02-01 03:02:58: [{"metadata_ownerId":46277,"date":"2018-02-01","createdTime":"2018-02-01 02:00:51","auditedTime":"2018-02-01 03:02:58","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:34 INFO  Pushed Record for 2018-02-01 03:02:59: [{"metadata_ownerId":62886,"date":"2018-02-01","createdTime":"2018-02-01 02:50:58","auditedTime":"2018-02-01 03:02:59","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:36 INFO  Pushed Record for 2018-02-01 03:03:00: [{"metadata_ownerId":54390,"date":"2018-02-01","createdTime":"2018-01-31 03:50:20","auditedTime":"2018-02-01 03:03:00","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":14770,"date":"2018-02-01","createdTime":"2018-01-27 22:11:06","auditedTime":"2018-02-01 03:03:00","timeDelta":4,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":34537,"date":"2018-02-01","createdTime":"2018-01-31 16:42:07","auditedTime":"2018-02-01 03:03:00","timeDelta":0,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":77278,"date":"2018-02-01","createdTime":"2018-01-31 16:26:19","auditedTime":"2018-02-01 03:03:00","timeDelta":0,"createdHour":16,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:16:37 INFO  Pushed Record for 2018-02-01 03:03:01: [{"metadata_ownerId":37063,"date":"2018-02-01","createdTime":"2018-01-28 22:51:04","auditedTime":"2018-02-01 03:03:01","timeDelta":3,"createdHour":22,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":79061,"date":"2018-02-01","createdTime":"2018-01-31 02:00:15","auditedTime":"2018-02-01 03:03:01","timeDelta":1,"createdHour":2,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":15268,"date":"2018-02-01","createdTime":"2018-01-31 23:52:19","auditedTime":"2018-02-01 03:03:01","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:39 INFO  Pushed Record for 2018-02-01 03:03:02: [{"metadata_ownerId":48580,"date":"2018-02-01","createdTime":"2018-01-29 12:36:09","auditedTime":"2018-02-01 03:03:02","timeDelta":2,"createdHour":12,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:41 INFO  Pushed Record for 2018-02-01 03:03:03: [{"metadata_ownerId":11831,"date":"2018-02-01","createdTime":"2018-02-01 02:20:18","auditedTime":"2018-02-01 03:03:03","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":12680,"date":"2018-02-01","createdTime":"2018-02-01 02:46:10","auditedTime":"2018-02-01 03:03:03","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":20958,"date":"2018-02-01","createdTime":"2018-01-28 16:34:07","auditedTime":"2018-02-01 03:03:03","timeDelta":3,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:43 INFO  Pushed Record for 2018-02-01 03:03:04: [{"metadata_ownerId":18870,"date":"2018-02-01","createdTime":"2018-01-31 21:06:10","auditedTime":"2018-02-01 03:03:04","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":1404,"date":"2018-02-01","createdTime":"2018-01-31 23:04:17","auditedTime":"2018-02-01 03:03:04","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":42585,"date":"2018-02-01","createdTime":"2018-01-31 20:01:27","auditedTime":"2018-02-01 03:03:04","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Clicked"],"target":0}][0m
[0m2021.07.12 18:16:45 INFO  Pushed Record for 2018-02-01 03:03:05: [{"metadata_ownerId":61557,"date":"2018-02-01","createdTime":"2018-01-31 18:30:04","auditedTime":"2018-02-01 03:03:05","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":60772,"date":"2018-02-01","createdTime":"2018-01-31 21:35:07","auditedTime":"2018-02-01 03:03:05","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":37132,"date":"2018-02-01","createdTime":"2018-01-31 22:30:05","auditedTime":"2018-02-01 03:03:05","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":11101,"date":"2018-02-01","createdTime":"2018-02-01 01:05:19","auditedTime":"2018-02-01 03:03:05","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:16:46 INFO  Pushed Record for 2018-02-01 03:03:06: [{"metadata_ownerId":10932,"date":"2018-02-01","createdTime":"2018-02-01 02:58:07","auditedTime":"2018-02-01 03:03:06","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":11502,"date":"2018-02-01","createdTime":"2018-01-31 20:39:15","auditedTime":"2018-02-01 03:03:06","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":68723,"date":"2018-02-01","createdTime":"2018-01-30 22:04:27","auditedTime":"2018-02-01 03:03:06","timeDelta":1,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:48 INFO  Pushed Record for 2018-02-01 03:03:07: [{"metadata_ownerId":4254,"date":"2018-02-01","createdTime":"2018-01-31 19:53:00","auditedTime":"2018-02-01 03:03:07","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":57881,"date":"2018-02-01","createdTime":"2018-01-24 23:43:48","auditedTime":"2018-02-01 03:03:07","timeDelta":7,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":14952,"date":"2018-02-01","createdTime":"2018-01-31 12:56:08","auditedTime":"2018-02-01 03:03:07","timeDelta":0,"createdHour":12,"auditedHour":3,"feedback":["Disliked","Liked"],"target":1},{"metadata_ownerId":14285,"date":"2018-02-01","createdTime":"2018-01-31 14:03:38","auditedTime":"2018-02-01 03:03:07","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:50 INFO  Pushed Record for 2018-02-01 03:03:08: [{"metadata_ownerId":18127,"date":"2018-02-01","createdTime":"2018-02-01 02:05:15","auditedTime":"2018-02-01 03:03:08","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":58737,"date":"2018-02-01","createdTime":"2018-02-01 03:02:49","auditedTime":"2018-02-01 03:03:08","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:52 INFO  Pushed Record for 2018-02-01 03:03:09: [{"metadata_ownerId":5930,"date":"2018-02-01","createdTime":"2018-01-28 13:30:33","auditedTime":"2018-02-01 03:03:09","timeDelta":3,"createdHour":13,"auditedHour":3,"feedback":["Clicked","Liked"],"target":1},{"metadata_ownerId":38600,"date":"2018-02-01","createdTime":"2018-01-31 01:07:25","auditedTime":"2018-02-01 03:03:09","timeDelta":1,"createdHour":1,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:16:53 INFO  Pushed Record for 2018-02-01 03:03:10: [{"metadata_ownerId":15759,"date":"2018-02-01","createdTime":"2018-01-29 20:23:15","auditedTime":"2018-02-01 03:03:10","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:55 INFO  Pushed Record for 2018-02-01 03:03:11: [{"metadata_ownerId":30830,"date":"2018-02-01","createdTime":"2018-01-31 21:01:35","auditedTime":"2018-02-01 03:03:11","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Clicked"],"target":0}][0m
[0m2021.07.12 18:16:57 INFO  Pushed Record for 2018-02-01 03:03:12: [{"metadata_ownerId":5430,"date":"2018-02-01","createdTime":"2018-01-29 11:35:04","auditedTime":"2018-02-01 03:03:12","timeDelta":2,"createdHour":11,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":24808,"date":"2018-02-01","createdTime":"2018-01-31 14:10:52","auditedTime":"2018-02-01 03:03:12","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":64417,"date":"2018-02-01","createdTime":"2018-01-31 14:53:07","auditedTime":"2018-02-01 03:03:12","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:16:59 INFO  Pushed Record for 2018-02-01 03:03:13: [{"metadata_ownerId":17474,"date":"2018-02-01","createdTime":"2018-01-31 20:50:36","auditedTime":"2018-02-01 03:03:13","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:01 INFO  Pushed Record for 2018-02-01 03:03:14: [{"metadata_ownerId":78985,"date":"2018-02-01","createdTime":"2018-02-01 02:15:09","auditedTime":"2018-02-01 03:03:14","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Commented"],"target":0},{"metadata_ownerId":30830,"date":"2018-02-01","createdTime":"2018-01-31 19:30:23","auditedTime":"2018-02-01 03:03:14","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:02 INFO  Pushed Record for 2018-02-01 03:03:15: [{"metadata_ownerId":66150,"date":"2018-02-01","createdTime":"2018-01-31 19:39:46","auditedTime":"2018-02-01 03:03:15","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:04 INFO  Pushed Record for 2018-02-01 03:03:16: [{"metadata_ownerId":17235,"date":"2018-02-01","createdTime":"2018-01-29 14:11:02","auditedTime":"2018-02-01 03:03:16","timeDelta":2,"createdHour":14,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":74422,"date":"2018-02-01","createdTime":"2018-01-29 22:06:44","auditedTime":"2018-02-01 03:03:16","timeDelta":2,"createdHour":22,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":76851,"date":"2018-02-01","createdTime":"2018-01-30 23:00:55","auditedTime":"2018-02-01 03:03:16","timeDelta":1,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:06 INFO  Pushed Record for 2018-02-01 03:03:17: [{"metadata_ownerId":64261,"date":"2018-02-01","createdTime":"2018-01-30 20:30:05","auditedTime":"2018-02-01 03:03:17","timeDelta":1,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":36484,"date":"2018-02-01","createdTime":"2018-01-29 20:15:15","auditedTime":"2018-02-01 03:03:17","timeDelta":2,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":8717,"date":"2018-02-01","createdTime":"2018-01-31 11:23:39","auditedTime":"2018-02-01 03:03:17","timeDelta":0,"createdHour":11,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":34013,"date":"2018-02-01","createdTime":"2018-02-01 00:05:22","auditedTime":"2018-02-01 03:03:17","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:09 INFO  Pushed Record for 2018-02-01 03:03:19: [{"metadata_ownerId":36479,"date":"2018-02-01","createdTime":"2018-01-31 13:02:11","auditedTime":"2018-02-01 03:03:19","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:17:11 INFO  Pushed Record for 2018-02-01 03:03:20: [{"metadata_ownerId":76197,"date":"2018-02-01","createdTime":"2018-01-29 12:02:55","auditedTime":"2018-02-01 03:03:20","timeDelta":2,"createdHour":12,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":29491,"date":"2018-02-01","createdTime":"2018-02-01 00:55:49","auditedTime":"2018-02-01 03:03:20","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:17:13 INFO  Pushed Record for 2018-02-01 03:03:21: [{"metadata_ownerId":65443,"date":"2018-02-01","createdTime":"2018-01-31 21:50:05","auditedTime":"2018-02-01 03:03:21","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:14 INFO  Pushed Record for 2018-02-01 03:03:22: [{"metadata_ownerId":46939,"date":"2018-02-01","createdTime":"2018-02-01 03:00:55","auditedTime":"2018-02-01 03:03:22","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:16 INFO  Pushed Record for 2018-02-01 03:03:23: [{"metadata_ownerId":43033,"date":"2018-02-01","createdTime":"2018-02-01 01:00:06","auditedTime":"2018-02-01 03:03:23","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:18 INFO  Pushed Record for 2018-02-01 03:03:24: [{"metadata_ownerId":75665,"date":"2018-02-01","createdTime":"2018-01-29 06:20:04","auditedTime":"2018-02-01 03:03:24","timeDelta":2,"createdHour":6,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":63345,"date":"2018-02-01","createdTime":"2018-01-31 15:00:15","auditedTime":"2018-02-01 03:03:24","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":75404,"date":"2018-02-01","createdTime":"2018-02-01 00:07:37","auditedTime":"2018-02-01 03:03:24","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":33057,"date":"2018-02-01","createdTime":"2018-01-31 22:38:32","auditedTime":"2018-02-01 03:03:24","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Viewed"],"target":0},{"metadata_ownerId":29751,"date":"2018-02-01","createdTime":"2018-02-01 03:01:24","auditedTime":"2018-02-01 03:03:24","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:20 INFO  Pushed Record for 2018-02-01 03:03:25: [{"metadata_ownerId":18661,"date":"2018-02-01","createdTime":"2018-01-30 19:31:56","auditedTime":"2018-02-01 03:03:25","timeDelta":1,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":26007,"date":"2018-02-01","createdTime":"2018-01-31 13:42:09","auditedTime":"2018-02-01 03:03:25","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:21 INFO  Pushed Record for 2018-02-01 03:03:26: [{"metadata_ownerId":26843,"date":"2018-02-01","createdTime":"2018-01-30 19:05:41","auditedTime":"2018-02-01 03:03:26","timeDelta":1,"createdHour":19,"auditedHour":3,"feedback":["Clicked"],"target":0}][0m
[0m2021.07.12 18:17:23 INFO  Pushed Record for 2018-02-01 03:03:27: [{"metadata_ownerId":42342,"date":"2018-02-01","createdTime":"2018-01-31 18:58:47","auditedTime":"2018-02-01 03:03:27","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":26618,"date":"2018-02-01","createdTime":"2018-01-31 23:00:12","auditedTime":"2018-02-01 03:03:27","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:25 INFO  Pushed Record for 2018-02-01 03:03:28: [{"metadata_ownerId":9301,"date":"2018-02-01","createdTime":"2018-01-31 10:10:35","auditedTime":"2018-02-01 03:03:28","timeDelta":0,"createdHour":10,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":62772,"date":"2018-02-01","createdTime":"2018-01-31 21:25:17","auditedTime":"2018-02-01 03:03:28","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":60932,"date":"2018-02-01","createdTime":"2018-01-30 19:35:14","auditedTime":"2018-02-01 03:03:28","timeDelta":1,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":34989,"date":"2018-02-01","createdTime":"2018-01-31 16:04:14","auditedTime":"2018-02-01 03:03:28","timeDelta":0,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":32890,"date":"2018-02-01","createdTime":"2018-01-30 15:04:35","auditedTime":"2018-02-01 03:03:28","timeDelta":1,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:27 INFO  Pushed Record for 2018-02-01 03:03:29: [{"metadata_ownerId":18062,"date":"2018-02-01","createdTime":"2018-01-31 18:22:06","auditedTime":"2018-02-01 03:03:29","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":84483,"date":"2018-02-01","createdTime":"2018-01-31 04:00:13","auditedTime":"2018-02-01 03:03:29","timeDelta":0,"createdHour":4,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:17:29 INFO  Pushed Record for 2018-02-01 03:03:30: [{"metadata_ownerId":11222,"date":"2018-02-01","createdTime":"2018-01-31 15:30:52","auditedTime":"2018-02-01 03:03:30","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Clicked","Liked"],"target":1},{"metadata_ownerId":1496,"date":"2018-02-01","createdTime":"2018-01-31 14:49:14","auditedTime":"2018-02-01 03:03:30","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":34013,"date":"2018-02-01","createdTime":"2018-01-31 12:05:17","auditedTime":"2018-02-01 03:03:30","timeDelta":0,"createdHour":12,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:17:32 INFO  Pushed Record for 2018-02-01 03:03:32: [{"metadata_ownerId":16812,"date":"2018-02-01","createdTime":"2018-01-31 23:10:26","auditedTime":"2018-02-01 03:03:32","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":51588,"date":"2018-02-01","createdTime":"2018-02-01 02:23:38","auditedTime":"2018-02-01 03:03:32","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":76851,"date":"2018-02-01","createdTime":"2018-01-31 02:30:13","auditedTime":"2018-02-01 03:03:32","timeDelta":1,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":12141,"date":"2018-02-01","createdTime":"2018-02-01 02:55:19","auditedTime":"2018-02-01 03:03:32","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":41992,"date":"2018-02-01","createdTime":"2018-02-01 00:30:49","auditedTime":"2018-02-01 03:03:32","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":61284,"date":"2018-02-01","createdTime":"2018-01-31 22:37:04","auditedTime":"2018-02-01 03:03:32","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:34 INFO  Pushed Record for 2018-02-01 03:03:33: [{"metadata_ownerId":10644,"date":"2018-02-01","createdTime":"2018-01-28 17:30:28","auditedTime":"2018-02-01 03:03:33","timeDelta":3,"createdHour":17,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:17:35 INFO  Pushed Record for 2018-02-01 03:03:34: [{"metadata_ownerId":61553,"date":"2018-02-01","createdTime":"2018-02-01 02:00:07","auditedTime":"2018-02-01 03:03:34","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked","ReShared"],"target":0},{"metadata_ownerId":24243,"date":"2018-02-01","createdTime":"2018-01-31 23:12:34","auditedTime":"2018-02-01 03:03:34","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":44828,"date":"2018-02-01","createdTime":"2018-01-30 08:05:23","auditedTime":"2018-02-01 03:03:34","timeDelta":1,"createdHour":8,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:37 INFO  Pushed Record for 2018-02-01 03:03:35: [{"metadata_ownerId":43086,"date":"2018-02-01","createdTime":"2018-01-31 16:52:51","auditedTime":"2018-02-01 03:03:35","timeDelta":0,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":41987,"date":"2018-02-01","createdTime":"2018-02-01 01:40:17","auditedTime":"2018-02-01 03:03:35","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Clicked","ReShared","Liked"],"target":1},{"metadata_ownerId":81703,"date":"2018-02-01","createdTime":"2018-01-27 03:49:43","auditedTime":"2018-02-01 03:03:35","timeDelta":4,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":18760,"date":"2018-02-01","createdTime":"2018-01-31 02:10:34","auditedTime":"2018-02-01 03:03:35","timeDelta":1,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:39 INFO  Pushed Record for 2018-02-01 03:03:36: [{"metadata_ownerId":5304,"date":"2018-02-01","createdTime":"2018-02-01 02:49:28","auditedTime":"2018-02-01 03:03:36","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked","Viewed"],"target":0}][0m
[0m2021.07.12 18:17:41 INFO  Pushed Record for 2018-02-01 03:03:37: [{"metadata_ownerId":39495,"date":"2018-02-01","createdTime":"2018-01-31 23:20:27","auditedTime":"2018-02-01 03:03:37","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:43 INFO  Pushed Record for 2018-02-01 03:03:38: [{"metadata_ownerId":51932,"date":"2018-02-01","createdTime":"2018-02-01 02:40:58","auditedTime":"2018-02-01 03:03:38","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":51932,"date":"2018-02-01","createdTime":"2018-02-01 03:00:59","auditedTime":"2018-02-01 03:03:38","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":31477,"date":"2018-02-01","createdTime":"2018-01-25 13:51:04","auditedTime":"2018-02-01 03:03:38","timeDelta":6,"createdHour":13,"auditedHour":3,"feedback":["Clicked","Liked"],"target":1},{"metadata_ownerId":67220,"date":"2018-02-01","createdTime":"2018-01-30 21:34:39","auditedTime":"2018-02-01 03:03:38","timeDelta":1,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":50524,"date":"2018-02-01","createdTime":"2017-11-16 05:30:10","auditedTime":"2018-02-01 03:03:38","timeDelta":76,"createdHour":5,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":6077,"date":"2018-02-01","createdTime":"2018-02-01 02:14:18","auditedTime":"2018-02-01 03:03:38","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:46 INFO  Pushed Record for 2018-02-01 03:03:40: [{"metadata_ownerId":40248,"date":"2018-02-01","createdTime":"2018-02-01 00:51:53","auditedTime":"2018-02-01 03:03:40","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":50598,"date":"2018-02-01","createdTime":"2018-02-01 00:25:33","auditedTime":"2018-02-01 03:03:40","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":13451,"date":"2018-02-01","createdTime":"2018-01-31 05:06:07","auditedTime":"2018-02-01 03:03:40","timeDelta":0,"createdHour":5,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:48 INFO  Pushed Record for 2018-02-01 03:03:41: [{"metadata_ownerId":37981,"date":"2018-02-01","createdTime":"2018-02-01 01:05:28","auditedTime":"2018-02-01 03:03:41","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":43076,"date":"2018-02-01","createdTime":"2018-01-30 14:20:16","auditedTime":"2018-02-01 03:03:41","timeDelta":1,"createdHour":14,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:17:49 INFO  Pushed Record for 2018-02-01 03:03:42: [{"metadata_ownerId":16348,"date":"2018-02-01","createdTime":"2018-01-30 22:04:13","auditedTime":"2018-02-01 03:03:42","timeDelta":1,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":12439,"date":"2018-02-01","createdTime":"2018-01-30 14:46:33","auditedTime":"2018-02-01 03:03:42","timeDelta":1,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:51 INFO  Pushed Record for 2018-02-01 03:03:43: [{"metadata_ownerId":16316,"date":"2018-02-01","createdTime":"2018-01-31 22:29:05","auditedTime":"2018-02-01 03:03:43","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":11386,"date":"2018-02-01","createdTime":"2018-01-29 21:06:35","auditedTime":"2018-02-01 03:03:43","timeDelta":2,"createdHour":21,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":16593,"date":"2018-02-01","createdTime":"2018-02-01 02:34:43","auditedTime":"2018-02-01 03:03:43","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":16593,"date":"2018-02-01","createdTime":"2018-02-01 02:34:17","auditedTime":"2018-02-01 03:03:43","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:17:53 INFO  Pushed Record for 2018-02-01 03:03:44: [{"metadata_ownerId":37463,"date":"2018-02-01","createdTime":"2018-01-31 13:36:32","auditedTime":"2018-02-01 03:03:44","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":3839,"date":"2018-02-01","createdTime":"2018-01-30 04:00:31","auditedTime":"2018-02-01 03:03:44","timeDelta":1,"createdHour":4,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":4036,"date":"2018-02-01","createdTime":"2018-01-31 15:08:22","auditedTime":"2018-02-01 03:03:44","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:55 INFO  Pushed Record for 2018-02-01 03:03:45: [{"metadata_ownerId":63307,"date":"2018-02-01","createdTime":"2018-01-31 23:10:19","auditedTime":"2018-02-01 03:03:45","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:17:56 INFO  Pushed Record for 2018-02-01 03:03:46: [{"metadata_ownerId":19736,"date":"2018-02-01","createdTime":"2018-01-31 13:58:03","auditedTime":"2018-02-01 03:03:46","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":14971,"date":"2018-02-01","createdTime":"2018-02-01 03:02:49","auditedTime":"2018-02-01 03:03:46","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":18840,"date":"2018-02-01","createdTime":"2018-01-31 20:00:09","auditedTime":"2018-02-01 03:03:46","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:18:00 INFO  Pushed Record for 2018-02-01 03:03:48: [{"metadata_ownerId":81308,"date":"2018-02-01","createdTime":"2018-02-01 01:04:36","auditedTime":"2018-02-01 03:03:48","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:18:01 INFO  Pushed Record for 2018-02-01 03:03:49: [{"metadata_ownerId":19559,"date":"2018-02-01","createdTime":"2018-01-30 19:30:24","auditedTime":"2018-02-01 03:03:49","timeDelta":1,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:18:03 INFO  Pushed Record for 2018-02-01 03:03:50: [{"metadata_ownerId":47592,"date":"2018-02-01","createdTime":"2018-01-31 13:44:59","auditedTime":"2018-02-01 03:03:50","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":44830,"date":"2018-02-01","createdTime":"2018-01-31 18:05:45","auditedTime":"2018-02-01 03:03:50","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":16524,"date":"2018-02-01","createdTime":"2018-02-01 01:01:40","auditedTime":"2018-02-01 03:03:50","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":39723,"date":"2018-02-01","createdTime":"2018-01-31 14:10:26","auditedTime":"2018-02-01 03:03:50","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:18:05 INFO  Pushed Record for 2018-02-01 03:03:51: [{"metadata_ownerId":78243,"date":"2018-02-01","createdTime":"2018-01-30 23:01:04","auditedTime":"2018-02-01 03:03:51","timeDelta":1,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":33344,"date":"2018-02-01","createdTime":"2018-01-31 04:20:21","auditedTime":"2018-02-01 03:03:51","timeDelta":0,"createdHour":4,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":12967,"date":"2018-02-01","createdTime":"2018-01-29 13:05:35","auditedTime":"2018-02-01 03:03:51","timeDelta":2,"createdHour":13,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":16035,"date":"2018-02-01","createdTime":"2018-01-31 13:35:25","auditedTime":"2018-02-01 03:03:51","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:18:07 INFO  Pushed Record for 2018-02-01 03:03:52: [{"metadata_ownerId":25773,"date":"2018-02-01","createdTime":"2018-02-01 00:30:05","auditedTime":"2018-02-01 03:03:52","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:18:08 INFO  Pushed Record for 2018-02-01 03:03:53: [{"metadata_ownerId":32262,"date":"2018-02-01","createdTime":"2018-02-01 01:05:50","auditedTime":"2018-02-01 03:03:53","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":32890,"date":"2018-02-01","createdTime":"2018-02-01 00:55:30","auditedTime":"2018-02-01 03:03:53","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":46987,"date":"2018-02-01","createdTime":"2018-01-30 10:05:19","auditedTime":"2018-02-01 03:03:53","timeDelta":1,"createdHour":10,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":9236,"date":"2018-02-01","createdTime":"2018-01-29 08:50:07","auditedTime":"2018-02-01 03:03:53","timeDelta":2,"createdHour":8,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":12730,"date":"2018-02-01","createdTime":"2018-01-31 09:01:33","auditedTime":"2018-02-01 03:03:53","timeDelta":0,"createdHour":9,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:18:11 INFO  Pushed Record for 2018-02-01 03:03:55: [{"metadata_ownerId":20578,"date":"2018-02-01","createdTime":"2018-01-31 19:32:59","auditedTime":"2018-02-01 03:03:55","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Viewed"],"target":0},{"metadata_ownerId":62010,"date":"2018-02-01","createdTime":"2018-01-31 19:19:03","auditedTime":"2018-02-01 03:03:55","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":7617,"date":"2018-02-01","createdTime":"2018-01-31 14:20:22","auditedTime":"2018-02-01 03:03:55","timeDelta":0,"createdHour":14,"auditedHour":3,"feedback":["Clicked"],"target":0}][0m
[0m2021.07.12 18:18:13 INFO  Pushed Record for 2018-02-01 03:03:56: [{"metadata_ownerId":33266,"date":"2018-02-01","createdTime":"2018-02-01 02:18:43","auditedTime":"2018-02-01 03:03:56","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":14770,"date":"2018-02-01","createdTime":"2018-01-31 23:33:15","auditedTime":"2018-02-01 03:03:56","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":18796,"date":"2018-02-01","createdTime":"2018-02-01 01:40:14","auditedTime":"2018-02-01 03:03:56","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Disliked","Clicked"],"target":0},{"metadata_ownerId":80037,"date":"2018-02-01","createdTime":"2018-01-29 09:50:04","auditedTime":"2018-02-01 03:03:56","timeDelta":2,"createdHour":9,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:18:15 INFO  Pushed Record for 2018-02-01 03:03:57: [{"metadata_ownerId":35046,"date":"2018-02-01","createdTime":"2018-01-25 11:01:24","auditedTime":"2018-02-01 03:03:57","timeDelta":6,"createdHour":11,"auditedHour":3,"feedback":["Clicked","Liked"],"target":1},{"metadata_ownerId":15729,"date":"2018-02-01","createdTime":"2018-01-31 19:05:34","auditedTime":"2018-02-01 03:03:57","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:18:17 INFO  Pushed Record for 2018-02-01 03:03:58: [{"metadata_ownerId":50435,"date":"2018-02-01","createdTime":"2018-01-31 22:26:44","auditedTime":"2018-02-01 03:03:58","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0},{"metadata_ownerId":20569,"date":"2018-02-01","createdTime":"2018-01-31 10:50:21","auditedTime":"2018-02-01 03:03:58","timeDelta":0,"createdHour":10,"auditedHour":3,"feedback":["Disliked","Ignored"],"target":0},{"metadata_ownerId":8225,"date":"2018-02-01","createdTime":"2018-02-01 01:05:23","auditedTime":"2018-02-01 03:03:58","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":79195,"date":"2018-02-01","createdTime":"2018-01-31 05:17:51","auditedTime":"2018-02-01 03:03:58","timeDelta":0,"createdHour":5,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":12312,"date":"2018-02-01","createdTime":"2018-01-31 21:15:29","auditedTime":"2018-02-01 03:03:58","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":78486,"date":"2018-02-01","createdTime":"2018-01-31 15:01:29","auditedTime":"2018-02-01 03:03:58","timeDelta":0,"createdHour":15,"auditedHour":3,"feedback":["Clicked","Liked"],"target":1}][0m
[0m2021.07.12 18:18:18 INFO  Pushed Record for 2018-02-01 03:03:59: [{"metadata_ownerId":6732,"date":"2018-02-01","createdTime":"2018-02-01 02:14:04","auditedTime":"2018-02-01 03:03:59","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked"],"target":0},{"metadata_ownerId":11132,"date":"2018-02-01","createdTime":"2018-02-01 00:01:02","auditedTime":"2018-02-01 03:03:59","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":8498,"date":"2018-02-01","createdTime":"2018-02-01 00:12:02","auditedTime":"2018-02-01 03:03:59","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":178,"date":"2018-02-01","createdTime":"2018-01-31 20:29:45","auditedTime":"2018-02-01 03:03:59","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:18:20 INFO  Pushed Record for 2018-02-01 03:04:00: [{"metadata_ownerId":25012,"date":"2018-02-01","createdTime":"2018-02-01 00:01:02","auditedTime":"2018-02-01 03:04:00","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":62467,"date":"2018-02-01","createdTime":"2017-12-01 16:10:24","auditedTime":"2018-02-01 03:04:00","timeDelta":61,"createdHour":16,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":75937,"date":"2018-02-01","createdTime":"2018-02-01 02:22:15","auditedTime":"2018-02-01 03:04:00","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":24198,"date":"2018-02-01","createdTime":"2018-02-01 02:46:19","auditedTime":"2018-02-01 03:04:00","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":82681,"date":"2018-02-01","createdTime":"2018-02-01 00:52:03","auditedTime":"2018-02-01 03:04:00","timeDelta":0,"createdHour":0,"auditedHour":3,"feedback":["Disliked","Liked"],"target":1},{"metadata_ownerId":9325,"date":"2018-02-01","createdTime":"2018-02-01 01:33:58","auditedTime":"2018-02-01 03:04:00","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:18:23 INFO  Pushed Record for 2018-02-01 03:04:02: [{"metadata_ownerId":7939,"date":"2018-02-01","createdTime":"2018-01-31 20:05:33","auditedTime":"2018-02-01 03:04:02","timeDelta":0,"createdHour":20,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":32993,"date":"2018-02-01","createdTime":"2018-01-31 22:03:31","auditedTime":"2018-02-01 03:04:02","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":62639,"date":"2018-02-01","createdTime":"2018-01-31 23:04:53","auditedTime":"2018-02-01 03:04:02","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":19464,"date":"2018-02-01","createdTime":"2018-01-29 13:10:43","auditedTime":"2018-02-01 03:04:02","timeDelta":2,"createdHour":13,"auditedHour":3,"feedback":["Clicked","ReShared","Liked","Ignored"],"target":1}][0m
[0m2021.07.12 18:18:26 INFO  Pushed Record for 2018-02-01 03:04:04: [{"metadata_ownerId":37333,"date":"2018-02-01","createdTime":"2018-01-30 17:02:03","auditedTime":"2018-02-01 03:04:04","timeDelta":1,"createdHour":17,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":24604,"date":"2018-02-01","createdTime":"2018-01-31 21:27:29","auditedTime":"2018-02-01 03:04:04","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Clicked","ReShared"],"target":0},{"metadata_ownerId":23740,"date":"2018-02-01","createdTime":"2018-01-31 23:02:54","auditedTime":"2018-02-01 03:04:04","timeDelta":0,"createdHour":23,"auditedHour":3,"feedback":["Disliked"],"target":0}][0m
[0m2021.07.12 18:18:29 INFO  Pushed Record for 2018-02-01 03:04:06: [{"metadata_ownerId":25791,"date":"2018-02-01","createdTime":"2018-01-31 18:13:47","auditedTime":"2018-02-01 03:04:06","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:18:31 INFO  Pushed Record for 2018-02-01 03:04:07: [{"metadata_ownerId":31682,"date":"2018-02-01","createdTime":"2018-02-01 03:00:02","auditedTime":"2018-02-01 03:04:07","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":20946,"date":"2018-02-01","createdTime":"2018-02-01 03:02:45","auditedTime":"2018-02-01 03:04:07","timeDelta":0,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":80097,"date":"2018-02-01","createdTime":"2018-01-24 00:35:24","auditedTime":"2018-02-01 03:04:07","timeDelta":8,"createdHour":0,"auditedHour":3,"feedback":["Disliked"],"target":0},{"metadata_ownerId":14959,"date":"2018-02-01","createdTime":"2018-01-31 13:02:25","auditedTime":"2018-02-01 03:04:07","timeDelta":0,"createdHour":13,"auditedHour":3,"feedback":["Clicked","Liked"],"target":1},{"metadata_ownerId":77849,"date":"2018-02-01","createdTime":"2018-01-31 21:06:25","auditedTime":"2018-02-01 03:04:07","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:18:33 INFO  Pushed Record for 2018-02-01 03:04:08: [{"metadata_ownerId":65046,"date":"2018-02-01","createdTime":"2018-01-31 19:19:29","auditedTime":"2018-02-01 03:04:08","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:18:35 INFO  Pushed Record for 2018-02-01 03:04:09: [{"metadata_ownerId":59662,"date":"2018-02-01","createdTime":"2018-02-01 02:10:56","auditedTime":"2018-02-01 03:04:09","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Clicked","ReShared","Liked"],"target":1},{"metadata_ownerId":23461,"date":"2018-02-01","createdTime":"2018-02-01 02:50:14","auditedTime":"2018-02-01 03:04:09","timeDelta":0,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":12492,"date":"2018-02-01","createdTime":"2018-01-31 22:10:24","auditedTime":"2018-02-01 03:04:09","timeDelta":0,"createdHour":22,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:18:38 INFO  Pushed Record for 2018-02-01 03:04:11: [{"metadata_ownerId":64261,"date":"2018-02-01","createdTime":"2018-01-29 08:00:05","auditedTime":"2018-02-01 03:04:11","timeDelta":2,"createdHour":8,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":16810,"date":"2018-02-01","createdTime":"2018-01-30 03:26:07","auditedTime":"2018-02-01 03:04:11","timeDelta":1,"createdHour":3,"auditedHour":3,"feedback":["Liked"],"target":1},{"metadata_ownerId":65381,"date":"2018-02-01","createdTime":"2018-01-31 03:01:21","auditedTime":"2018-02-01 03:04:11","timeDelta":1,"createdHour":3,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:18:40 INFO  Pushed Record for 2018-02-01 03:04:12: [{"metadata_ownerId":20728,"date":"2018-02-01","createdTime":"2018-01-31 19:06:32","auditedTime":"2018-02-01 03:04:12","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:18:43 INFO  Pushed Record for 2018-02-01 03:04:14: [{"metadata_ownerId":10782,"date":"2018-02-01","createdTime":"2018-01-31 02:29:36","auditedTime":"2018-02-01 03:04:14","timeDelta":1,"createdHour":2,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":67227,"date":"2018-02-01","createdTime":"2018-01-31 19:45:41","auditedTime":"2018-02-01 03:04:14","timeDelta":0,"createdHour":19,"auditedHour":3,"feedback":["Liked"],"target":1}][0m
[0m2021.07.12 18:18:45 INFO  Pushed Record for 2018-02-01 03:04:15: [{"metadata_ownerId":30496,"date":"2018-02-01","createdTime":"2018-02-01 01:01:04","auditedTime":"2018-02-01 03:04:15","timeDelta":0,"createdHour":1,"auditedHour":3,"feedback":["Clicked","ReShared"],"target":0}][0m
[0m2021.07.12 18:18:46 INFO  Pushed Record for 2018-02-01 03:04:16: [{"metadata_ownerId":34408,"date":"2018-02-01","createdTime":"2018-01-30 08:40:08","auditedTime":"2018-02-01 03:04:16","timeDelta":1,"createdHour":8,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:18:48 INFO  Pushed Record for 2018-02-01 03:04:17: [{"metadata_ownerId":72316,"date":"2018-02-01","createdTime":"2018-01-31 21:56:01","auditedTime":"2018-02-01 03:04:17","timeDelta":0,"createdHour":21,"auditedHour":3,"feedback":["Clicked","Viewed"],"target":0},{"metadata_ownerId":57901,"date":"2018-02-01","createdTime":"2018-01-29 21:05:35","auditedTime":"2018-02-01 03:04:17","timeDelta":2,"createdHour":21,"auditedHour":3,"feedback":["Clicked"],"target":0}][0m
[0m2021.07.12 18:18:50 INFO  Pushed Record for 2018-02-01 03:04:18: [{"metadata_ownerId":51957,"date":"2018-02-01","createdTime":"2018-01-31 18:03:04","auditedTime":"2018-02-01 03:04:18","timeDelta":0,"createdHour":18,"auditedHour":3,"feedback":["Ignored"],"target":0},{"metadata_ownerId":75525,"date":"2018-02-01","createdTime":"2018-01-31 16:30:15","auditedTime":"2018-02-01 03:04:18","timeDelta":0,"createdHour":16,"auditedHour":3,"feedback":["Ignored"],"target":0}][0m
[0m2021.07.12 18:18:52 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.DataProvider][0m
[0m2021.07.12 18:18:52 INFO  Listening for transport dt_socket at address: 48279[0m
Jul 12, 2021 6:23:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1375
[0m2021.07.12 18:24:41 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 5h 26m 26.417s)[0m
[0m2021.07.12 18:24:41 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 18:24:41 INFO  Listening for transport dt_socket at address: 56697[0m
[0m2021.07.12 18:24:41 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 18:24:41 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.12 18:24:41 INFO  Trying to attach to remote debuggee VM localhost:56697 .[0m
[0m2021.07.12 18:24:41 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 18:24:43 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.12 18:24:43 ERROR 21/07/12 18:24:43 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.149.194 instead (on interface eth0)[0m
[0m2021.07.12 18:24:43 ERROR 21/07/12 18:24:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.12 18:24:44 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.12 18:24:44 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.12 18:24:44 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.12 18:24:44 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.12 18:24:44 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.12 18:24:44 ERROR 21/07/12 18:24:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.12 18:24:45 ERROR 21/07/12 18:24:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.[0m
[0m2021.07.12 18:24:45 ERROR 21/07/12 18:24:46 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.[0m
[0m2021.07.12 18:24:46 ERROR 21/07/12 18:24:46 WARN KafkaUtils: overriding enable.auto.commit to false for executor[0m
[0m2021.07.12 18:24:46 ERROR 21/07/12 18:24:46 WARN KafkaUtils: overriding auto.offset.reset to none for executor[0m
[0m2021.07.12 18:24:46 ERROR 21/07/12 18:24:46 WARN KafkaUtils: overriding executor group.id to spark-executor-spark_ml_testing[0m
[0m2021.07.12 18:24:46 ERROR 21/07/12 18:24:46 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135[0m
[0m2021.07.12 18:25:04 INFO  root[0m
[0m2021.07.12 18:25:04 INFO   |-- auditedHour: long (nullable = true)[0m
[0m2021.07.12 18:25:04 INFO   |-- auditedTime: string (nullable = true)[0m
[0m2021.07.12 18:25:04 INFO   |-- createdHour: long (nullable = true)[0m
[0m2021.07.12 18:25:04 INFO   |-- createdTime: string (nullable = true)[0m
[0m2021.07.12 18:25:04 INFO   |-- date: string (nullable = true)[0m
[0m2021.07.12 18:25:04 INFO   |-- feedback: array (nullable = true)[0m
[0m2021.07.12 18:25:04 INFO   |    |-- element: string (containsNull = true)[0m
[0m2021.07.12 18:25:04 INFO   |-- metadata_ownerId: long (nullable = true)[0m
[0m2021.07.12 18:25:04 INFO   |-- target: long (nullable = true)[0m
[0m2021.07.12 18:25:04 INFO   |-- timeDelta: long (nullable = true)[0m
[0m2021.07.12 18:25:04 INFO  [0m
[0m2021.07.12 18:25:04 INFO  +-----------+-------------------+-----------+-------------------+----------+--------------------+----------------+------+---------+[0m
[0m2021.07.12 18:25:04 INFO  |auditedHour|        auditedTime|createdHour|        createdTime|      date|            feedback|metadata_ownerId|target|timeDelta|[0m
[0m2021.07.12 18:25:04 INFO  +-----------+-------------------+-----------+-------------------+----------+--------------------+----------------+------+---------+[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:00|          0|2018-02-01 00:25:14|2018-02-01|[Clicked, Unliked...|           49367|     1|        0|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:00|         10|2018-01-30 10:30:14|2018-02-01|           [Ignored]|           76851|     0|        1|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:00|          2|2018-02-01 02:36:21|2018-02-01|           [Ignored]|           19357|     0|        0|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:01|          1|2018-02-01 01:50:18|2018-02-01|           [Ignored]|           77159|     0|        0|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:02|          0|2018-02-01 00:03:09|2018-02-01|          [Disliked]|           25161|     0|        0|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:02|         20|2018-01-29 20:21:50|2018-02-01|           [Clicked]|           80203|     0|        2|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:02|         15|2018-01-23 15:15:04|2018-02-01|           [Ignored]|           65675|     0|        8|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:03|          1|2018-02-01 01:00:21|2018-02-01|           [Ignored]|           75062|     0|        0|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:03|         15|2018-01-31 15:06:46|2018-02-01|          [Disliked]|           68682|     0|        0|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:03|         13|2018-01-31 13:00:48|2018-02-01|           [Ignored]|           49570|     0|        0|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:04|          2|2018-02-01 02:50:09|2018-02-01|           [Clicked]|           40612|     0|        0|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:04|         14|2018-01-15 14:22:34|2018-02-01|           [Ignored]|           27554|     0|       16|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:05|          6|2018-01-31 06:00:05|2018-02-01|           [Ignored]|           64597|     0|        0|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:05|          3|2018-01-31 03:04:19|2018-02-01|           [Ignored]|           33827|     0|        0|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:06|          0|2018-02-01 00:26:49|2018-02-01|           [Clicked]|           24079|     0|        0|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:06|         20|2018-01-31 20:01:09|2018-02-01|           [Ignored]|           22580|     0|        0|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:06|         15|2017-10-05 15:45:48|2018-02-01|           [Ignored]|           35049|     0|      118|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:06|         19|2018-01-30 19:36:22|2018-02-01|           [Ignored]|            5862|     0|        1|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:07|         22|2018-01-31 22:16:22|2018-02-01|           [Ignored]|            5049|     0|        0|[0m
[0m2021.07.12 18:25:04 INFO  |          3|2018-02-01 03:00:08|          1|2018-02-01 01:01:15|2018-02-01|           [Clicked]|           45299|     0|        0|[0m
[0m2021.07.12 18:25:04 INFO  +-----------+-------------------+-----------+-------------------+----------+--------------------+----------------+------+---------+[0m
[0m2021.07.12 18:25:04 INFO  only showing top 20 rows[0m
[0m2021.07.12 18:25:04 INFO  [0m
[0m2021.07.12 18:25:30 INFO  root[0m
[0m2021.07.12 18:25:30 INFO   |-- auditedHour: long (nullable = true)[0m
[0m2021.07.12 18:25:30 INFO   |-- auditedTime: string (nullable = true)[0m
[0m2021.07.12 18:25:30 INFO   |-- createdHour: long (nullable = true)[0m
[0m2021.07.12 18:25:30 INFO   |-- createdTime: string (nullable = true)[0m
[0m2021.07.12 18:25:30 INFO   |-- date: string (nullable = true)[0m
[0m2021.07.12 18:25:30 INFO   |-- feedback: array (nullable = true)[0m
[0m2021.07.12 18:25:30 INFO   |    |-- element: string (containsNull = true)[0m
[0m2021.07.12 18:25:30 INFO   |-- metadata_ownerId: long (nullable = true)[0m
[0m2021.07.12 18:25:30 INFO   |-- target: long (nullable = true)[0m
[0m2021.07.12 18:25:30 INFO   |-- timeDelta: long (nullable = true)[0m
[0m2021.07.12 18:25:30 INFO  [0m
[0m2021.07.12 18:25:30 INFO  +-----------+-------------------+-----------+-------------------+----------+-------------------+----------------+------+---------+[0m
[0m2021.07.12 18:25:30 INFO  |auditedHour|        auditedTime|createdHour|        createdTime|      date|           feedback|metadata_ownerId|target|timeDelta|[0m
[0m2021.07.12 18:25:30 INFO  +-----------+-------------------+-----------+-------------------+----------+-------------------+----------------+------+---------+[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:13|         18|2018-01-30 18:49:09|2018-02-01|          [Ignored]|           24072|     0|        1|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:13|          4|2018-01-29 04:19:24|2018-02-01|          [Ignored]|           31314|     0|        2|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:14|         21|2018-01-31 21:00:22|2018-02-01|[Disliked, Ignored]|           61558|     0|        0|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:15|         10|2018-01-31 10:39:38|2018-02-01|            [Liked]|           11925|     1|        0|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:15|          0|2018-02-01 00:52:18|2018-02-01|          [Ignored]|           20251|     0|        0|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:15|         13|2018-01-28 13:46:07|2018-02-01|         [Disliked]|           23289|     0|        3|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:16|          1|2018-02-01 01:58:27|2018-02-01| [Clicked, Ignored]|           63735|     0|        0|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:16|         14|2018-01-31 14:45:05|2018-02-01|         [Disliked]|           74173|     0|        0|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:16|          9|2018-01-31 09:30:28|2018-02-01|          [Ignored]|           51455|     0|        0|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:16|          8|2018-01-26 08:56:52|2018-02-01|          [Ignored]|           46163|     0|        5|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:16|         15|2018-01-31 15:29:36|2018-02-01|          [Clicked]|           81171|     0|        0|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:17|         21|2018-01-31 21:30:38|2018-02-01|          [Ignored]|           50547|     0|        0|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:18|         14|2018-01-31 14:10:52|2018-02-01|          [Ignored]|           18706|     0|        0|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:18|          2|2018-02-01 02:40:16|2018-02-01|         [Disliked]|            3641|     0|        0|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:19|         18|2018-01-31 18:20:16|2018-02-01|          [Ignored]|            3163|     0|        0|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:19|         20|2018-01-29 20:50:33|2018-02-01|          [Ignored]|           32749|     0|        2|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:20|         19|2018-01-31 19:20:14|2018-02-01|          [Ignored]|           77398|     0|        0|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:20|          0|2018-01-28 00:01:26|2018-02-01|          [Ignored]|           26735|     0|        4|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:20|         20|2018-01-31 20:01:01|2018-02-01|         [Disliked]|           24469|     0|        0|[0m
[0m2021.07.12 18:25:30 INFO  |          3|2018-02-01 03:00:20|         12|2018-01-31 12:10:13|2018-02-01|         [Disliked]|           80588|     0|        0|[0m
[0m2021.07.12 18:25:30 INFO  +-----------+-------------------+-----------+-------------------+----------+-------------------+----------------+------+---------+[0m
[0m2021.07.12 18:25:30 INFO  only showing top 20 rows[0m
[0m2021.07.12 18:25:30 INFO  [0m
[0m2021.07.12 18:26:00 INFO  root[0m
[0m2021.07.12 18:26:00 INFO   |-- auditedHour: long (nullable = true)[0m
[0m2021.07.12 18:26:00 INFO   |-- auditedTime: string (nullable = true)[0m
[0m2021.07.12 18:26:00 INFO   |-- createdHour: long (nullable = true)[0m
[0m2021.07.12 18:26:00 INFO   |-- createdTime: string (nullable = true)[0m
[0m2021.07.12 18:26:00 INFO   |-- date: string (nullable = true)[0m
[0m2021.07.12 18:26:00 INFO   |-- feedback: array (nullable = true)[0m
[0m2021.07.12 18:26:00 INFO   |    |-- element: string (containsNull = true)[0m
[0m2021.07.12 18:26:00 INFO   |-- metadata_ownerId: long (nullable = true)[0m
[0m2021.07.12 18:26:00 INFO   |-- target: long (nullable = true)[0m
[0m2021.07.12 18:26:00 INFO   |-- timeDelta: long (nullable = true)[0m
[0m2021.07.12 18:26:00 INFO  [0m
[0m2021.07.12 18:26:00 INFO  +-----------+-------------------+-----------+-------------------+----------+------------------+----------------+------+---------+[0m
[0m2021.07.12 18:26:00 INFO  |auditedHour|        auditedTime|createdHour|        createdTime|      date|          feedback|metadata_ownerId|target|timeDelta|[0m
[0m2021.07.12 18:26:00 INFO  +-----------+-------------------+-----------+-------------------+----------+------------------+----------------+------+---------+[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:29|         19|2018-01-30 19:50:10|2018-02-01|           [Liked]|            3370|     1|        1|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:29|         17|2018-01-30 17:11:56|2018-02-01|         [Ignored]|           30094|     0|        1|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:31|          1|2018-02-01 01:11:52|2018-02-01|         [Ignored]|           25334|     0|        0|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:32|         11|2018-01-30 11:03:26|2018-02-01|         [Ignored]|           33596|     0|        1|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:32|          1|2018-01-30 01:05:17|2018-02-01|         [Ignored]|           38648|     0|        2|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:33|         23|2018-01-31 23:30:34|2018-02-01|         [Ignored]|           34617|     0|        0|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:33|          8|2018-01-31 08:00:34|2018-02-01|         [Ignored]|           26098|     0|        0|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:33|          2|2018-02-01 02:00:53|2018-02-01|         [Ignored]|            9486|     0|        0|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:34|         23|2017-12-13 23:52:44|2018-02-01|           [Liked]|           78936|     1|       49|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:34|          1|2018-02-01 01:57:12|2018-02-01|         [Ignored]|           50598|     0|        0|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:35|          0|2018-01-31 00:46:25|2018-02-01|         [Ignored]|            4159|     0|        1|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:35|         19|2018-01-31 19:46:54|2018-02-01|         [Ignored]|           61553|     0|        0|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:35|         21|2018-01-31 21:09:11|2018-02-01|         [Ignored]|           25657|     0|        0|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:35|          3|2018-01-30 03:06:12|2018-02-01|         [Ignored]|           13451|     0|        1|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:36|         16|2018-01-31 16:58:20|2018-02-01|         [Ignored]|           48644|     0|        0|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:36|          8|2018-01-31 08:16:09|2018-02-01|         [Ignored]|           80855|     0|        0|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:36|          9|2018-01-31 09:16:04|2018-02-01|[Commented, Liked]|           38665|     1|        0|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:37|         15|2018-01-30 15:37:14|2018-02-01|         [Ignored]|           69046|     0|        1|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:38|         11|2018-01-31 11:40:13|2018-02-01|         [Ignored]|           42180|     0|        0|[0m
[0m2021.07.12 18:26:00 INFO  |          3|2018-02-01 03:00:38|         23|2018-01-30 23:45:50|2018-02-01|           [Liked]|           37057|     1|        1|[0m
[0m2021.07.12 18:26:00 INFO  +-----------+-------------------+-----------+-------------------+----------+------------------+----------------+------+---------+[0m
[0m2021.07.12 18:26:00 INFO  only showing top 20 rows[0m
[0m2021.07.12 18:26:00 INFO  [0m
[0m2021.07.12 18:26:14 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.12 18:26:14 INFO  Listening for transport dt_socket at address: 49999[0m
Jul 12, 2021 6:27:23 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1404
Jul 12, 2021 6:27:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1426
Jul 12, 2021 6:28:12 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1475
Jul 12, 2021 6:28:19 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1488
[0m2021.07.12 18:28:37 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.12 18:28:39 INFO  time: compiled ht3 in 1.53s[0m
[0m2021.07.12 18:28:58 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 5h 30m 43.358s)[0m
[0m2021.07.12 18:28:58 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.12 18:28:58 INFO  Listening for transport dt_socket at address: 48061[0m
[0m2021.07.12 18:28:58 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.12 18:28:58 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.12 18:28:58 INFO  Trying to attach to remote debuggee VM localhost:48061 .[0m
[0m2021.07.12 18:28:58 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.12 18:29:00 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.12 18:29:00 ERROR 21/07/12 18:29:00 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.19.149.194 instead (on interface eth0)[0m
[0m2021.07.12 18:29:00 ERROR 21/07/12 18:29:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.12 18:29:01 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.12 18:29:01 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.12 18:29:01 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.12 18:29:01 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.12 18:29:01 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.12 18:29:01 ERROR 21/07/12 18:29:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.12 18:29:03 ERROR 21/07/12 18:29:03 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.[0m
[0m2021.07.12 18:29:03 ERROR 21/07/12 18:29:03 WARN KafkaUtils: overriding enable.auto.commit to false for executor[0m
[0m2021.07.12 18:29:03 ERROR 21/07/12 18:29:03 WARN KafkaUtils: overriding auto.offset.reset to none for executor[0m
[0m2021.07.12 18:29:03 ERROR 21/07/12 18:29:03 WARN KafkaUtils: overriding executor group.id to spark-executor-spark_ml_testing[0m
[0m2021.07.12 18:29:03 ERROR 21/07/12 18:29:03 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135[0m
[0m2021.07.12 18:29:35 INFO  Data: loaded 932.0 rows.[0m
[0m2021.07.12 18:31:01 INFO  Data: loaded 3.0 rows.[0m
[0m2021.07.12 18:31:30 INFO  Data: loaded 35.0 rows.[0m
[0m2021.07.12 18:32:00 INFO  Data: loaded 41.0 rows.[0m
[0m2021.07.12 18:32:30 INFO  Data: loaded 33.0 rows.[0m
[0m2021.07.12 18:33:05 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLRegressionFromKafka][0m
[0m2021.07.12 18:33:05 INFO  Listening for transport dt_socket at address: 57219[0m
[0m2021.07.13 00:54:17 INFO  shutting down Metals[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
Jul 13, 2021 12:54:17 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint notify
INFO: Failed to send notification message.
org.eclipse.lsp4j.jsonrpc.JsonRpcException: java.nio.channels.AsynchronousCloseException
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:72)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.notify(RemoteEndpoint.java:126)
	at org.eclipse.lsp4j.jsonrpc.services.EndpointProxy.invoke(EndpointProxy.java:88)
	at com.sun.proxy.$Proxy11.onBuildExit(Unknown Source)
	at scala.meta.internal.metals.BuildServerConnection.$anonfun$shutdown$1(BuildServerConnection.scala:89)
	at scala.meta.internal.metals.BuildServerConnection.$anonfun$shutdown$1$adapted(BuildServerConnection.scala:85)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.nio.channels.AsynchronousCloseException
	at java.base/java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at java.base/sun.nio.ch.SinkChannelImpl.endWrite(SinkChannelImpl.java:241)
	at java.base/sun.nio.ch.SinkChannelImpl.write(SinkChannelImpl.java:259)
	at java.base/java.nio.channels.Channels.writeFullyImpl(Channels.java:74)
	at java.base/java.nio.channels.Channels.writeFully(Channels.java:94)
	at java.base/java.nio.channels.Channels$1.write(Channels.java:172)
	at java.base/java.io.OutputStream.write(OutputStream.java:122)
	at java.base/java.nio.channels.Channels$1.write(Channels.java:152)
	at scala.meta.internal.metals.ClosableOutputStream.write(ClosableOutputStream.scala:26)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:137)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:67)
	... 14 more

Jul 13, 2021 12:54:17 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint notify
INFO: Failed to send notification message.
org.eclipse.lsp4j.jsonrpc.JsonRpcException: java.nio.channels.AsynchronousCloseException
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:72)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.notify(RemoteEndpoint.java:126)
	at org.eclipse.lsp4j.jsonrpc.services.EndpointProxy.invoke(EndpointProxy.java:88)
	at com.sun.proxy.$Proxy11.onBuildExit(Unknown Source)
	at scala.meta.internal.metals.BuildServerConnection.$anonfun$shutdown$1(BuildServerConnection.scala:89)
	at scala.meta.internal.metals.BuildServerConnection.$anonfun$shutdown$1$adapted(BuildServerConnection.scala:85)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.nio.channels.AsynchronousCloseException
	at java.base/java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at java.base/sun.nio.ch.SinkChannelImpl.endWrite(SinkChannelImpl.java:241)
	at java.base/sun.nio.ch.SinkChannelImpl.write(SinkChannelImpl.java:259)
	at java.base/java.nio.channels.Channels.writeFullyImpl(Channels.java:74)
	at java.base/java.nio.channels.Channels.writeFully(Channels.java:94)
	at java.base/java.nio.channels.Channels$1.write(Channels.java:172)
	at java.base/java.io.OutputStream.write(OutputStream.java:122)
	at java.base/java.nio.channels.Channels$1.write(Channels.java:152)
	at scala.meta.internal.metals.ClosableOutputStream.write(ClosableOutputStream.scala:26)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:137)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:67)
	... 14 more

[0m2021.07.13 00:54:17 INFO  Shut down connection with build server.[0m
[0m2021.07.13 00:54:17 INFO  Shut down connection with build server.[0m
[0m2021.07.13 12:57:42 INFO  Started: Metals version 0.10.4 in workspace '/home/hwait/dev/ht3' for client Visual Studio Code 1.58.0.[0m
[0m2021.07.13 12:57:45 INFO  time: initialize in 2.68s[0m
[0m2021.07.13 12:57:45 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher7838959322054160530/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.07.13 12:57:45 WARN  no build target for: /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLRegressionFromKafka.scala[0m
[0m2021.07.13 12:57:45 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.07.13 12:57:48 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8-19-4d9f966b...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.12.10.
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.6/jline-2.14.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.12/jansi-1.12.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.10/scala-compiler-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.10/scala-reflect-2.12.10.jar
[0m[32m[D][0m Configured SemanticDB in projects 'ht3-test', 'ht3'
[0m[32m[D][0m Loading previous analysis for 'ht3-test' from '/home/hwait/dev/ht3/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Loading previous analysis for 'ht3' from '/home/hwait/dev/ht3/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher7838959322054160530/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher7838959322054160530/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.13 12:57:49 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.13 12:57:49 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher359203240577649943/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher359203240577649943/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher359203240577649943/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.13 12:57:50 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.13 12:57:50 INFO  time: Connected to build server in 4.89s[0m
[0m2021.07.13 12:57:50 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
[0m2021.07.13 12:57:50 INFO  time: Imported build in 0.29s[0m
[0m2021.07.13 12:57:53 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
[0m2021.07.13 12:57:53 INFO  time: code lens generation in 7.2s[0m
[0m2021.07.13 12:57:53 INFO  time: code lens generation in 7.77s[0m
[0m2021.07.13 12:57:55 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.07.13 12:57:55 INFO  time: indexed workspace in 4.97s[0m
Jul 13, 2021 7:59:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 57
[0m2021.07.13 23:04:17 INFO  compiling ht3 (1 scala source)[0m
Jul 13, 2021 11:04:22 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 137
[0m2021.07.13 23:04:23 INFO  time: compiled ht3 in 6.11s[0m
Jul 13, 2021 11:04:26 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 147
Jul 13, 2021 11:04:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 152
Exception in thread "pool-5-thread-1" java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[0m2021.07.13 23:29:49 INFO  time: evaluated worksheet 'check.worksheet.sc' in 7.53s[0m
[0m2021.07.13 23:30:59 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.13 23:31:01 INFO  time: compiled ht3 in 1.39s[0m
[0m2021.07.13 23:31:03 INFO  time: evaluated worksheet 'check.worksheet.sc' in 1.86s[0m
[0m2021.07.13 23:31:26 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.85s[0m
[0m2021.07.13 23:31:38 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.64s[0m
Jul 13, 2021 11:32:02 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 438
[0m2021.07.13 23:32:14 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.13 23:32:14 INFO  time: compiled ht3 in 0.51s[0m
[0m2021.07.13 23:32:19 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.13 23:32:21 INFO  time: compiled ht3 in 1.89s[0m
Jul 13, 2021 11:48:26 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 539
[0m2021.07.13 23:48:27 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.72s[0m
Jul 13, 2021 11:52:27 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 557
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/OneHotEncoderToKnownClassesNumber.scala in org.apache.spark.sql.Dataset[_]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/OneHotEncoderToKnownClassesNumber.scala, 1167, 1167, 1177)
Jul 13, 2021 11:53:55 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 695
[0m2021.07.13 23:53:55 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.46s[0m
Jul 13, 2021 11:53:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 706
Jul 13, 2021 11:54:34 PM scala.meta.internal.pc.CompilerAccess retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.07.13 23:55:53 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.13 23:55:53 INFO  time: compiled ht3 in 0.65s[0m
[0m2021.07.13 23:57:19 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.13 23:57:19 INFO  time: compiled ht3 in 0.83s[0m
[0m2021.07.13 23:57:54 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.13 23:57:54 INFO  time: compiled ht3 in 0.43s[0m
[0m2021.07.13 23:58:20 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.13 23:58:20 INFO  time: compiled ht3 in 0.39s[0m
[0m2021.07.13 23:58:58 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.13 23:58:58 INFO  time: compiled ht3 in 0.34s[0m
Jul 13, 2021 11:59:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1238
[0m2021.07.13 23:59:19 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.13 23:59:19 INFO  time: compiled ht3 in 0.3s[0m
Jul 14, 2021 12:00:35 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1341
Jul 14, 2021 12:00:39 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1349
[0m2021.07.14 00:02:35 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:02:35 INFO  time: compiled ht3 in 0.15s[0m
[0m2021.07.14 00:02:54 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:02:54 INFO  time: compiled ht3 in 0.52s[0m
[0m2021.07.14 00:03:00 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:03:00 INFO  time: compiled ht3 in 0.34s[0m
[0m2021.07.14 00:03:16 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:03:16 INFO  time: compiled ht3 in 0.28s[0m
[0m2021.07.14 00:03:24 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:03:24 INFO  time: compiled ht3 in 0.29s[0m
[0m2021.07.14 00:03:37 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:03:39 INFO  time: compiled ht3 in 1.92s[0m
[0m2021.07.14 00:03:56 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:03:57 INFO  time: compiled ht3 in 1.33s[0m
[0m2021.07.14 00:04:53 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:04:54 INFO  time: compiled ht3 in 1.32s[0m
Jul 14, 2021 12:04:59 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1699
[0m2021.07.14 00:05:29 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:05:29 INFO  time: compiled ht3 in 0.66s[0m
Jul 14, 2021 12:06:34 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1787
[0m2021.07.14 00:06:34 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.71s[0m
[31merror[39m: src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:63: missing parameter type
Note: The expected type requires a one-argument function accepting a 2-Tuple.
      Consider a pattern matching anonymous function, `{ case (name, idx) =>  ... }`
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                              ^^^^
[31merror[39m: src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:69: missing parameter type
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                                    ^^^
[31merror[39m: src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:81: not found: type OneHotEncoderToKnownClassesNumber
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[31merror[39m: src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:155: not found: value c
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                                                                                                                          ^
[0m2021.07.14 00:08:40 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.56s[0m
[0m2021.07.14 00:09:05 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:63: stale mdoc error: missing parameter type
Note: The expected type requires a one-argument function accepting a 2-Tuple.
      Consider a pattern matching anonymous function, `{ case (name, idx) =>  ... }`
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                              ^^^^[0m
[0m2021.07.14 00:09:05 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:69: stale mdoc error: missing parameter type
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                                    ^^^[0m
[0m2021.07.14 00:09:05 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:81: stale mdoc error: not found: type OneHotEncoderToKnownClassesNumber
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.07.14 00:09:05 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:155: stale mdoc error: not found: value c
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                                                                                                                          ^[0m
[0m2021.07.14 00:09:05 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:63: stale mdoc error: missing parameter type
Note: The expected type requires a one-argument function accepting a 2-Tuple.
      Consider a pattern matching anonymous function, `{ case (name, idx) =>  ... }`
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                              ^^^^[0m
[0m2021.07.14 00:09:05 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:69: stale mdoc error: missing parameter type
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                                    ^^^[0m
[0m2021.07.14 00:09:05 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:81: stale mdoc error: not found: type OneHotEncoderToKnownClassesNumber
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.07.14 00:09:05 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:155: stale mdoc error: not found: value c
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                                                                                                                          ^[0m
[0m2021.07.14 00:09:05 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:63: stale mdoc error: missing parameter type
Note: The expected type requires a one-argument function accepting a 2-Tuple.
      Consider a pattern matching anonymous function, `{ case (name, idx) =>  ... }`
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                              ^^^^[0m
[0m2021.07.14 00:09:05 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:69: stale mdoc error: missing parameter type
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                                    ^^^[0m
[0m2021.07.14 00:09:05 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:81: stale mdoc error: not found: type OneHotEncoderToKnownClassesNumber
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.07.14 00:09:05 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:155: stale mdoc error: not found: value c
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                                                                                                                          ^[0m
[0m2021.07.14 00:09:05 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:63: stale mdoc error: missing parameter type
Note: The expected type requires a one-argument function accepting a 2-Tuple.
      Consider a pattern matching anonymous function, `{ case (name, idx) =>  ... }`
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                              ^^^^[0m
[0m2021.07.14 00:09:05 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:69: stale mdoc error: missing parameter type
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                                    ^^^[0m
[0m2021.07.14 00:09:05 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:81: stale mdoc error: not found: type OneHotEncoderToKnownClassesNumber
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.07.14 00:09:05 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:155: stale mdoc error: not found: value c
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => new OneHotEncoderToKnownClassesNumber(idx).setInputCol(name).setOutputCol(s"${c}_enc"))
                                                                                                                                                          ^[0m
Jul 14, 2021 12:09:11 AM scala.meta.internal.pc.CompilerAccess handleError
SEVERE: file%3A%2F%2F%2Fhome%2Fhwait%2Fdev%2Fht3%2Fsrc%2Fmain%2Fscala%2Fru.otus.bigdataml.ht3%2Fcheck.worksheet.sc:16: error: ; expected but ) found
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => s"${c}_enc_${idx}"))
                                                                                               ^
file%3A%2F%2F%2Fhome%2Fhwait%2Fdev%2Fht3%2Fsrc%2Fmain%2Fscala%2Fru.otus.bigdataml.ht3%2Fcheck.worksheet.sc:16: error: ; expected but ) found
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => s"${c}_enc_${idx}"))
                                                                                               ^
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.ScalametaParser.syntaxErrorExpected(ScalametaParser.scala:897)
	at scala.meta.internal.parsers.ScalametaParser.accept(ScalametaParser.scala:903)
	at scala.meta.internal.parsers.ScalametaParser.acceptStatSep(ScalametaParser.scala:912)
	at scala.meta.internal.parsers.ScalametaParser.acceptStatSepOpt(ScalametaParser.scala:916)
	at scala.meta.internal.parsers.ScalametaParser.statSeq(ScalametaParser.scala:4915)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$scriptSource$1(ScalametaParser.scala:5104)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:847)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:878)
	at scala.meta.internal.parsers.ScalametaParser.scriptSource(ScalametaParser.scala:5103)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$source$1(ScalametaParser.scala:5093)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:847)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:878)
	at scala.meta.internal.parsers.ScalametaParser.source(ScalametaParser.scala:5093)
	at scala.meta.internal.parsers.ScalametaParser.entrypointSource(ScalametaParser.scala:5099)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$parseSource$2(ScalametaParser.scala:143)
	at scala.meta.internal.parsers.ScalametaParser.parseRule(ScalametaParser.scala:53)
	at scala.meta.internal.parsers.ScalametaParser.parseSource(ScalametaParser.scala:143)
	at scala.meta.parsers.Parse$.$anonfun$parseSource$1(Parse.scala:29)
	at scala.meta.internal.parsers.ScalametaParser$$anon$261.apply(ScalametaParser.scala:5156)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.semanticdb.scalac.ParseOps$XtensionCompilationUnitSource.toSource(ParseOps.scala:17)
	at scala.meta.internal.semanticdb.scalac.TextDocumentOps$XtensionCompilationUnitDocument.toTextDocument(TextDocumentOps.scala:201)
	at scala.meta.internal.pc.SemanticdbTextDocumentProvider.textDocument(SemanticdbTextDocumentProvider.scala:51)

[31merror[39m: src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:96: ; expected but ) found
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => s"${c}_enc_${idx}"))
                                                                                               ^
[0m2021.07.14 00:09:11 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.31s[0m
Jul 14, 2021 12:09:15 AM scala.meta.internal.pc.CompilerAccess handleError
SEVERE: file%3A%2F%2F%2Fhome%2Fhwait%2Fdev%2Fht3%2Fsrc%2Fmain%2Fscala%2Fru.otus.bigdataml.ht3%2Fcheck.worksheet.sc:16: error: ; expected but ) found
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => s"${c}_enc_${idx}"))
                                                                                               ^
file%3A%2F%2F%2Fhome%2Fhwait%2Fdev%2Fht3%2Fsrc%2Fmain%2Fscala%2Fru.otus.bigdataml.ht3%2Fcheck.worksheet.sc:16: error: ; expected but ) found
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => s"${c}_enc_${idx}"))
                                                                                               ^
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.ScalametaParser.syntaxErrorExpected(ScalametaParser.scala:897)
	at scala.meta.internal.parsers.ScalametaParser.accept(ScalametaParser.scala:903)
	at scala.meta.internal.parsers.ScalametaParser.acceptStatSep(ScalametaParser.scala:912)
	at scala.meta.internal.parsers.ScalametaParser.acceptStatSepOpt(ScalametaParser.scala:916)
	at scala.meta.internal.parsers.ScalametaParser.statSeq(ScalametaParser.scala:4915)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$scriptSource$1(ScalametaParser.scala:5104)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:847)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:878)
	at scala.meta.internal.parsers.ScalametaParser.scriptSource(ScalametaParser.scala:5103)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$source$1(ScalametaParser.scala:5093)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:847)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:878)
	at scala.meta.internal.parsers.ScalametaParser.source(ScalametaParser.scala:5093)
	at scala.meta.internal.parsers.ScalametaParser.entrypointSource(ScalametaParser.scala:5099)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$parseSource$2(ScalametaParser.scala:143)
	at scala.meta.internal.parsers.ScalametaParser.parseRule(ScalametaParser.scala:53)
	at scala.meta.internal.parsers.ScalametaParser.parseSource(ScalametaParser.scala:143)
	at scala.meta.parsers.Parse$.$anonfun$parseSource$1(Parse.scala:29)
	at scala.meta.internal.parsers.ScalametaParser$$anon$261.apply(ScalametaParser.scala:5156)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.semanticdb.scalac.ParseOps$XtensionCompilationUnitSource.toSource(ParseOps.scala:17)
	at scala.meta.internal.semanticdb.scalac.TextDocumentOps$XtensionCompilationUnitDocument.toTextDocument(TextDocumentOps.scala:201)
	at scala.meta.internal.pc.SemanticdbTextDocumentProvider.textDocument(SemanticdbTextDocumentProvider.scala:51)

[31merror[39m: src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:63: missing parameter type
Note: The expected type requires a one-argument function accepting a 2-Tuple.
      Consider a pattern matching anonymous function, `{ case (name, idx) =>  ... }`
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => s"${c}_enc_${idx}")
                                                              ^^^^
[31merror[39m: src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:69: missing parameter type
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => s"${c}_enc_${idx}")
                                                                    ^^^
[31merror[39m: src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:16:81: not found: value c
val encoders = (colsCategorical zip colsCategoricalNum).map ((name, idx) => s"${c}_enc_${idx}")
                                                                                ^
[0m2021.07.14 00:09:17 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.36s[0m
[31merror[39m: src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:18:27: missing parameter type
Note: The expected type requires a one-argument function accepting a 2-Tuple.
      Consider a pattern matching anonymous function, `{ case (name, idx) =>  ... }`
val encoders = map1.map ((name, idx) => s"${c}_enc_${idx}")
                          ^^^^
[31merror[39m: src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:18:33: missing parameter type
val encoders = map1.map ((name, idx) => s"${c}_enc_${idx}")
                                ^^^
[31merror[39m: src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:18:45: not found: value c
val encoders = map1.map ((name, idx) => s"${c}_enc_${idx}")
                                            ^
[0m2021.07.14 00:09:48 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.39s[0m
[0m2021.07.14 00:09:52 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.68s[0m
[31merror[39m: src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:18:27: missing parameter type
Note: The expected type requires a one-argument function accepting a 2-Tuple.
      Consider a pattern matching anonymous function, `{ case (name, idx) =>  ... }`
val encoders = map1.map ((name, idx) => s"${name}_enc_${idx}")
                          ^^^^
[31merror[39m: src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:18:33: missing parameter type
val encoders = map1.map ((name, idx) => s"${name}_enc_${idx}")
                                ^^^
[0m2021.07.14 00:10:10 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.32s[0m
[0m2021.07.14 00:11:38 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.69s[0m
[0m2021.07.14 00:11:49 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.76s[0m
[31merror[39m: src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:18:30: missing parameter type for expanded function ((x$1: <error>) => x$1._1)
val encoders = map1.map (s"${_._1}_enc_${_._2}")
                             ^
[31merror[39m: src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:18:42: missing parameter type for expanded function ((x$2: <error>) => x$2._2)
val encoders = map1.map (s"${_._1}_enc_${_._2}")
                                         ^
[0m2021.07.14 00:12:07 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.28s[0m
[31merror[39m: src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:18:33: missing parameter type for expanded function ((x$2: <error>) => x$2._1)
val encoders = map1.map (_=>s"${_._1}_enc_${_._2}")
                                ^
[31merror[39m: src/main/scala/ru.otus.bigdataml.ht3/check.worksheet.sc:18:45: missing parameter type for expanded function ((x$3: <error>) => x$3._2)
val encoders = map1.map (_=>s"${_._1}_enc_${_._2}")
                                            ^
[0m2021.07.14 00:12:12 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.29s[0m
[0m2021.07.14 00:12:18 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.54s[0m
Jul 14, 2021 12:12:26 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2554
Jul 14, 2021 12:12:43 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2575
Jul 14, 2021 12:13:19 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2585
Jul 14, 2021 12:13:28 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2593
[0m2021.07.14 00:14:07 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:14:07 INFO  time: compiled ht3 in 0.65s[0m
Jul 14, 2021 12:14:12 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2609
Jul 14, 2021 12:14:20 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2614
[0m2021.07.14 00:16:44 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:16:44 INFO  time: compiled ht3 in 0.39s[0m
[0m2021.07.14 00:16:44 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:16:44 INFO  time: compiled ht3 in 0.34s[0m
[0m2021.07.14 00:17:18 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:17:20 INFO  time: compiled ht3 in 1.88s[0m
[0m2021.07.14 00:17:39 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:17:41 INFO  time: compiled ht3 in 1.79s[0m
[0m2021.07.14 00:20:57 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:20:57 INFO  time: compiled ht3 in 0.47s[0m
[0m2021.07.14 00:21:14 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:21:14 INFO  time: compiled ht3 in 0.45s[0m
[0m2021.07.14 00:21:19 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:21:20 INFO  time: compiled ht3 in 1.58s[0m
[0m2021.07.14 00:21:45 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:21:47 INFO  time: compiled ht3 in 1.79s[0m
[0m2021.07.14 00:21:52 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 00:21:52 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 00:21:52 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.14 00:21:52 INFO  Listening for transport dt_socket at address: 40739[0m
[0m2021.07.14 00:21:52 INFO  Trying to attach to remote debuggee VM localhost:40739 .[0m
[0m2021.07.14 00:21:52 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.14 00:21:54 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 00:21:54 ERROR 21/07/14 00:21:54 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.177.44 instead (on interface eth0)[0m
[0m2021.07.14 00:21:54 ERROR 21/07/14 00:21:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 00:21:55 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 00:21:55 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 00:21:55 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 00:21:55 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 00:21:55 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 00:21:55 ERROR 21/07/14 00:21:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 00:22:04 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.14 00:22:05 ERROR 21/07/14 00:22:05 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.14 00:22:12 ERROR 21/07/14 00:22:12 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 79)[0m
[0m2021.07.14 00:22:12 ERROR org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3683/0x000000084155e840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:22:12 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:22:12 ERROR 	... 17 more[0m
[0m2021.07.14 00:22:12 ERROR 21/07/14 00:22:12 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 79) (172.26.177.44 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3683/0x000000084155e840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:22:12 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:22:12 ERROR 	... 17 more[0m
[0m2021.07.14 00:22:12 ERROR [0m
[0m2021.07.14 00:22:12 ERROR 21/07/14 00:22:12 ERROR TaskSetManager: Task 0 in stage 6.0 failed 1 times; aborting job[0m
[0m2021.07.14 00:22:12 ERROR 21/07/14 00:22:12 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 79) (172.26.177.44 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3683/0x000000084155e840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:22:12 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:22:12 ERROR 	... 17 more[0m
[0m2021.07.14 00:22:12 ERROR [0m
[0m2021.07.14 00:22:12 ERROR Driver stacktrace:[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.Option.foreach(Option.scala:407)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2729)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.Dataset.first(Dataset.scala:2736)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.getVectorLengthsFromFirstRow(VectorAssembler.scala:205)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.getLengths(VectorAssembler.scala:231)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:95)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$4(Pipeline.scala:311)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$3(Pipeline.scala:311)[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:198)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$2(Pipeline.scala:310)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$1(Pipeline.scala:308)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.PipelineModel.transform(Pipeline.scala:307)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:117)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
[0m2021.07.14 00:22:12 ERROR Caused by: org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3683/0x000000084155e840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:22:12 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:22:12 ERROR 	... 17 more[0m
[0m2021.07.14 00:22:12 ERROR [0m
[0m2021.07.14 00:22:12 ERROR Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 79) (172.26.177.44 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3683/0x000000084155e840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:22:12 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:22:12 ERROR 	... 17 more[0m
[0m2021.07.14 00:22:12 ERROR [0m
[0m2021.07.14 00:22:12 ERROR Driver stacktrace:[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.Option.foreach(Option.scala:407)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2729)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.Dataset.first(Dataset.scala:2736)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.getVectorLengthsFromFirstRow(VectorAssembler.scala:205)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.getLengths(VectorAssembler.scala:231)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:95)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$4(Pipeline.scala:311)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$3(Pipeline.scala:311)[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:198)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$2(Pipeline.scala:310)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$1(Pipeline.scala:308)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.PipelineModel.transform(Pipeline.scala:307)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:117)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
[0m2021.07.14 00:22:12 ERROR Caused by: org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3683/0x000000084155e840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:22:12 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:22:12 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:22:12 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:22:12 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:22:12 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:22:12 ERROR 	... 17 more[0m
[0m2021.07.14 00:22:25 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
Jul 14, 2021 12:24:51 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3262
Jul 14, 2021 12:25:32 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3289
[0m2021.07.14 00:25:34 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 11h 27m 43.623s)[0m
[0m2021.07.14 00:25:34 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 00:25:34 INFO  Listening for transport dt_socket at address: 39835[0m
[0m2021.07.14 00:25:34 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 00:25:34 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.14 00:25:34 INFO  Trying to attach to remote debuggee VM localhost:39835 .[0m
[0m2021.07.14 00:25:34 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.14 00:25:35 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 00:25:35 ERROR 21/07/14 00:25:35 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.177.44 instead (on interface eth0)[0m
[0m2021.07.14 00:25:35 ERROR 21/07/14 00:25:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 00:25:36 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 00:25:36 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 00:25:36 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 00:25:36 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 00:25:36 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 00:25:36 ERROR 21/07/14 00:25:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 00:25:45 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.14 00:25:47 ERROR 21/07/14 00:25:47 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.14 00:25:53 ERROR 21/07/14 00:25:53 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 79)[0m
[0m2021.07.14 00:25:53 ERROR org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3723/0x00000008415be840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:25:53 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:25:53 ERROR 	... 17 more[0m
[0m2021.07.14 00:25:53 ERROR 21/07/14 00:25:53 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 79) (172.26.177.44 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3723/0x00000008415be840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:25:53 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:25:53 ERROR 	... 17 more[0m
[0m2021.07.14 00:25:53 ERROR [0m
[0m2021.07.14 00:25:53 ERROR 21/07/14 00:25:53 ERROR TaskSetManager: Task 0 in stage 6.0 failed 1 times; aborting job[0m
[0m2021.07.14 00:25:53 ERROR 21/07/14 00:25:53 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 79) (172.26.177.44 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3723/0x00000008415be840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:25:53 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:25:53 ERROR 	... 17 more[0m
[0m2021.07.14 00:25:53 ERROR [0m
[0m2021.07.14 00:25:53 ERROR Driver stacktrace:[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.Option.foreach(Option.scala:407)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2729)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.Dataset.first(Dataset.scala:2736)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.getVectorLengthsFromFirstRow(VectorAssembler.scala:205)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.getLengths(VectorAssembler.scala:231)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:95)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$4(Pipeline.scala:311)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$3(Pipeline.scala:311)[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:198)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$2(Pipeline.scala:310)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$1(Pipeline.scala:308)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.PipelineModel.transform(Pipeline.scala:307)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:117)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
[0m2021.07.14 00:25:53 ERROR Caused by: org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3723/0x00000008415be840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:25:53 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:25:53 ERROR 	... 17 more[0m
[0m2021.07.14 00:25:53 ERROR [0m
[0m2021.07.14 00:25:53 ERROR Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 79) (172.26.177.44 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3723/0x00000008415be840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:25:53 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:25:53 ERROR 	... 17 more[0m
[0m2021.07.14 00:25:53 ERROR [0m
[0m2021.07.14 00:25:53 ERROR Driver stacktrace:[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.Option.foreach(Option.scala:407)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2729)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.Dataset.first(Dataset.scala:2736)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.getVectorLengthsFromFirstRow(VectorAssembler.scala:205)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.getLengths(VectorAssembler.scala:231)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:95)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$4(Pipeline.scala:311)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$3(Pipeline.scala:311)[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:198)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$2(Pipeline.scala:310)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$1(Pipeline.scala:308)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.PipelineModel.transform(Pipeline.scala:307)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:117)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
[0m2021.07.14 00:25:53 ERROR Caused by: org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3723/0x00000008415be840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:25:53 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:25:53 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:25:53 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:25:53 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:25:53 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:25:53 ERROR 	... 17 more[0m
[0m2021.07.14 00:26:03 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
Jul 14, 2021 12:26:05 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3302
Jul 14, 2021 12:27:01 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3395
Jul 14, 2021 12:27:07 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3402
[0m2021.07.14 00:27:25 INFO  compiling ht3 (1 scala source)[0m
Jul 14, 2021 12:27:26 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3476
[0m2021.07.14 00:27:26 INFO  time: compiled ht3 in 1.64s[0m
[0m2021.07.14 00:27:29 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 11h 29m 38.88s)[0m
[0m2021.07.14 00:27:29 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 00:27:29 INFO  Listening for transport dt_socket at address: 47679[0m
[0m2021.07.14 00:27:29 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 00:27:29 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.14 00:27:29 INFO  Trying to attach to remote debuggee VM localhost:47679 .[0m
[0m2021.07.14 00:27:29 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.14 00:27:30 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 00:27:30 ERROR 21/07/14 00:27:30 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.177.44 instead (on interface eth0)[0m
[0m2021.07.14 00:27:30 ERROR 21/07/14 00:27:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 00:27:31 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 00:27:31 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 00:27:31 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 00:27:31 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 00:27:31 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 00:27:31 ERROR 21/07/14 00:27:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 00:27:40 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.14 00:27:41 ERROR 21/07/14 00:27:41 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.14 00:27:49 INFO  0 from 3[0m
[0m2021.07.14 00:27:49 ERROR 21/07/14 00:27:49 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 79)[0m
[0m2021.07.14 00:27:49 ERROR org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3648/0x0000000841546840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:27:49 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:38)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:44)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:44)[0m
[0m2021.07.14 00:27:49 ERROR 	... 17 more[0m
[0m2021.07.14 00:27:49 ERROR 21/07/14 00:27:49 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 79) (172.26.177.44 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3648/0x0000000841546840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:27:49 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:38)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:44)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:44)[0m
[0m2021.07.14 00:27:49 ERROR 	... 17 more[0m
[0m2021.07.14 00:27:49 ERROR [0m
[0m2021.07.14 00:27:49 ERROR 21/07/14 00:27:49 ERROR TaskSetManager: Task 0 in stage 6.0 failed 1 times; aborting job[0m
[0m2021.07.14 00:27:49 ERROR 21/07/14 00:27:49 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 79) (172.26.177.44 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3648/0x0000000841546840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:27:49 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:38)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:44)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:44)[0m
[0m2021.07.14 00:27:49 ERROR 	... 17 more[0m
[0m2021.07.14 00:27:49 ERROR [0m
[0m2021.07.14 00:27:49 ERROR Driver stacktrace:[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.Option.foreach(Option.scala:407)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2729)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.Dataset.first(Dataset.scala:2736)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.getVectorLengthsFromFirstRow(VectorAssembler.scala:205)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.getLengths(VectorAssembler.scala:231)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:95)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$4(Pipeline.scala:311)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$3(Pipeline.scala:311)[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:198)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$2(Pipeline.scala:310)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$1(Pipeline.scala:308)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.PipelineModel.transform(Pipeline.scala:307)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:117)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
[0m2021.07.14 00:27:49 ERROR Caused by: org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3648/0x0000000841546840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:27:49 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:38)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:44)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:44)[0m
[0m2021.07.14 00:27:49 ERROR 	... 17 more[0m
[0m2021.07.14 00:27:49 ERROR [0m
[0m2021.07.14 00:27:49 ERROR Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 79) (172.26.177.44 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3648/0x0000000841546840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:27:49 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:38)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:44)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:44)[0m
[0m2021.07.14 00:27:49 ERROR 	... 17 more[0m
[0m2021.07.14 00:27:49 ERROR [0m
[0m2021.07.14 00:27:49 ERROR Driver stacktrace:[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.Option.foreach(Option.scala:407)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2729)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.Dataset.first(Dataset.scala:2736)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.getVectorLengthsFromFirstRow(VectorAssembler.scala:205)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.getLengths(VectorAssembler.scala:231)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:95)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$4(Pipeline.scala:311)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$3(Pipeline.scala:311)[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:198)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$2(Pipeline.scala:310)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$1(Pipeline.scala:308)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.PipelineModel.transform(Pipeline.scala:307)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:117)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
[0m2021.07.14 00:27:49 ERROR Caused by: org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3648/0x0000000841546840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:27:49 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:27:49 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:27:49 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:27:49 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:38)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:44)[0m
[0m2021.07.14 00:27:49 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:44)[0m
[0m2021.07.14 00:27:49 ERROR 	... 17 more[0m
[0m2021.07.14 00:27:58 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
Jul 14, 2021 12:28:40 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3487
[0m2021.07.14 00:28:40 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.74s[0m
[0m2021.07.14 00:28:47 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.51s[0m
Jul 14, 2021 12:28:53 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3510
Jul 14, 2021 12:31:21 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3547
[0m2021.07.14 00:33:46 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:33:46 INFO  time: compiled ht3 in 0.48s[0m
Jul 14, 2021 12:33:48 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3625
Jul 14, 2021 12:33:49 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3631
[0m2021.07.14 00:33:57 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:33:59 INFO  time: compiled ht3 in 1.4s[0m
Jul 14, 2021 12:33:59 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3671
[0m2021.07.14 00:34:03 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 11h 36m 13.044s)[0m
[0m2021.07.14 00:34:03 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 00:34:03 INFO  Listening for transport dt_socket at address: 50503[0m
[0m2021.07.14 00:34:03 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 00:34:03 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.14 00:34:03 INFO  Trying to attach to remote debuggee VM localhost:50503 .[0m
[0m2021.07.14 00:34:03 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.14 00:34:04 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 00:34:04 ERROR 21/07/14 00:34:04 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.177.44 instead (on interface eth0)[0m
[0m2021.07.14 00:34:04 ERROR 21/07/14 00:34:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 00:34:06 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 00:34:06 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 00:34:06 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 00:34:06 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 00:34:06 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 00:34:06 ERROR 21/07/14 00:34:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 00:34:14 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.14 00:34:16 ERROR 21/07/14 00:34:16 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.14 00:34:22 ERROR 21/07/14 00:34:22 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 79)[0m
[0m2021.07.14 00:34:22 ERROR org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3711/0x000000084154e840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:34:22 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:34:22 ERROR 	... 17 more[0m
[0m2021.07.14 00:34:22 ERROR 21/07/14 00:34:22 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 79) (172.26.177.44 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3711/0x000000084154e840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:34:22 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:34:22 ERROR 	... 17 more[0m
[0m2021.07.14 00:34:22 ERROR [0m
[0m2021.07.14 00:34:22 ERROR 21/07/14 00:34:22 ERROR TaskSetManager: Task 0 in stage 6.0 failed 1 times; aborting job[0m
[0m2021.07.14 00:34:22 ERROR 21/07/14 00:34:22 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 79) (172.26.177.44 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3711/0x000000084154e840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:34:22 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:34:22 ERROR 	... 17 more[0m
[0m2021.07.14 00:34:22 ERROR [0m
[0m2021.07.14 00:34:22 ERROR Driver stacktrace:[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.Option.foreach(Option.scala:407)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2729)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.Dataset.first(Dataset.scala:2736)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.getVectorLengthsFromFirstRow(VectorAssembler.scala:205)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.getLengths(VectorAssembler.scala:231)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:95)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$4(Pipeline.scala:311)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$3(Pipeline.scala:311)[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:198)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$2(Pipeline.scala:310)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$1(Pipeline.scala:308)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.PipelineModel.transform(Pipeline.scala:307)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:117)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
[0m2021.07.14 00:34:22 ERROR Caused by: org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3711/0x000000084154e840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:34:22 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:34:22 ERROR 	... 17 more[0m
[0m2021.07.14 00:34:22 ERROR [0m
[0m2021.07.14 00:34:22 ERROR Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 79) (172.26.177.44 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3711/0x000000084154e840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:34:22 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:34:22 ERROR 	... 17 more[0m
[0m2021.07.14 00:34:22 ERROR [0m
[0m2021.07.14 00:34:22 ERROR Driver stacktrace:[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.Option.foreach(Option.scala:407)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.Dataset.head(Dataset.scala:2729)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.Dataset.first(Dataset.scala:2736)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.getVectorLengthsFromFirstRow(VectorAssembler.scala:205)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.feature.VectorAssembler$.getLengths(VectorAssembler.scala:231)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:95)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$4(Pipeline.scala:311)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$3(Pipeline.scala:311)[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:198)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$2(Pipeline.scala:310)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.PipelineModel.$anonfun$transform$1(Pipeline.scala:308)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.PipelineModel.transform(Pipeline.scala:307)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining$.main(SparkMLModelTraining.scala:117)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining.main(SparkMLModelTraining.scala)[0m
[0m2021.07.14 00:34:22 ERROR Caused by: org.apache.spark.SparkException: Failed to execute user defined function(OneHotEncoderToKnownClassesNumber$$Lambda$3711/0x000000084154e840: (double) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 00:34:22 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 00:34:22 ERROR Caused by: java.lang.IllegalArgumentException: requirement failed: Sparse vectors require that the dimension of the indices match the dimension of the values. You provided 0 indices and  1 values.[0m
[0m2021.07.14 00:34:22 ERROR 	at scala.Predef$.require(Predef.scala:281)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:611)[0m
[0m2021.07.14 00:34:22 ERROR 	at org.apache.spark.ml.linalg.Vectors$.sparse(Vectors.scala:261)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.vectorize(OneHotEncoderToKnownClassesNumber.scala:37)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:34:22 ERROR 	at ru$u002Eotus$u002Ebigdataml$u002Eht3.OneHotEncoderToKnownClassesNumber.$anonfun$vectorize_udf$1$adapted(OneHotEncoderToKnownClassesNumber.scala:43)[0m
[0m2021.07.14 00:34:22 ERROR 	... 17 more[0m
[0m2021.07.14 00:35:34 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
Jul 14, 2021 12:35:37 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3693
Jul 14, 2021 12:36:40 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3704
[0m2021.07.14 00:36:41 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.64s[0m
[0m2021.07.14 00:36:50 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.45s[0m
Jul 14, 2021 12:37:12 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3733
[0m2021.07.14 00:37:17 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:37:19 INFO  time: compiled ht3 in 1.67s[0m
Jul 14, 2021 12:37:24 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3760
[0m2021.07.14 00:37:26 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 11h 39m 35.659s)[0m
[0m2021.07.14 00:37:26 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 00:37:26 INFO  Listening for transport dt_socket at address: 34811[0m
[0m2021.07.14 00:37:26 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 00:37:26 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.14 00:37:26 INFO  Trying to attach to remote debuggee VM localhost:34811 .[0m
[0m2021.07.14 00:37:26 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.14 00:37:27 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 00:37:27 ERROR 21/07/14 00:37:27 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.26.177.44 instead (on interface eth0)[0m
[0m2021.07.14 00:37:27 ERROR 21/07/14 00:37:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 00:37:28 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 00:37:28 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 00:37:28 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 00:37:28 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 00:37:28 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 00:37:28 ERROR 21/07/14 00:37:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 00:37:37 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.14 00:37:39 ERROR 21/07/14 00:37:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.14 00:37:46 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 00:37:46 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.14 00:37:46 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 00:37:46 INFO  |[1.0,0.0,0.0,0.0,...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (8,[1],[1.0])|      (5,[],[])|       (5,[],[])|[0m
[0m2021.07.14 00:37:46 INFO  |[1.0,0.0,0.0,0.0,...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (8,[1],[1.0])|  (5,[4],[1.0])|       (5,[],[])|[0m
[0m2021.07.14 00:37:46 INFO  |(179,[0,6,8,11,22...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (8,[1],[1.0])|  (5,[4],[1.0])|       (5,[],[])|[0m
[0m2021.07.14 00:37:46 INFO  |(179,[0,5,8,28,31...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|                (8,[],[])|      (5,[],[])|       (5,[],[])|[0m
[0m2021.07.14 00:37:46 INFO  |(179,[0,7,8,22,28...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|                (8,[],[])|  (5,[4],[1.0])|       (5,[],[])|[0m
[0m2021.07.14 00:37:46 INFO  |(179,[0,7,8,28,37...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|                (8,[],[])|      (5,[],[])|       (5,[],[])|[0m
[0m2021.07.14 00:37:46 INFO  |(179,[1,3,9,28,37...|                (3,[1],[1.0])|             (5,[0],[1.0])|             (2,[1],[1.0])|                (8,[],[])|      (5,[],[])|       (5,[],[])|[0m
[0m2021.07.14 00:37:46 INFO  |(179,[0,6,8,28,34...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|                (8,[],[])|      (5,[],[])|       (5,[],[])|[0m
[0m2021.07.14 00:37:46 INFO  |(179,[0,6,8,11,30...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (8,[1],[1.0])|      (5,[],[])|       (5,[],[])|[0m
[0m2021.07.14 00:37:46 INFO  |(179,[0,5,8,11,28...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (8,[1],[1.0])|      (5,[],[])|       (5,[],[])|[0m
[0m2021.07.14 00:37:46 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 00:37:46 INFO  only showing top 10 rows[0m
[0m2021.07.14 00:37:46 INFO  [0m
[0m2021.07.14 00:37:47 ERROR 21/07/14 00:37:47 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.14 00:37:47 ERROR 21/07/14 00:37:47 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.14 00:37:53 ERROR 21/07/14 00:37:53 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.14 00:37:53 ERROR 21/07/14 00:37:53 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.14 00:37:58 INFO  Coefficients: [-0.08494333369123472,-0.07870288629855353,-0.02157024726719453,0.08965643706137855,-0.035260724414880436,-0.03336139972936475,-0.01667187600566822,-0.15260668515265635,-0.3185736179068392,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.09558938204799985,-0.1467240608265514,-0.03413348809226261,0.08438951967142026,0.03526034191501996,1.089563414834422,-0.24220644742235023,-0.2552218664495061,-0.19910163699496825,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007625877204562599,0.03885050030846866,0.003518047860299391,-0.023323911851234932,-0.3139634526131445,0.0016032894950288475,-0.024732340111845774,-0.05861362831361694,0.006058484508710499,-0.02507789703812764,-4.3401426296798824E-4,-0.01085339884048514,-0.013356369039272955,0.006236852717879939,0.0,0.07533663900727694,0.03397596232213769,0.012274691545147338,0.02947572129930118,-0.03293049182097023,0.03102502619743231,-0.060504194103579725,-0.06886601997591603,-0.06387160802234047,-0.06386958321270866,0.0,-0.025983647266056342,0.0,0.035716269564364515,-0.060093320482354634,-0.0640758843234995,0.11024710463308296,-0.05376798930811617,-0.05141561696279172,-0.03524156000098747,-0.049023339475539295,-0.0402213303156428,0.0,0.0,0.0,-0.02738090639933708,0.0,0.0,-0.03759571745164571,0.05640514957980577,-0.029232184115918977,0.041120153170711264,0.002666105090338192,-0.03790695595226767,0.03916513524780699,-0.010263552001773826,0.0,0.0,0.0,0.0,-0.010651558419666175,-0.04451798828251661,0.0,-0.04195437233014068,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.035525707781619065,0.0,-0.01087194993938894,0.0,0.0,-0.054615811210484276,0.021380176973030504,0.046280181759740184,-0.053979572601383384,0.039124353577819584,-0.017333379686025368,-0.050394544359349445] Intercept: 0.18467473029955825[0m
[0m2021.07.14 00:37:58 INFO  numIterations: 0[0m
[0m2021.07.14 00:37:58 INFO  objectiveHistory: [0.0][0m
[0m2021.07.14 00:37:58 INFO  +--------------------+[0m
[0m2021.07.14 00:37:58 INFO  |           residuals|[0m
[0m2021.07.14 00:37:58 INFO  +--------------------+[0m
[0m2021.07.14 00:37:58 INFO  |  0.8069982679672075|[0m
[0m2021.07.14 00:37:58 INFO  |-0.12503977531963448|[0m
[0m2021.07.14 00:37:58 INFO  | -0.1830109058178555|[0m
[0m2021.07.14 00:37:58 INFO  |  0.8338566527599081|[0m
[0m2021.07.14 00:37:58 INFO  | -0.1761532519785264|[0m
[0m2021.07.14 00:37:58 INFO  |-0.17615335816061298|[0m
[0m2021.07.14 00:37:58 INFO  |  0.8238464858929834|[0m
[0m2021.07.14 00:37:58 INFO  |-0.17664323261640796|[0m
[0m2021.07.14 00:37:58 INFO  |-0.17274344678798667|[0m
[0m2021.07.14 00:37:58 INFO  | -0.1709386295202867|[0m
[0m2021.07.14 00:37:58 INFO  | -0.1773378205979728|[0m
[0m2021.07.14 00:37:58 INFO  | -0.1381637731291444|[0m
[0m2021.07.14 00:37:58 INFO  | -0.1695445747185052|[0m
[0m2021.07.14 00:37:58 INFO  |-0.15708238885562337|[0m
[0m2021.07.14 00:37:58 INFO  |-0.18519647255900165|[0m
[0m2021.07.14 00:37:58 INFO  |-0.19930421536598408|[0m
[0m2021.07.14 00:37:58 INFO  | -0.1747434658691899|[0m
[0m2021.07.14 00:37:58 INFO  |-0.17644616309908226|[0m
[0m2021.07.14 00:37:58 INFO  |-0.21226368332229376|[0m
[0m2021.07.14 00:37:58 INFO  |-0.15650672115526088|[0m
[0m2021.07.14 00:37:58 INFO  +--------------------+[0m
[0m2021.07.14 00:37:58 INFO  only showing top 20 rows[0m
[0m2021.07.14 00:37:58 INFO  [0m
[0m2021.07.14 00:37:58 INFO  RMSE: 0.3817728207315983[0m
[0m2021.07.14 00:37:58 INFO  r2: 0.013179156503166256[0m
[0m2021.07.14 00:37:58 INFO  +-------------------------+----------------------+----------------------+---------------------+-----------+------------+[0m
[0m2021.07.14 00:37:58 INFO  |instanceId_objectType_num|audit_resourceType_num|metadata_ownerType_num|membership_status_num|log_ownerId|log_authorId|[0m
[0m2021.07.14 00:37:58 INFO  +-------------------------+----------------------+----------------------+---------------------+-----------+------------+[0m
[0m2021.07.14 00:37:58 INFO  |                        0|                     3|                     0|                    1|        5.0|         6.0|[0m
[0m2021.07.14 00:37:58 INFO  |                        0|                     3|                     1|                    1|        4.0|         6.0|[0m
[0m2021.07.14 00:37:58 INFO  |                        0|                     3|                     0|                    1|        4.0|         6.0|[0m
[0m2021.07.14 00:37:58 INFO  |                        0|                     2|                     0|                    8|        5.0|         6.0|[0m
[0m2021.07.14 00:37:58 INFO  |                        0|                     4|                     0|                    8|        4.0|         6.0|[0m
[0m2021.07.14 00:37:58 INFO  |                        0|                     4|                     0|                    8|        5.0|         6.0|[0m
[0m2021.07.14 00:37:58 INFO  |                        1|                     0|                     1|                    8|        5.0|         6.0|[0m
[0m2021.07.14 00:37:58 INFO  |                        0|                     3|                     0|                    8|        5.0|         6.0|[0m
[0m2021.07.14 00:37:58 INFO  |                        0|                     3|                     0|                    1|        5.0|         6.0|[0m
[0m2021.07.14 00:37:58 INFO  |                        0|                     2|                     0|                    1|        5.0|         6.0|[0m
[0m2021.07.14 00:37:58 INFO  +-------------------------+----------------------+----------------------+---------------------+-----------+------------+[0m
[0m2021.07.14 00:37:58 INFO  only showing top 10 rows[0m
[0m2021.07.14 00:37:58 INFO  [0m
[0m2021.07.14 00:37:58 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.14 00:38:58 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:39:00 INFO  time: compiled ht3 in 1.95s[0m
[0m2021.07.14 00:39:38 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 00:39:40 INFO  time: compiled ht3 in 1.62s[0m
[0m2021.07.14 00:40:02 INFO  shutting down Metals[0m
[0m2021.07.14 00:40:02 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
[0m2021.07.14 00:40:02 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.07.14 11:04:17 INFO  Started: Metals version 0.10.4 in workspace '/home/hwait/dev/ht3' for client Visual Studio Code 1.58.1.[0m
[0m2021.07.14 11:04:19 INFO  time: initialize in 2.42s[0m
[0m2021.07.14 11:04:19 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher12741265476975516439/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.07.14 11:04:19 WARN  no build target for: /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala[0m
[0m2021.07.14 11:04:19 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.07.14 11:04:23 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8-19-4d9f966b...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.12.10.
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.6/jline-2.14.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.12/jansi-1.12.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.10/scala-compiler-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.10/scala-reflect-2.12.10.jar
[0m[32m[D][0m Configured SemanticDB in projects 'ht3-test', 'ht3'
[0m[32m[D][0m Loading previous analysis for 'ht3' from '/home/hwait/dev/ht3/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Loading previous analysis for 'ht3-test' from '/home/hwait/dev/ht3/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher12741265476975516439/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher12741265476975516439/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.14 11:04:24 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.14 11:04:24 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher3482132123236722044/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher3482132123236722044/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher3482132123236722044/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.14 11:04:24 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.14 11:04:24 INFO  time: Connected to build server in 5.17s[0m
[0m2021.07.14 11:04:24 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
[0m2021.07.14 11:04:25 INFO  time: Imported build in 0.26s[0m
[0m2021.07.14 11:04:27 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
[0m2021.07.14 11:04:27 INFO  time: code lens generation in 7.39s[0m
[0m2021.07.14 11:04:28 INFO  time: code lens generation in 8.68s[0m
[0m2021.07.14 11:04:29 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.07.14 11:04:29 INFO  time: indexed workspace in 5.15s[0m
Jul 14, 2021 11:35:16 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 133
Jul 14, 2021 11:35:49 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 138
Jul 14, 2021 11:36:04 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 161
Jul 14, 2021 11:36:11 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 172
Jul 14, 2021 11:36:30 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 231
Jul 14, 2021 11:36:44 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 240
Jul 14, 2021 11:40:33 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 358
[0m2021.07.14 11:40:43 INFO  compiling ht3 (2 scala sources)[0m
Jul 14, 2021 11:40:44 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 376
[0m2021.07.14 11:40:47 INFO  time: compiled ht3 in 4.25s[0m
[0m2021.07.14 11:40:47 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 11:40:48 INFO  time: compiled ht3 in 0.99s[0m
Jul 14, 2021 11:40:49 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 383
Jul 14, 2021 11:41:04 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 406
Jul 14, 2021 11:41:12 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 425
Jul 14, 2021 11:41:17 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 435
Jul 14, 2021 11:41:33 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 452
Jul 14, 2021 11:41:43 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 470
[0m2021.07.14 11:41:48 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 11:41:52 INFO  time: compiled ht3 in 4.08s[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala in reflect.runtime.universe.TypeTag[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala, 2271, 2271, 2281)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala in List[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala, 2388, 2388, 2395)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala in Map[T,Int]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala, 2530, 2530, 2540)
Jul 14, 2021 11:51:25 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1551
Jul 14, 2021 12:14:22 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2489
Jul 14, 2021 12:22:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2860
[0m2021.07.14 12:30:17 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 12:30:19 INFO  time: compiled ht3 in 2.76s[0m
Jul 14, 2021 12:33:18 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.InterruptedException
java.util.concurrent.CompletionException: java.lang.InterruptedException
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:319)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:718)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:42)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.pc.VirtualFileParams.checkCanceled(VirtualFileParams.java:25)
	at scala.meta.internal.pc.CompletionProvider.$anonfun$completions$1(CompletionProvider.scala:77)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
	at scala.collection.Iterator.toStream(Iterator.scala:1415)
	at scala.collection.Iterator.toStream$(Iterator.scala:1414)
	at scala.collection.AbstractIterator.toStream(Iterator.scala:1429)
	at scala.collection.Iterator.$anonfun$toStream$1(Iterator.scala:1415)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1171)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1161)
	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1061)
	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1050)
	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1050)
	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1055)
	at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:31)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:134)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:40)
	at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.write(TypeAdapterRuntimeTypeWrapper.java:69)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.write(ReflectiveTypeAdapterFactory.java:127)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.write(ReflectiveTypeAdapterFactory.java:245)
	at com.google.gson.Gson.toJson(Gson.java:704)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:423)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:55)
	at com.google.gson.Gson.toJson(Gson.java:704)
	at com.google.gson.Gson.toJson(Gson.java:683)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:145)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:140)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:59)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.lambda$handleRequest$1(RemoteEndpoint.java:281)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:714)
	... 8 more

Jul 14, 2021 12:33:51 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4178
[0m2021.07.14 12:43:04 INFO  compiling ht3 (1 scala source)[0m
Jul 14, 2021 12:43:04 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4323
[0m2021.07.14 12:43:04 INFO  time: compiled ht3 in 0.86s[0m
Jul 14, 2021 12:43:13 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4329
Jul 14, 2021 12:43:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4358
Jul 14, 2021 12:43:45 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4364
[0m2021.07.14 12:45:01 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 12:45:01 INFO  time: compiled ht3 in 0.47s[0m
[0m2021.07.14 12:45:01 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 12:45:01 INFO  time: compiled ht3 in 0.3s[0m
[0m2021.07.14 12:45:52 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 12:45:52 INFO  time: compiled ht3 in 0.14s[0m
Jul 14, 2021 12:46:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4584
[0m2021.07.14 12:46:56 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 12:46:58 INFO  time: compiled ht3 in 2.27s[0m
[0m2021.07.14 12:47:53 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 12:47:54 INFO  time: compiled ht3 in 1.04s[0m
Jul 14, 2021 12:47:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4689
Jul 14, 2021 12:50:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4815
[0m2021.07.14 12:51:37 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 12:51:39 INFO  time: compiled ht3 in 1.95s[0m
Jul 14, 2021 12:51:39 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4897
Jul 14, 2021 12:52:21 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4927
Jul 14, 2021 12:52:22 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4934
[0m2021.07.14 12:54:03 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 12:54:05 INFO  time: compiled ht3 in 1.41s[0m
Jul 14, 2021 12:56:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5102
Jul 14, 2021 12:57:36 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5136
[0m2021.07.14 12:58:18 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 12:58:18 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 12:58:18 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.14 12:58:18 INFO  Listening for transport dt_socket at address: 36651[0m
[0m2021.07.14 12:58:18 INFO  Trying to attach to remote debuggee VM localhost:36651 .[0m
[0m2021.07.14 12:58:18 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.14 12:58:19 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 12:58:19 ERROR 21/07/14 12:58:19 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.31.26.139 instead (on interface eth0)[0m
[0m2021.07.14 12:58:19 ERROR 21/07/14 12:58:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 12:58:20 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 12:58:20 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 12:58:20 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 12:58:20 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 12:58:20 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 12:58:20 ERROR 21/07/14 12:58:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 12:58:29 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.14 12:58:30 ERROR 21/07/14 12:58:30 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.14 12:58:36 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 12:58:36 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.14 12:58:36 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 12:58:36 INFO  |[1.0,0.0,0.0,0.0,...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (8,[1],[1.0])|  (6,[5],[1.0])|       (6,[],[])|[0m
[0m2021.07.14 12:58:36 INFO  |[1.0,0.0,0.0,0.0,...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (8,[1],[1.0])|  (6,[4],[1.0])|       (6,[],[])|[0m
[0m2021.07.14 12:58:36 INFO  |(177,[0,6,8,11,22...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (8,[1],[1.0])|  (6,[4],[1.0])|       (6,[],[])|[0m
[0m2021.07.14 12:58:36 INFO  |(177,[0,5,8,23,30...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|                (8,[],[])|  (6,[5],[1.0])|       (6,[],[])|[0m
[0m2021.07.14 12:58:36 INFO  |(177,[0,7,8,22,30...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|                (8,[],[])|  (6,[4],[1.0])|       (6,[],[])|[0m
[0m2021.07.14 12:58:36 INFO  |(177,[0,7,8,23,30...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|                (8,[],[])|  (6,[5],[1.0])|       (6,[],[])|[0m
[0m2021.07.14 12:58:36 INFO  |(177,[1,3,9,23,30...|                (3,[1],[1.0])|             (5,[0],[1.0])|             (2,[1],[1.0])|                (8,[],[])|  (6,[5],[1.0])|       (6,[],[])|[0m
[0m2021.07.14 12:58:36 INFO  |(177,[0,6,8,23,30...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|                (8,[],[])|  (6,[5],[1.0])|       (6,[],[])|[0m
[0m2021.07.14 12:58:36 INFO  |(177,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (8,[1],[1.0])|  (6,[5],[1.0])|       (6,[],[])|[0m
[0m2021.07.14 12:58:36 INFO  |(177,[0,5,8,11,23...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (8,[1],[1.0])|  (6,[5],[1.0])|       (6,[],[])|[0m
[0m2021.07.14 12:58:36 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 12:58:36 INFO  only showing top 10 rows[0m
[0m2021.07.14 12:58:36 INFO  [0m
[0m2021.07.14 12:58:38 ERROR 21/07/14 12:58:38 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.14 12:58:38 ERROR 21/07/14 12:58:38 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.14 12:58:42 ERROR 21/07/14 12:58:42 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.14 12:58:42 ERROR 21/07/14 12:58:42 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.14 12:58:46 INFO  Coefficients: [-0.08806494785203728,-0.07870275654353391,-0.018722996831261142,0.08977793560291729,-0.03591229382762058,-0.034223296800220196,-0.016585379062706726,-0.1526063553027683,-0.3185735142521335,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.09558926147734015,-0.14672397544202398,-0.03413347213375431,0.08438949767680379,0.035260507569200024,1.0895639909669084,-0.24220601859414082,-0.255221703128325,-0.1991011737119727,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007631505845330603,0.03885064894837995,0.0035185824445462385,-0.02332242026758806,-0.3139218569871724,0.001603533056943409,-0.024728574613039893,-0.058612741861423574,0.006059350024928699,-0.025074088907241523,-4.3157035972638485E-4,-0.010853314086764843,-0.013356238499154816,0.0058533018334510955,0.0,0.07467043884192406,0.03387457884725231,0.01379208656254529,0.030207550516802673,-0.032930414201143766,0.03175653444072176,-0.06050411109510653,-0.06886592268957026,-0.06810058891681436,-0.06809856410824902,0.0,-0.025983586506423045,0.0,0.03656568770879554,-0.06432228924932176,-0.06863044750735124,0.11377285148277504,-0.07359280279067051,-0.053188699458715925,-0.037642164588416356,-0.049289658862552314,-0.040221296504744194,0.0,0.0,0.0,-0.027380843599785185,0.0,0.0,-0.03759571170001149,0.05711001523214404,-0.029232112022713558,0.04107047391235192,0.0026661651908720906,-0.03790695906294236,0.039165203714166374,-0.010263504726562605,0.0,0.0,0.0,0.0,-0.010651510427363411,-0.04451797199093349,0.0,-0.04195431799631538,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0355257059986166,0.0,-0.010871873837306938,0.0,0.0,-0.055682341301940684,0.02036470242289428,0.04653853889232395] Intercept: 0.18466293560576402[0m
[0m2021.07.14 12:58:46 INFO  numIterations: 0[0m
[0m2021.07.14 12:58:46 INFO  objectiveHistory: [0.0][0m
[0m2021.07.14 12:58:46 INFO  +--------------------+[0m
[0m2021.07.14 12:58:46 INFO  |           residuals|[0m
[0m2021.07.14 12:58:46 INFO  +--------------------+[0m
[0m2021.07.14 12:58:46 INFO  |  0.8069983354454131|[0m
[0m2021.07.14 12:58:46 INFO  | -0.1250396629943703|[0m
[0m2021.07.14 12:58:46 INFO  |-0.18301068516461402|[0m
[0m2021.07.14 12:58:46 INFO  |  0.8338565765261712|[0m
[0m2021.07.14 12:58:46 INFO  |-0.17615305753969535|[0m
[0m2021.07.14 12:58:46 INFO  |-0.17615317539908576|[0m
[0m2021.07.14 12:58:46 INFO  |  0.8238466563317683|[0m
[0m2021.07.14 12:58:46 INFO  |-0.17664305487949558|[0m
[0m2021.07.14 12:58:46 INFO  |-0.17274369508722595|[0m
[0m2021.07.14 12:58:46 INFO  |-0.17093858479897345|[0m
[0m2021.07.14 12:58:46 INFO  |-0.17733809999307334|[0m
[0m2021.07.14 12:58:46 INFO  | -0.1381674303133206|[0m
[0m2021.07.14 12:58:46 INFO  |-0.16954481503055166|[0m
[0m2021.07.14 12:58:46 INFO  |-0.15708236972998046|[0m
[0m2021.07.14 12:58:46 INFO  | -0.1851970271924482|[0m
[0m2021.07.14 12:58:46 INFO  |-0.19930423774363942|[0m
[0m2021.07.14 12:58:46 INFO  |-0.17474338802233325|[0m
[0m2021.07.14 12:58:46 INFO  |-0.17644605094695628|[0m
[0m2021.07.14 12:58:46 INFO  | -0.2122640790637656|[0m
[0m2021.07.14 12:58:46 INFO  |-0.15650982873995076|[0m
[0m2021.07.14 12:58:46 INFO  +--------------------+[0m
[0m2021.07.14 12:58:46 INFO  only showing top 20 rows[0m
[0m2021.07.14 12:58:46 INFO  [0m
[0m2021.07.14 12:58:46 INFO  RMSE: 0.3817729869930513[0m
[0m2021.07.14 12:58:46 INFO  r2: 0.013178296985179916[0m
[0m2021.07.14 12:58:47 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
Jul 14, 2021 1:00:36 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5156
Jul 14, 2021 1:00:46 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5165
Jul 14, 2021 1:02:16 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5249
Jul 14, 2021 1:02:44 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5267
Jul 14, 2021 1:02:53 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5280
Jul 14, 2021 1:03:02 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5294
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala in Array[String]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala, 673, 673, 686)
[0m2021.07.14 13:03:24 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 13:03:26 INFO  time: compiled ht3 in 1.37s[0m
[0m2021.07.14 13:03:29 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 13:03:31 INFO  time: compiled ht3 in 1.83s[0m
Jul 14, 2021 1:23:11 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5352
Jul 14, 2021 1:23:16 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5364
Jul 14, 2021 1:23:17 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5370
Jul 14, 2021 1:23:34 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5377
[0m2021.07.14 13:23:39 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 13:23:40 INFO  time: compiled ht3 in 1.1s[0m
Jul 14, 2021 1:23:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5389
[0m2021.07.14 13:23:50 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 2h 19m 25.71s)[0m
[0m2021.07.14 13:23:50 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 13:23:50 INFO  Listening for transport dt_socket at address: 46795[0m
[0m2021.07.14 13:23:50 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 13:23:50 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.14 13:23:50 INFO  Trying to attach to remote debuggee VM localhost:46795 .[0m
[0m2021.07.14 13:23:50 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.14 13:23:51 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 13:23:51 ERROR 21/07/14 13:23:51 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.31.26.139 instead (on interface eth0)[0m
[0m2021.07.14 13:23:51 ERROR 21/07/14 13:23:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 13:23:52 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 13:23:52 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 13:23:52 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 13:23:52 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 13:23:52 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 13:23:52 ERROR 21/07/14 13:23:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 13:24:01 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.14 13:24:03 ERROR 21/07/14 13:24:03 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.14 13:24:09 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 13:24:09 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.14 13:24:09 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 13:24:09 INFO  |[1.0,0.0,0.0,0.0,...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (8,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:24:09 INFO  |[1.0,0.0,0.0,0.0,...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (8,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:24:09 INFO  |(178,[0,6,8,11,22...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (8,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:24:09 INFO  |(178,[0,5,8,23,30...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|                (8,[],[])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:24:09 INFO  |(178,[0,7,8,22,30...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|                (8,[],[])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:24:09 INFO  |(178,[0,7,8,23,30...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|                (8,[],[])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:24:09 INFO  |(178,[1,3,9,23,30...|                (3,[1],[1.0])|             (5,[0],[1.0])|             (2,[1],[1.0])|                (8,[],[])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:24:09 INFO  |(178,[0,6,8,23,30...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|                (8,[],[])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:24:09 INFO  |(178,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (8,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:24:09 INFO  |(178,[0,5,8,11,23...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (8,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:24:09 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 13:24:09 INFO  only showing top 10 rows[0m
[0m2021.07.14 13:24:09 INFO  [0m
[0m2021.07.14 13:24:10 ERROR 21/07/14 13:24:10 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.14 13:24:10 ERROR 21/07/14 13:24:10 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.14 13:24:14 ERROR 21/07/14 13:24:14 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.14 13:24:14 ERROR 21/07/14 13:24:14 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.14 13:24:18 INFO  Coefficients: [-0.08806494785203728,-0.07870275654353391,-0.018722996831261142,0.08977793560291729,-0.03591229382762058,-0.034223296800220196,-0.016585379062706726,-0.1526063553027683,-0.3185735142521335,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.09558926147734015,-0.14672397544202398,-0.03413347213375431,0.08438949767680379,0.035260507569200024,1.0895639909669084,-0.24220601859414082,-0.255221703128325,-0.1991011737119727,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007631505845330603,0.03885064894837995,0.0035185824445462385,-0.02332242026758806,-0.3139218569871724,0.001603533056943409,-0.024728574613039893,-0.058612741861423574,0.006059350024928699,-0.025074088907241523,-4.3157035972638485E-4,-0.010853314086764843,-0.013356238499154816,0.0058533018334510955,0.0,0.07467043884192406,0.03387457884725231,0.01379208656254529,0.030207550516802673,-0.032930414201143766,0.03175653444072176,-0.06050411109510653,-0.06886592268957026,-0.06810058891681436,-0.06809856410824902,0.0,-0.025983586506423045,0.0,0.03656568770879554,-0.06432228924932176,-0.06863044750735124,0.11377285148277504,-0.07359280279067051,-0.053188699458715925,-0.037642164588416356,-0.049289658862552314,-0.040221296504744194,0.0,0.0,0.0,-0.027380843599785185,0.0,0.0,-0.03759571170001149,0.05711001523214404,-0.029232112022713558,0.04107047391235192,0.0026661651908720906,-0.03790695906294236,0.039165203714166374,-0.010263504726562605,0.0,0.0,0.0,0.0,-0.010651510427363411,-0.04451797199093349,0.0,-0.04195431799631538,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0355257059986166,0.0,-0.010871873837306938,0.0,0.0,-0.055682341301940684,0.02036470242289428,0.04653853889232395] Intercept: 0.18466293560576402[0m
[0m2021.07.14 13:24:18 INFO  numIterations: 0[0m
[0m2021.07.14 13:24:18 INFO  objectiveHistory: [0.0][0m
[0m2021.07.14 13:24:18 INFO  +--------------------+[0m
[0m2021.07.14 13:24:18 INFO  |           residuals|[0m
[0m2021.07.14 13:24:18 INFO  +--------------------+[0m
[0m2021.07.14 13:24:18 INFO  |  0.8069983354454131|[0m
[0m2021.07.14 13:24:18 INFO  | -0.1250396629943703|[0m
[0m2021.07.14 13:24:18 INFO  |-0.18301068516461402|[0m
[0m2021.07.14 13:24:18 INFO  |  0.8338565765261712|[0m
[0m2021.07.14 13:24:18 INFO  |-0.17615305753969535|[0m
[0m2021.07.14 13:24:18 INFO  |-0.17615317539908576|[0m
[0m2021.07.14 13:24:18 INFO  |  0.8238466563317683|[0m
[0m2021.07.14 13:24:18 INFO  |-0.17664305487949558|[0m
[0m2021.07.14 13:24:18 INFO  |-0.17274369508722595|[0m
[0m2021.07.14 13:24:18 INFO  |-0.17093858479897345|[0m
[0m2021.07.14 13:24:18 INFO  |-0.17733809999307334|[0m
[0m2021.07.14 13:24:18 INFO  | -0.1381674303133206|[0m
[0m2021.07.14 13:24:18 INFO  |-0.16954481503055166|[0m
[0m2021.07.14 13:24:18 INFO  |-0.15708236972998046|[0m
[0m2021.07.14 13:24:18 INFO  | -0.1851970271924482|[0m
[0m2021.07.14 13:24:18 INFO  |-0.19930423774363942|[0m
[0m2021.07.14 13:24:18 INFO  |-0.17474338802233325|[0m
[0m2021.07.14 13:24:18 INFO  |-0.17644605094695628|[0m
[0m2021.07.14 13:24:18 INFO  | -0.2122640790637656|[0m
[0m2021.07.14 13:24:18 INFO  |-0.15650982873995076|[0m
[0m2021.07.14 13:24:18 INFO  +--------------------+[0m
[0m2021.07.14 13:24:18 INFO  only showing top 20 rows[0m
[0m2021.07.14 13:24:18 INFO  [0m
[0m2021.07.14 13:24:18 INFO  RMSE: 0.3817729869930513[0m
[0m2021.07.14 13:24:18 INFO  r2: 0.013178296985179916[0m
[0m2021.07.14 13:24:19 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
Jul 14, 2021 1:26:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5413
[0m2021.07.14 13:26:22 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 2h 21m 57.791s)[0m
[0m2021.07.14 13:26:22 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 13:26:22 INFO  Listening for transport dt_socket at address: 51971[0m
[0m2021.07.14 13:26:22 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 13:26:22 INFO  Starting debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
[0m2021.07.14 13:26:22 INFO  Trying to attach to remote debuggee VM localhost:51971 .[0m
[0m2021.07.14 13:26:22 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.14 13:26:23 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 13:26:23 ERROR 21/07/14 13:26:23 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.31.26.139 instead (on interface eth0)[0m
[0m2021.07.14 13:26:23 ERROR 21/07/14 13:26:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 13:26:24 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 13:26:24 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 13:26:24 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 13:26:24 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 13:26:24 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 13:26:24 ERROR 21/07/14 13:26:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 13:26:33 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.14 13:26:35 ERROR 21/07/14 13:26:35 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.14 13:26:41 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 13:26:41 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.14 13:26:41 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 13:26:41 INFO  |[1.0,0.0,0.0,0.0,...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (8,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:26:41 INFO  |[1.0,0.0,0.0,0.0,...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (8,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:26:41 INFO  |(178,[0,6,8,11,22...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (8,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:26:41 INFO  |(178,[0,5,8,23,30...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|                (8,[],[])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:26:41 INFO  |(178,[0,7,8,22,30...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|                (8,[],[])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:26:41 INFO  |(178,[0,7,8,23,30...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|                (8,[],[])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:26:41 INFO  |(178,[1,3,9,23,30...|                (3,[1],[1.0])|             (5,[0],[1.0])|             (2,[1],[1.0])|                (8,[],[])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:26:41 INFO  |(178,[0,6,8,23,30...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|                (8,[],[])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:26:41 INFO  |(178,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (8,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:26:41 INFO  |(178,[0,5,8,11,23...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (8,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 13:26:41 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 13:26:41 INFO  only showing top 10 rows[0m
[0m2021.07.14 13:26:41 INFO  [0m
[0m2021.07.14 13:26:43 ERROR 21/07/14 13:26:43 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.14 13:26:43 ERROR 21/07/14 13:26:43 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.14 13:26:47 ERROR 21/07/14 13:26:47 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.14 13:26:47 ERROR 21/07/14 13:26:47 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.14 13:26:51 INFO  Coefficients: [-0.08806494785203745,-0.0787027565435339,-0.01872299683126128,0.08977793560291711,-0.035912293827620755,-0.034223296800220404,-0.01658537906270698,-0.15260635530276848,-0.3185735142521332,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.09558926147734012,-0.14672397544202395,-0.034133472133754304,0.08438949767680395,0.03526050756919984,1.0895639909669081,-0.24220601859414073,-0.255221703128325,-0.19910117371197306,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007631505845330891,0.038850648948379994,0.003518582444546314,-0.023322420267589003,-0.31392185698716696,0.0016035330569434982,-0.02472857461303762,-0.05861274186142332,0.006059350024928985,-0.025074088907245294,-4.3157035972653756E-4,-0.010853314086764853,-0.013356238499154887,0.0058533018334508595,0.0,0.07467043884192381,0.03387457884725228,0.01379208656254508,0.030207550516802583,-0.03293041420114378,0.03175653444072157,-0.06050411109510653,-0.06886592268957022,-0.06810058891681453,-0.06809856410824909,0.0,-0.025983586506423004,0.0,0.03656568770879546,-0.06432228924932178,-0.06863044750735128,0.11377285148277487,-0.07359280279067074,-0.05318869945871625,-0.037642164588416516,-0.04928965886255234,-0.04022129650474419,0.0,0.0,0.0,-0.027380843599785175,0.0,0.0,-0.03759571170001147,0.05711001523214386,-0.029232112022713596,0.041070473912351815,0.0026661651908720975,-0.03790695906294235,0.039165203714166354,-0.010263504726562601,0.0,0.0,0.0,0.0,-0.010651510427363403,-0.04451797199093349,0.0,-0.04195431799631536,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.035525705998616576,0.0,-0.010871873837306948,0.0,0.0,-0.055682341301940906,0.020364702422894145,0.046538538892323865] Intercept: 0.1846629356057651[0m
[0m2021.07.14 13:26:51 INFO  numIterations: 0[0m
[0m2021.07.14 13:26:51 INFO  objectiveHistory: [0.0][0m
[0m2021.07.14 13:26:51 INFO  +--------------------+[0m
[0m2021.07.14 13:26:51 INFO  |           residuals|[0m
[0m2021.07.14 13:26:51 INFO  +--------------------+[0m
[0m2021.07.14 13:26:51 INFO  |  0.8069983354454132|[0m
[0m2021.07.14 13:26:51 INFO  |-0.12503966299437022|[0m
[0m2021.07.14 13:26:51 INFO  |  -0.183010685164614|[0m
[0m2021.07.14 13:26:51 INFO  |  0.8338565765261714|[0m
[0m2021.07.14 13:26:51 INFO  |-0.17615305753969526|[0m
[0m2021.07.14 13:26:51 INFO  |-0.17615317539908565|[0m
[0m2021.07.14 13:26:51 INFO  |  0.8238466563317685|[0m
[0m2021.07.14 13:26:51 INFO  |-0.17664305487949547|[0m
[0m2021.07.14 13:26:51 INFO  |-0.17274369508722567|[0m
[0m2021.07.14 13:26:51 INFO  |-0.17093858479897336|[0m
[0m2021.07.14 13:26:51 INFO  |  -0.177338099993073|[0m
[0m2021.07.14 13:26:51 INFO  | -0.1381674303133197|[0m
[0m2021.07.14 13:26:51 INFO  |-0.16954481503055138|[0m
[0m2021.07.14 13:26:51 INFO  |-0.15708236972998038|[0m
[0m2021.07.14 13:26:51 INFO  |-0.18519702719244793|[0m
[0m2021.07.14 13:26:51 INFO  |-0.19930423774363934|[0m
[0m2021.07.14 13:26:51 INFO  |-0.17474338802233316|[0m
[0m2021.07.14 13:26:51 INFO  | -0.1764460509469562|[0m
[0m2021.07.14 13:26:51 INFO  |-0.21226407906376535|[0m
[0m2021.07.14 13:26:51 INFO  | -0.1565098287399502|[0m
[0m2021.07.14 13:26:51 INFO  +--------------------+[0m
[0m2021.07.14 13:26:51 INFO  only showing top 20 rows[0m
[0m2021.07.14 13:26:51 INFO  [0m
[0m2021.07.14 13:26:51 INFO  RMSE: 0.3817729869930513[0m
[0m2021.07.14 13:26:51 INFO  r2: 0.013178296985179916[0m
[0m2021.07.14 13:26:52 INFO  Canceling debug proxy for [ru$u002Eotus$u002Ebigdataml$u002Eht3.SparkMLModelTraining][0m
Jul 14, 2021 1:27:48 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5423
[0m2021.07.14 13:27:54 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 13:27:55 INFO  time: compiled ht3 in 1.16s[0m
Jul 14, 2021 1:27:58 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5437
Jul 14, 2021 1:28:03 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5447
[0m2021.07.14 13:28:25 INFO  compiling ht3 (5 scala sources)[0m
Jul 14, 2021 1:28:26 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5456
[0m2021.07.14 13:28:28 INFO  time: compiled ht3 in 2.56s[0m
Jul 14, 2021 1:28:30 PM scala.meta.internal.metals.Docstrings indexSymbol
SEVERE: file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataColumns.scala
java.nio.file.NoSuchFileException: /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataColumns.scala
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:219)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:371)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:422)
	at java.base/java.nio.file.Files.readAllBytes(Files.java:3206)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:437)
	at scala.meta.internal.metals.Docstrings.indexSymbolDefinition(Docstrings.scala:90)
	at scala.meta.internal.metals.Docstrings.indexSymbol(Docstrings.scala:74)
	at scala.meta.internal.metals.Docstrings.documentation(Docstrings.scala:39)
	at scala.meta.internal.metals.MetalsSymbolSearch.documentation(MetalsSymbolSearch.scala:38)
	at scala.meta.internal.pc.MetalsGlobal.symbolDocumentation(MetalsGlobal.scala:162)
	at scala.meta.internal.pc.Signatures$SignaturePrinter.<init>(Signatures.scala:277)
	at scala.meta.internal.pc.HoverProvider.toHover(HoverProvider.scala:199)
	at scala.meta.internal.pc.HoverProvider.hover(HoverProvider.scala:82)
	at scala.meta.internal.pc.ScalaPresentationCompiler.$anonfun$hover$1(ScalaPresentationCompiler.scala:185)
	at scala.meta.internal.pc.CompilerAccess.withSharedCompiler(CompilerAccess.scala:137)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$withNonInterruptableCompiler$1(CompilerAccess.scala:125)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:197)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Jul 14, 2021 1:28:42 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5477
Jul 14, 2021 1:28:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5484
Jul 14, 2021 1:29:26 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5494
Jul 14, 2021 1:32:39 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5511
Jul 14, 2021 1:32:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5519
Jul 14, 2021 1:33:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5549
Jul 14, 2021 1:34:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5560
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataStreamer.scala in reflect.runtime.universe.TypeTag[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataStreamer.scala, 2753, 2753, 2763)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataStreamer.scala in reflect.runtime.universe.TypeTag[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataStreamer.scala, 2753, 2753, 2763)
Jul 14, 2021 7:24:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5640
Jul 14, 2021 7:31:21 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5684
Jul 14, 2021 7:31:27 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5691
Jul 14, 2021 7:31:36 PM scala.meta.internal.metals.Docstrings indexSymbol
SEVERE: file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataColumns.scala
java.nio.file.NoSuchFileException: /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataColumns.scala
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:219)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:371)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:422)
	at java.base/java.nio.file.Files.readAllBytes(Files.java:3206)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:437)
	at scala.meta.internal.metals.Docstrings.indexSymbolDefinition(Docstrings.scala:90)
	at scala.meta.internal.metals.Docstrings.indexSymbol(Docstrings.scala:74)
	at scala.meta.internal.metals.Docstrings.documentation(Docstrings.scala:39)
	at scala.meta.internal.metals.MetalsSymbolSearch.documentation(MetalsSymbolSearch.scala:38)
	at scala.meta.internal.pc.MetalsGlobal.symbolDocumentation(MetalsGlobal.scala:162)
	at scala.meta.internal.pc.Signatures$SignaturePrinter.<init>(Signatures.scala:277)
	at scala.meta.internal.pc.HoverProvider.toHover(HoverProvider.scala:199)
	at scala.meta.internal.pc.HoverProvider.hover(HoverProvider.scala:50)
	at scala.meta.internal.pc.ScalaPresentationCompiler.$anonfun$hover$1(ScalaPresentationCompiler.scala:185)
	at scala.meta.internal.pc.CompilerAccess.withSharedCompiler(CompilerAccess.scala:137)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$withNonInterruptableCompiler$1(CompilerAccess.scala:125)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:197)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Jul 14, 2021 7:37:26 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5736
Jul 14, 2021 7:37:28 PM scala.meta.internal.metals.Docstrings indexSymbol
SEVERE: file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataColumns.scala
java.nio.file.NoSuchFileException: /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataColumns.scala
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:219)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:371)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:422)
	at java.base/java.nio.file.Files.readAllBytes(Files.java:3206)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:437)
	at scala.meta.internal.metals.Docstrings.indexSymbolDefinition(Docstrings.scala:90)
	at scala.meta.internal.metals.Docstrings.indexSymbol(Docstrings.scala:74)
	at scala.meta.internal.metals.Docstrings.documentation(Docstrings.scala:39)
	at scala.meta.internal.metals.MetalsSymbolSearch.documentation(MetalsSymbolSearch.scala:38)
	at scala.meta.internal.pc.MetalsGlobal.symbolDocumentation(MetalsGlobal.scala:162)
	at scala.meta.internal.pc.Signatures$SignaturePrinter.<init>(Signatures.scala:277)
	at scala.meta.internal.pc.HoverProvider.toHover(HoverProvider.scala:199)
	at scala.meta.internal.pc.HoverProvider.hover(HoverProvider.scala:82)
	at scala.meta.internal.pc.ScalaPresentationCompiler.$anonfun$hover$1(ScalaPresentationCompiler.scala:185)
	at scala.meta.internal.pc.CompilerAccess.withSharedCompiler(CompilerAccess.scala:137)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$withNonInterruptableCompiler$1(CompilerAccess.scala:125)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:197)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Jul 14, 2021 7:37:37 PM scala.meta.internal.metals.Docstrings indexSymbol
SEVERE: file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataColumns.scala
java.nio.file.NoSuchFileException: /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataColumns.scala
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:219)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:371)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:422)
	at java.base/java.nio.file.Files.readAllBytes(Files.java:3206)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:437)
	at scala.meta.internal.metals.Docstrings.indexSymbolDefinition(Docstrings.scala:90)
	at scala.meta.internal.metals.Docstrings.indexSymbol(Docstrings.scala:74)
	at scala.meta.internal.metals.Docstrings.documentation(Docstrings.scala:39)
	at scala.meta.internal.metals.MetalsSymbolSearch.documentation(MetalsSymbolSearch.scala:38)
	at scala.meta.internal.pc.MetalsGlobal.symbolDocumentation(MetalsGlobal.scala:162)
	at scala.meta.internal.pc.Signatures$SignaturePrinter.<init>(Signatures.scala:277)
	at scala.meta.internal.pc.HoverProvider.toHover(HoverProvider.scala:199)
	at scala.meta.internal.pc.HoverProvider.$anonfun$hover$5(HoverProvider.scala:57)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.pc.HoverProvider.$anonfun$hover$3(HoverProvider.scala:55)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.pc.HoverProvider.hover(HoverProvider.scala:54)
	at scala.meta.internal.pc.ScalaPresentationCompiler.$anonfun$hover$1(ScalaPresentationCompiler.scala:185)
	at scala.meta.internal.pc.CompilerAccess.withSharedCompiler(CompilerAccess.scala:137)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$withNonInterruptableCompiler$1(CompilerAccess.scala:125)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:197)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Jul 14, 2021 7:37:41 PM scala.meta.internal.metals.Docstrings indexSymbol
SEVERE: file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataColumns.scala
java.nio.file.NoSuchFileException: /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataColumns.scala
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:219)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:371)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:422)
	at java.base/java.nio.file.Files.readAllBytes(Files.java:3206)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:437)
	at scala.meta.internal.metals.Docstrings.indexSymbolDefinition(Docstrings.scala:90)
	at scala.meta.internal.metals.Docstrings.indexSymbol(Docstrings.scala:74)
	at scala.meta.internal.metals.Docstrings.documentation(Docstrings.scala:39)
	at scala.meta.internal.metals.MetalsSymbolSearch.documentation(MetalsSymbolSearch.scala:38)
	at scala.meta.internal.pc.MetalsGlobal.symbolDocumentation(MetalsGlobal.scala:162)
	at scala.meta.internal.pc.Signatures$SignaturePrinter.<init>(Signatures.scala:277)
	at scala.meta.internal.pc.HoverProvider.toHover(HoverProvider.scala:199)
	at scala.meta.internal.pc.HoverProvider.hover(HoverProvider.scala:82)
	at scala.meta.internal.pc.ScalaPresentationCompiler.$anonfun$hover$1(ScalaPresentationCompiler.scala:185)
	at scala.meta.internal.pc.CompilerAccess.withSharedCompiler(CompilerAccess.scala:137)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$withNonInterruptableCompiler$1(CompilerAccess.scala:125)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:197)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Jul 14, 2021 7:38:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5771
Jul 14, 2021 7:43:57 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5786
Jul 14, 2021 7:47:04 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5825
[0m2021.07.14 19:47:13 ERROR code navigation does not work for the file '/home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider copy.scala' because the build target ht3 is being compiled. Wait until compilation is finished and try again.[0m
Jul 14, 2021 7:47:17 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5835
Jul 14, 2021 7:47:20 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5841
Jul 14, 2021 7:47:33 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5867
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in List[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 1957, 1957, 1964)
Jul 14, 2021 7:47:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5881
Jul 14, 2021 7:47:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5882
Jul 14, 2021 7:47:44 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5884
Jul 14, 2021 7:47:45 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5890
Jul 14, 2021 7:47:45 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5891
Jul 14, 2021 7:48:03 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5934
Jul 14, 2021 7:48:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5940
Jul 14, 2021 7:48:23 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5968
Jul 14, 2021 7:49:33 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6083
Jul 14, 2021 7:51:17 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6246
Jul 14, 2021 7:51:23 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6257
Jul 14, 2021 7:51:28 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6269
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in Array[String]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 1072, 1072, 1085)
Jul 14, 2021 7:52:28 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6331
Jul 14, 2021 7:52:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6337
Jul 14, 2021 7:52:33 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6344
Jul 14, 2021 7:52:35 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6350
Jul 14, 2021 7:53:51 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6389
Jul 14, 2021 7:54:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6404
Jul 14, 2021 7:55:03 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6427
Jul 14, 2021 7:55:04 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6433
[0m2021.07.14 19:55:28 INFO  shutting down Metals[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
Jul 14, 2021 7:55:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint notify
INFO: Failed to send notification message.
org.eclipse.lsp4j.jsonrpc.JsonRpcException: java.nio.channels.AsynchronousCloseException
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:72)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.notify(RemoteEndpoint.java:126)
	at org.eclipse.lsp4j.jsonrpc.services.EndpointProxy.invoke(EndpointProxy.java:88)
	at com.sun.proxy.$Proxy11.onBuildExit(Unknown Source)
	at scala.meta.internal.metals.BuildServerConnection.$anonfun$shutdown$1(BuildServerConnection.scala:89)
	at scala.meta.internal.metals.BuildServerConnection.$anonfun$shutdown$1$adapted(BuildServerConnection.scala:85)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.nio.channels.AsynchronousCloseException
	at java.base/java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at java.base/sun.nio.ch.SinkChannelImpl.endWrite(SinkChannelImpl.java:241)
	at java.base/sun.nio.ch.SinkChannelImpl.write(SinkChannelImpl.java:259)
	at java.base/java.nio.channels.Channels.writeFullyImpl(Channels.java:74)
	at java.base/java.nio.channels.Channels.writeFully(Channels.java:94)
	at java.base/java.nio.channels.Channels$1.write(Channels.java:172)
	at java.base/java.io.OutputStream.write(OutputStream.java:122)
	at java.base/java.nio.channels.Channels$1.write(Channels.java:152)
	at scala.meta.internal.metals.ClosableOutputStream.write(ClosableOutputStream.scala:26)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:137)
	at java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:67)
	... 14 more

[0m2021.07.14 19:55:29 INFO  Shut down connection with build server.[0m
[0m2021.07.14 19:55:30 ERROR timeout: build server 'bloop' during shutdown[0m
java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@45bc5bb6 rejected from java.util.concurrent.ThreadPoolExecutor@795908a0[Shutting down, pool size = 3, active threads = 3, queued tasks = 0, completed tasks = 43222]
	at java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)
	at java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)
	at java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[0m2021.07.14 19:56:03 INFO  Started: Metals version 0.10.4 in workspace '/home/hwait/dev/ht3' for client Visual Studio Code 1.58.1.[0m
[0m2021.07.14 19:56:06 INFO  time: initialize in 3.02s[0m
[0m2021.07.14 19:56:06 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher7414327055024472877/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.07.14 19:56:06 WARN  no build target for: /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala[0m
[0m2021.07.14 19:56:06 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.07.14 19:56:10 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8-19-4d9f966b...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.12.10.
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.6/jline-2.14.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.12/jansi-1.12.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.10/scala-compiler-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.10/scala-reflect-2.12.10.jar
[0m[32m[D][0m Configured SemanticDB in projects 'ht3', 'ht3-test'
[0m[32m[D][0m Loading previous analysis for 'ht3-test' from '/home/hwait/dev/ht3/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Loading previous analysis for 'ht3' from '/home/hwait/dev/ht3/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher7414327055024472877/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher7414327055024472877/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.14 19:56:11 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.14 19:56:11 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher16594733467019687028/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher16594733467019687028/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher16594733467019687028/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.14 19:56:11 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.14 19:56:11 INFO  time: Connected to build server in 4.73s[0m
[0m2021.07.14 19:56:11 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
[0m2021.07.14 19:56:11 INFO  time: Imported build in 0.14s[0m
[0m2021.07.14 19:56:14 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
[0m2021.07.14 19:56:14 INFO  time: code lens generation in 7.4s[0m
[0m2021.07.14 19:56:14 INFO  time: code lens generation in 7.67s[0m
[0m2021.07.14 19:56:15 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.07.14 19:56:15 INFO  time: indexed workspace in 4.48s[0m
[0m2021.07.14 19:56:17 INFO  compiling ht3 (6 scala sources)[0m
[0m2021.07.14 19:56:21 INFO  time: compiled ht3 in 4.44s[0m
[0m2021.07.14 19:57:39 INFO  compiling ht3 (6 scala sources)[0m
[0m2021.07.14 19:57:40 INFO  time: compiled ht3 in 1.14s[0m
Jul 14, 2021 7:57:46 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 24
[0m2021.07.14 19:57:47 INFO  compiling ht3 (6 scala sources)[0m
[0m2021.07.14 19:57:47 INFO  time: compiled ht3 in 0.79s[0m
Jul 14, 2021 7:57:53 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 44
[0m2021.07.14 19:57:57 INFO  compiling ht3 (6 scala sources)[0m
[0m2021.07.14 19:57:57 INFO  time: compiled ht3 in 0.94s[0m
Jul 14, 2021 7:57:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 61
Jul 14, 2021 7:58:12 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 71
[0m2021.07.14 19:58:16 INFO  compiling ht3 (6 scala sources)[0m
[0m2021.07.14 19:58:16 INFO  time: compiled ht3 in 0.73s[0m
Jul 14, 2021 7:58:17 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 88
Jul 14, 2021 7:58:21 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 98
[0m2021.07.14 19:58:23 INFO  compiling ht3 (6 scala sources)[0m
[0m2021.07.14 19:58:23 INFO  time: compiled ht3 in 0.65s[0m
Jul 14, 2021 7:59:17 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 134
Jul 14, 2021 7:59:35 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 142
[0m2021.07.14 20:00:07 INFO  compiling ht3 (6 scala sources)[0m
[0m2021.07.14 20:00:07 INFO  time: compiled ht3 in 0.71s[0m
Jul 14, 2021 8:00:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 255
Jul 14, 2021 8:02:57 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 261
[0m2021.07.14 20:03:29 INFO  compiling ht3 (6 scala sources)[0m
[0m2021.07.14 20:03:29 INFO  time: compiled ht3 in 0.55s[0m
[0m2021.07.14 20:03:43 INFO  compiling ht3 (6 scala sources)[0m
[0m2021.07.14 20:03:43 INFO  time: compiled ht3 in 0.53s[0m
Jul 14, 2021 8:04:12 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 364
Jul 14, 2021 8:06:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 394
[0m2021.07.14 20:06:20 INFO  compiling ht3 (6 scala sources)[0m
[0m2021.07.14 20:06:20 INFO  time: compiled ht3 in 0.6s[0m
[0m2021.07.14 20:08:03 INFO  compiling ht3 (6 scala sources)[0m
[0m2021.07.14 20:08:03 INFO  time: compiled ht3 in 0.5s[0m
[0m2021.07.14 20:08:12 INFO  compiling ht3 (6 scala sources)[0m
[0m2021.07.14 20:08:12 INFO  time: compiled ht3 in 0.62s[0m
[0m2021.07.14 20:09:04 INFO  compiling ht3 (6 scala sources)[0m
[0m2021.07.14 20:09:04 INFO  time: compiled ht3 in 0.52s[0m
Jul 14, 2021 8:09:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 569
[0m2021.07.14 20:09:37 INFO  compiling ht3 (6 scala sources)[0m
[0m2021.07.14 20:09:37 INFO  time: compiled ht3 in 0.51s[0m
Jul 14, 2021 8:10:22 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 605
[0m2021.07.14 20:11:07 INFO  compiling ht3 (7 scala sources)[0m
Jul 14, 2021 8:11:07 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 622
[0m2021.07.14 20:11:07 INFO  time: compiled ht3 in 0.54s[0m
[0m2021.07.14 20:11:07 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:11:07 INFO  time: compiled ht3 in 0.38s[0m
Jul 14, 2021 8:11:34 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 668
Jul 14, 2021 8:11:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 674
Jul 14, 2021 8:11:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 700
Jul 14, 2021 8:12:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 718
Jul 14, 2021 8:12:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 735
[0m2021.07.14 20:12:15 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:12:15 INFO  time: compiled ht3 in 0.45s[0m
[0m2021.07.14 20:12:15 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:12:15 INFO  time: compiled ht3 in 0.35s[0m
Jul 14, 2021 8:12:39 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 768
Jul 14, 2021 8:13:03 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 809
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataStreamer.scala in Ordering[java.time.LocalDateTime]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataStreamer.scala, 692, 692, 715)
Jul 14, 2021 8:13:41 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 845
[0m2021.07.14 20:13:58 ERROR code navigation does not work for the file '/home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala' because the SemanticDB file '/home/hwait/dev/ht3/.bloop/ht3/bloop-bsp-clients-classes/classes-Metals-s-4d8qOVRY-OWLOyXfd3hg==/META-INF/semanticdb/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala.semanticdb' doesn't exist. There can be many reasons for this error. [0m
[0m2021.07.14 20:13:58 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:13:58 INFO  time: compiled ht3 in 0.53s[0m
Jul 14, 2021 8:14:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 893
[0m2021.07.14 20:14:07 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:14:07 INFO  time: compiled ht3 in 0.42s[0m
Jul 14, 2021 8:14:11 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 909
[0m2021.07.14 20:14:13 ERROR code navigation does not work for the file '/home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala' because the SemanticDB file '/home/hwait/dev/ht3/.bloop/ht3/bloop-bsp-clients-classes/classes-Metals-s-4d8qOVRY-OWLOyXfd3hg==/META-INF/semanticdb/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala.semanticdb' doesn't exist. There can be many reasons for this error. [0m
Jul 14, 2021 8:14:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 920
Jul 14, 2021 8:14:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1052
[0m2021.07.14 20:14:50 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:14:50 INFO  time: compiled ht3 in 0.49s[0m
[0m2021.07.14 20:14:50 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:14:50 INFO  time: compiled ht3 in 0.35s[0m
Jul 14, 2021 8:14:53 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1064
[0m2021.07.14 20:15:04 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:15:04 INFO  time: compiled ht3 in 0.47s[0m
[0m2021.07.14 20:15:57 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:15:57 INFO  time: compiled ht3 in 0.47s[0m
Jul 14, 2021 8:16:16 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1206
[0m2021.07.14 20:16:20 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:16:20 INFO  time: compiled ht3 in 0.42s[0m
Jul 14, 2021 8:16:22 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1224
Jul 14, 2021 8:17:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1311
[0m2021.07.14 20:17:33 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:17:33 INFO  time: compiled ht3 in 0.48s[0m
Jul 14, 2021 8:18:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1390
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala in reflect.runtime.universe.TypeTag[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala, 753, 753, 763)
[0m2021.07.14 20:18:24 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:18:24 INFO  time: compiled ht3 in 0.39s[0m
Jul 14, 2021 8:18:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1437
[0m2021.07.14 20:18:32 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala:45:16: stale bloop error: not enough arguments for method apply: (spark: org.apache.spark.sql.SparkSession, DATA_PATH: String)ru.otus.bigdataml.ht3.DataProvider in object DataProvider.
Unspecified value parameter DATA_PATH.
        val dp=DataProvider(spark)
               ^^^^^^^^^^^^^^^^^^^[0m
[0m2021.07.14 20:18:32 INFO  /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala:45:16: stale bloop error: not enough arguments for method apply: (spark: org.apache.spark.sql.SparkSession, DATA_PATH: String)ru.otus.bigdataml.ht3.DataProvider in object DataProvider.
Unspecified value parameter DATA_PATH.
        val dp=DataProvider(spark)
               ^^^^^^^^^^^^^^^^^^^[0m
[0m2021.07.14 20:18:35 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:18:35 INFO  time: compiled ht3 in 0.48s[0m
[0m2021.07.14 20:18:35 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:18:35 INFO  time: compiled ht3 in 0.36s[0m
Jul 14, 2021 8:18:37 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1458
Jul 14, 2021 8:18:41 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1466
Jul 14, 2021 8:18:46 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1478
Jul 14, 2021 8:18:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1491
Jul 14, 2021 8:19:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1581
[0m2021.07.14 20:20:04 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:20:04 INFO  time: compiled ht3 in 0.14s[0m
[0m2021.07.14 20:20:04 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:20:04 INFO  time: compiled ht3 in 57ms[0m
[0m2021.07.14 20:20:06 ERROR code navigation does not work for the file '/home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala' because the SemanticDB file '/home/hwait/dev/ht3/.bloop/ht3/bloop-bsp-clients-classes/classes-Metals-s-4d8qOVRY-OWLOyXfd3hg==/META-INF/semanticdb/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala.semanticdb' doesn't exist. There can be many reasons for this error. [0m
[0m2021.07.14 20:20:06 ERROR code navigation does not work for the file '/home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala' because the SemanticDB file '/home/hwait/dev/ht3/.bloop/ht3/bloop-bsp-clients-classes/classes-Metals-s-4d8qOVRY-OWLOyXfd3hg==/META-INF/semanticdb/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala.semanticdb' doesn't exist. There can be many reasons for this error. [0m
[0m2021.07.14 20:20:06 ERROR code navigation does not work for the file '/home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala' because the SemanticDB file '/home/hwait/dev/ht3/.bloop/ht3/bloop-bsp-clients-classes/classes-Metals-s-4d8qOVRY-OWLOyXfd3hg==/META-INF/semanticdb/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala.semanticdb' doesn't exist. There can be many reasons for this error. [0m
[0m2021.07.14 20:20:19 ERROR code navigation does not work for the file '/home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala' because the SemanticDB file '/home/hwait/dev/ht3/.bloop/ht3/bloop-bsp-clients-classes/classes-Metals-s-4d8qOVRY-OWLOyXfd3hg==/META-INF/semanticdb/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala.semanticdb' doesn't exist. There can be many reasons for this error. [0m
[0m2021.07.14 20:20:19 ERROR code navigation does not work for the file '/home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala' because the SemanticDB file '/home/hwait/dev/ht3/.bloop/ht3/bloop-bsp-clients-classes/classes-Metals-s-4d8qOVRY-OWLOyXfd3hg==/META-INF/semanticdb/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala.semanticdb' doesn't exist. There can be many reasons for this error. [0m
[0m2021.07.14 20:20:19 ERROR code navigation does not work for the file '/home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala' because the SemanticDB file '/home/hwait/dev/ht3/.bloop/ht3/bloop-bsp-clients-classes/classes-Metals-s-4d8qOVRY-OWLOyXfd3hg==/META-INF/semanticdb/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala.semanticdb' doesn't exist. There can be many reasons for this error. [0m
Jul 14, 2021 8:20:21 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1669
[0m2021.07.14 20:20:26 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:20:26 INFO  time: compiled ht3 in 0.52s[0m
[0m2021.07.14 20:20:26 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:20:26 INFO  time: compiled ht3 in 0.35s[0m
[0m2021.07.14 20:20:58 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:20:58 INFO  time: compiled ht3 in 0.41s[0m
Jul 14, 2021 8:21:01 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1761
[0m2021.07.14 20:21:03 ERROR code navigation does not work for the file '/home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala' because the SemanticDB file '/home/hwait/dev/ht3/.bloop/ht3/bloop-bsp-clients-classes/classes-Metals-s-4d8qOVRY-OWLOyXfd3hg==/META-INF/semanticdb/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala.semanticdb' doesn't exist. There can be many reasons for this error. [0m
[0m2021.07.14 20:21:03 ERROR code navigation does not work for the file '/home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala' because the SemanticDB file '/home/hwait/dev/ht3/.bloop/ht3/bloop-bsp-clients-classes/classes-Metals-s-4d8qOVRY-OWLOyXfd3hg==/META-INF/semanticdb/src/main/scala/ru.otus.bigdataml.ht3/Configurator.scala.semanticdb' doesn't exist. There can be many reasons for this error. [0m
[0m2021.07.14 20:21:06 INFO  compiling ht3 (7 scala sources)[0m
Jul 14, 2021 8:21:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1769
[0m2021.07.14 20:21:06 INFO  time: compiled ht3 in 0.61s[0m
[0m2021.07.14 20:21:06 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:21:06 INFO  time: compiled ht3 in 0.34s[0m
Jul 14, 2021 8:21:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1779
[0m2021.07.14 20:21:16 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:21:16 INFO  time: compiled ht3 in 0.41s[0m
Jul 14, 2021 8:21:22 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1814
Jul 14, 2021 8:21:24 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1820
Jul 14, 2021 8:21:25 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1825
[0m2021.07.14 20:21:36 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:21:36 INFO  time: compiled ht3 in 0.45s[0m
[0m2021.07.14 20:21:54 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:21:54 INFO  time: compiled ht3 in 0.45s[0m
[0m2021.07.14 20:22:02 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:22:02 INFO  time: compiled ht3 in 0.44s[0m
[0m2021.07.14 20:22:30 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:22:30 INFO  time: compiled ht3 in 0.45s[0m
[0m2021.07.14 20:22:50 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:22:50 INFO  time: compiled ht3 in 0.41s[0m
Jul 14, 2021 8:22:58 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1938
Jul 14, 2021 8:23:13 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1961
Jul 14, 2021 8:23:18 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1972
[0m2021.07.14 20:24:29 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:24:29 INFO  time: compiled ht3 in 0.46s[0m
[0m2021.07.14 20:24:43 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:24:43 INFO  time: compiled ht3 in 0.45s[0m
Jul 14, 2021 8:24:48 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2104
[0m2021.07.14 20:24:54 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:24:54 INFO  time: compiled ht3 in 0.42s[0m
[0m2021.07.14 20:25:00 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:25:00 INFO  time: compiled ht3 in 0.38s[0m
[0m2021.07.14 20:25:23 INFO  compiling ht3 (7 scala sources)[0m
[0m2021.07.14 20:25:27 INFO  time: compiled ht3 in 3.98s[0m
[0m2021.07.14 20:25:39 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 20:25:39 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 20:25:39 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
[0m2021.07.14 20:25:39 INFO  Listening for transport dt_socket at address: 54811[0m
[0m2021.07.14 20:25:39 INFO  Trying to attach to remote debuggee VM localhost:54811 .[0m
[0m2021.07.14 20:25:39 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.14 20:25:40 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 20:25:40 ERROR 21/07/14 20:25:40 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.31.26.139 instead (on interface eth0)[0m
[0m2021.07.14 20:25:40 ERROR 21/07/14 20:25:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 20:25:41 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 20:25:41 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 20:25:41 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 20:25:41 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 20:25:41 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 20:25:41 ERROR 21/07/14 20:25:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 20:25:50 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.14 20:25:52 ERROR 21/07/14 20:25:52 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.14 20:25:58 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 20:25:58 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.14 20:25:58 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 20:25:58 INFO  |[1.0,0.0,0.0,0.0,...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:25:58 INFO  |[1.0,0.0,0.0,0.0,...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:25:58 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:25:58 INFO  |(179,[0,5,8,18,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:25:58 INFO  |(179,[0,7,8,18,23...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:25:58 INFO  |(179,[0,7,8,18,24...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:25:58 INFO  |(179,[1,3,9,18,24...|                (3,[1],[1.0])|             (5,[0],[1.0])|             (2,[1],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:25:58 INFO  |(179,[0,6,8,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:25:58 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:25:58 INFO  |(179,[0,5,8,11,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:25:58 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 20:25:58 INFO  only showing top 10 rows[0m
[0m2021.07.14 20:25:58 INFO  [0m
[0m2021.07.14 20:26:00 ERROR 21/07/14 20:26:00 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.14 20:26:00 ERROR 21/07/14 20:26:00 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.14 20:26:04 ERROR 21/07/14 20:26:04 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.14 20:26:04 ERROR 21/07/14 20:26:04 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.14 20:26:09 INFO  Coefficients: [-0.08806494785203707,-0.07870275654353387,-0.018722996831260823,0.08977793560291772,-0.035912293827620206,-0.03422329680021981,-0.01658537906270628,-0.15260635530276837,-0.31857351425213337,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.09558926147734019,-0.14672397544202415,-0.034133472133754394,0.0843894976768038,0.035260507569199954,1.0895639909669097,-0.24220601859414065,-0.2552217031283249,-0.19910117371197256,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007631505845331393,0.03885064894838053,0.003518582444546453,-0.023322420267587015,-0.3139218569871667,0.0016035330569434474,-0.024728574613039293,-0.058612741861423324,0.00605935002492887,-0.025074088907241478,-4.315703597259844E-4,-0.010853314086764822,-0.013356238499154805,0.00585330183345148,0.0,0.07467043884192417,0.03387457884725255,0.013792086562545663,0.030207550516802916,-0.032930414201143766,0.03175653444072198,-0.060504111095106526,-0.06886592268957022,-0.06810058891681425,-0.06809856410824891,0.0,-0.025983586506422997,0.0,0.036565687708795744,-0.06432228924932164,-0.06863044750735123,0.11377285148277516,-0.07359280279067022,-0.05318869945871539,-0.037642164588416106,-0.04928965886255228,-0.04022129650474418,0.0,0.0,0.0,-0.027380843599785185,0.0,0.0,-0.0375957117000115,0.05711001523214416,-0.029232112022713544,0.041070473912352176,0.0026661651908720845,-0.03790695906294235,0.03916520371416636,-0.010263504726562594,0.0,0.0,0.0,0.0,-0.010651510427363424,-0.0445179719909335,0.0,-0.04195431799631535,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.035525705998616555,0.0,-0.010871873837306954,0.0,0.0,-0.055682341301940226,0.020364702422894503,0.046538538892324316] Intercept: 0.18466293560576175[0m
[0m2021.07.14 20:26:09 INFO  numIterations: 0[0m
[0m2021.07.14 20:26:09 INFO  objectiveHistory: [0.0][0m
[0m2021.07.14 20:26:09 INFO  +--------------------+[0m
[0m2021.07.14 20:26:09 INFO  |           residuals|[0m
[0m2021.07.14 20:26:09 INFO  +--------------------+[0m
[0m2021.07.14 20:26:09 INFO  |  0.8069983354454132|[0m
[0m2021.07.14 20:26:09 INFO  |-0.12503966299437022|[0m
[0m2021.07.14 20:26:09 INFO  |-0.18301068516461408|[0m
[0m2021.07.14 20:26:09 INFO  |  0.8338565765261711|[0m
[0m2021.07.14 20:26:09 INFO  |-0.17615305753969532|[0m
[0m2021.07.14 20:26:09 INFO  | -0.1761531753990857|[0m
[0m2021.07.14 20:26:09 INFO  |  0.8238466563317683|[0m
[0m2021.07.14 20:26:09 INFO  |-0.17664305487949553|[0m
[0m2021.07.14 20:26:09 INFO  |-0.17274369508722617|[0m
[0m2021.07.14 20:26:09 INFO  |-0.17093858479897342|[0m
[0m2021.07.14 20:26:09 INFO  |-0.17733809999307348|[0m
[0m2021.07.14 20:26:09 INFO  | -0.1381674303133214|[0m
[0m2021.07.14 20:26:09 INFO  |-0.16954481503055185|[0m
[0m2021.07.14 20:26:09 INFO  |-0.15708236972998044|[0m
[0m2021.07.14 20:26:09 INFO  |-0.18519702719244846|[0m
[0m2021.07.14 20:26:09 INFO  | -0.1993042377436394|[0m
[0m2021.07.14 20:26:09 INFO  |-0.17474338802233322|[0m
[0m2021.07.14 20:26:09 INFO  |-0.17644605094695628|[0m
[0m2021.07.14 20:26:09 INFO  |-0.21226407906376585|[0m
[0m2021.07.14 20:26:09 INFO  |-0.15650982873995128|[0m
[0m2021.07.14 20:26:09 INFO  +--------------------+[0m
[0m2021.07.14 20:26:09 INFO  only showing top 20 rows[0m
[0m2021.07.14 20:26:09 INFO  [0m
[0m2021.07.14 20:26:09 INFO  RMSE: 0.3817729869930513[0m
[0m2021.07.14 20:26:09 INFO  r2: 0.013178296985179916[0m
[0m2021.07.14 20:26:09 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
Jul 14, 2021 8:26:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2181
Jul 14, 2021 8:27:18 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2260
Jul 14, 2021 8:27:26 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2279
Jul 14, 2021 8:27:27 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2285
Jul 14, 2021 8:28:24 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2317
Jul 14, 2021 8:28:50 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2403
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala in Array[String]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala, 1070, 1070, 1083)
Jul 14, 2021 8:29:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2422
Jul 14, 2021 8:29:12 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2431
Jul 14, 2021 8:29:19 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2442
[0m2021.07.14 20:29:26 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 20:29:26 INFO  time: compiled ht3 in 0.4s[0m
Jul 14, 2021 8:29:32 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2470
Jul 14, 2021 8:29:51 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2499
Jul 14, 2021 8:30:01 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2516
[0m2021.07.14 20:30:05 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 20:30:05 INFO  time: compiled ht3 in 0.24s[0m
[0m2021.07.14 20:30:13 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 20:30:13 INFO  time: compiled ht3 in 0.26s[0m
[0m2021.07.14 20:30:37 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 20:30:37 INFO  time: compiled ht3 in 0.18s[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/ProjectConfiguration.scala in String => <error>RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/ProjectConfiguration.scala, 441, 448, 471)
[0m2021.07.14 20:30:59 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 20:30:59 INFO  time: compiled ht3 in 0.17s[0m
[0m2021.07.14 20:31:40 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 20:31:41 INFO  time: compiled ht3 in 1.28s[0m
Jul 14, 2021 8:32:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2749
Jul 14, 2021 8:32:11 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2756
Jul 14, 2021 8:32:42 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2775
Jul 14, 2021 8:33:44 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2797
Jul 14, 2021 8:33:57 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2824
Jul 14, 2021 8:34:10 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2842
[0m2021.07.14 20:34:40 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 20:34:41 INFO  time: compiled ht3 in 1.03s[0m
[0m2021.07.14 20:34:41 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 20:34:41 INFO  time: compiled ht3 in 0.21s[0m
Jul 14, 2021 8:34:44 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2929
[0m2021.07.14 20:35:09 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 20:35:10 INFO  time: compiled ht3 in 1.47s[0m
[0m2021.07.14 20:35:10 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 20:35:10 INFO  time: compiled ht3 in 0.18s[0m
Jul 14, 2021 8:35:12 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2984
[0m2021.07.14 20:35:48 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.14 20:35:50 INFO  time: compiled ht3 in 1.62s[0m
Jul 14, 2021 8:36:58 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3147
Jul 14, 2021 8:37:02 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3153
Jul 14, 2021 8:37:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3161
[0m2021.07.14 20:37:28 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 20:37:29 INFO  time: compiled ht3 in 1.19s[0m
Jul 14, 2021 8:38:11 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3253
[0m2021.07.14 20:38:24 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 20:38:25 INFO  time: compiled ht3 in 1.37s[0m
Jul 14, 2021 8:38:26 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3300
Jul 14, 2021 8:38:28 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3305
Jul 14, 2021 8:38:58 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3363
[0m2021.07.14 20:39:15 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 20:39:16 INFO  time: compiled ht3 in 1.39s[0m
[0m2021.07.14 20:40:28 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 20:40:30 INFO  time: compiled ht3 in 1.25s[0m
Jul 14, 2021 8:42:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3454
Jul 14, 2021 8:42:10 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3474
Jul 14, 2021 8:42:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3488
[0m2021.07.14 20:42:25 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.14 20:42:26 INFO  time: compiled ht3 in 1.59s[0m
[0m2021.07.14 20:42:26 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.14 20:42:26 INFO  time: compiled ht3 in 0.66s[0m
[0m2021.07.14 20:43:06 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 20:43:07 INFO  time: compiled ht3 in 1.03s[0m
Jul 14, 2021 8:43:19 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3595
Jul 14, 2021 8:43:36 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3606
[0m2021.07.14 20:47:00 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 20:47:01 INFO  time: compiled ht3 in 1.21s[0m
[0m2021.07.14 20:47:05 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 50m 54.334s)[0m
[0m2021.07.14 20:47:05 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 20:47:05 INFO  Listening for transport dt_socket at address: 34977[0m
[0m2021.07.14 20:47:05 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 20:47:05 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
[0m2021.07.14 20:47:05 INFO  Trying to attach to remote debuggee VM localhost:34977 .[0m
[0m2021.07.14 20:47:05 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.14 20:47:07 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 20:47:07 ERROR 21/07/14 20:47:07 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.31.26.139 instead (on interface eth0)[0m
[0m2021.07.14 20:47:07 ERROR 21/07/14 20:47:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 20:47:08 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 20:47:08 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 20:47:08 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 20:47:08 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 20:47:08 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 20:47:08 ERROR 21/07/14 20:47:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 20:47:16 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.14 20:47:18 ERROR 21/07/14 20:47:18 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.14 20:47:24 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 20:47:24 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.14 20:47:24 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 20:47:24 INFO  |[1.0,0.0,0.0,0.0,...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:47:24 INFO  |[1.0,0.0,0.0,0.0,...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:47:24 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:47:24 INFO  |(179,[0,5,8,18,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:47:24 INFO  |(179,[0,7,8,18,23...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:47:24 INFO  |(179,[0,7,8,18,24...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:47:24 INFO  |(179,[1,3,9,18,24...|                (3,[1],[1.0])|             (5,[0],[1.0])|             (2,[1],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:47:24 INFO  |(179,[0,6,8,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:47:24 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:47:24 INFO  |(179,[0,5,8,11,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:47:24 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 20:47:24 INFO  only showing top 10 rows[0m
[0m2021.07.14 20:47:24 INFO  [0m
[0m2021.07.14 20:47:26 ERROR 21/07/14 20:47:26 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.14 20:47:26 ERROR 21/07/14 20:47:26 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.14 20:47:30 ERROR 21/07/14 20:47:30 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.14 20:47:30 ERROR 21/07/14 20:47:30 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.14 20:47:34 ERROR 21/07/14 20:47:34 INFO SparkMLModelTraining$: Hello World[0m
[0m2021.07.14 20:47:34 INFO  Coefficients: [-0.08806494785203745,-0.0787027565435339,-0.01872299683126128,0.08977793560291711,-0.035912293827620755,-0.034223296800220404,-0.01658537906270698,-0.15260635530276848,-0.3185735142521332,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.09558926147734012,-0.14672397544202395,-0.034133472133754304,0.08438949767680395,0.03526050756919984,1.0895639909669081,-0.24220601859414073,-0.255221703128325,-0.19910117371197306,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007631505845330891,0.038850648948379994,0.003518582444546314,-0.023322420267589003,-0.31392185698716696,0.0016035330569434982,-0.02472857461303762,-0.05861274186142332,0.006059350024928985,-0.025074088907245294,-4.3157035972653756E-4,-0.010853314086764853,-0.013356238499154887,0.0058533018334508595,0.0,0.07467043884192381,0.03387457884725228,0.01379208656254508,0.030207550516802583,-0.03293041420114378,0.03175653444072157,-0.06050411109510653,-0.06886592268957022,-0.06810058891681453,-0.06809856410824909,0.0,-0.025983586506423004,0.0,0.03656568770879546,-0.06432228924932178,-0.06863044750735128,0.11377285148277487,-0.07359280279067074,-0.05318869945871625,-0.037642164588416516,-0.04928965886255234,-0.04022129650474419,0.0,0.0,0.0,-0.027380843599785175,0.0,0.0,-0.03759571170001147,0.05711001523214386,-0.029232112022713596,0.041070473912351815,0.0026661651908720975,-0.03790695906294235,0.039165203714166354,-0.010263504726562601,0.0,0.0,0.0,0.0,-0.010651510427363403,-0.04451797199093349,0.0,-0.04195431799631536,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.035525705998616576,0.0,-0.010871873837306948,0.0,0.0,-0.055682341301940906,0.020364702422894145,0.046538538892323865] Intercept: 0.1846629356057651[0m
[0m2021.07.14 20:47:34 INFO  numIterations: 0[0m
[0m2021.07.14 20:47:34 INFO  objectiveHistory: [0.0][0m
[0m2021.07.14 20:47:34 INFO  RMSE: 0.3817729869930513[0m
[0m2021.07.14 20:47:34 INFO  r2: 0.013178296985179916[0m
[0m2021.07.14 20:47:35 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
Jul 14, 2021 8:51:59 PM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTrace
[0m2021.07.14 20:53:06 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 20:53:07 INFO  time: compiled ht3 in 1.29s[0m
[0m2021.07.14 20:53:09 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 56m 57.723s)[0m
[0m2021.07.14 20:53:09 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 20:53:09 INFO  Listening for transport dt_socket at address: 53019[0m
[0m2021.07.14 20:53:09 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 20:53:09 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
[0m2021.07.14 20:53:09 INFO  Trying to attach to remote debuggee VM localhost:53019 .[0m
[0m2021.07.14 20:53:09 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.14 20:53:10 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 20:53:10 ERROR 21/07/14 20:53:10 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.31.26.139 instead (on interface eth0)[0m
[0m2021.07.14 20:53:10 ERROR 21/07/14 20:53:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 20:53:11 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 20:53:11 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 20:53:11 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 20:53:11 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 20:53:11 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 20:53:11 ERROR 21/07/14 20:53:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 20:53:20 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.14 20:53:22 ERROR 21/07/14 20:53:22 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.14 20:53:28 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 20:53:28 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.14 20:53:28 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 20:53:28 INFO  |[1.0,0.0,0.0,0.0,...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:53:28 INFO  |[1.0,0.0,0.0,0.0,...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:53:28 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:53:28 INFO  |(179,[0,5,8,18,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:53:28 INFO  |(179,[0,7,8,18,23...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:53:28 INFO  |(179,[0,7,8,18,24...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:53:28 INFO  |(179,[1,3,9,18,24...|                (3,[1],[1.0])|             (5,[0],[1.0])|             (2,[1],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:53:28 INFO  |(179,[0,6,8,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:53:28 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:53:28 INFO  |(179,[0,5,8,11,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 20:53:28 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 20:53:28 INFO  only showing top 10 rows[0m
[0m2021.07.14 20:53:28 INFO  [0m
[0m2021.07.14 20:53:29 ERROR 21/07/14 20:53:29 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.14 20:53:29 ERROR 21/07/14 20:53:29 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.14 20:53:34 ERROR 21/07/14 20:53:34 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.14 20:53:34 ERROR 21/07/14 20:53:34 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.14 20:53:38 ERROR 21/07/14 20:53:38 ERROR SparkMLModelTraining$: Hello World[0m
[0m2021.07.14 20:53:38 INFO  Coefficients: [-0.08806494785203745,-0.0787027565435339,-0.01872299683126128,0.08977793560291711,-0.035912293827620755,-0.034223296800220404,-0.01658537906270698,-0.15260635530276848,-0.3185735142521332,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.09558926147734012,-0.14672397544202395,-0.034133472133754304,0.08438949767680395,0.03526050756919984,1.0895639909669081,-0.24220601859414073,-0.255221703128325,-0.19910117371197306,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007631505845330891,0.038850648948379994,0.003518582444546314,-0.023322420267589003,-0.31392185698716696,0.0016035330569434982,-0.02472857461303762,-0.05861274186142332,0.006059350024928985,-0.025074088907245294,-4.3157035972653756E-4,-0.010853314086764853,-0.013356238499154887,0.0058533018334508595,0.0,0.07467043884192381,0.03387457884725228,0.01379208656254508,0.030207550516802583,-0.03293041420114378,0.03175653444072157,-0.06050411109510653,-0.06886592268957022,-0.06810058891681453,-0.06809856410824909,0.0,-0.025983586506423004,0.0,0.03656568770879546,-0.06432228924932178,-0.06863044750735128,0.11377285148277487,-0.07359280279067074,-0.05318869945871625,-0.037642164588416516,-0.04928965886255234,-0.04022129650474419,0.0,0.0,0.0,-0.027380843599785175,0.0,0.0,-0.03759571170001147,0.05711001523214386,-0.029232112022713596,0.041070473912351815,0.0026661651908720975,-0.03790695906294235,0.039165203714166354,-0.010263504726562601,0.0,0.0,0.0,0.0,-0.010651510427363403,-0.04451797199093349,0.0,-0.04195431799631536,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.035525705998616576,0.0,-0.010871873837306948,0.0,0.0,-0.055682341301940906,0.020364702422894145,0.046538538892323865] Intercept: 0.1846629356057651[0m
[0m2021.07.14 20:53:38 INFO  numIterations: 0[0m
[0m2021.07.14 20:53:38 INFO  objectiveHistory: [0.0][0m
[0m2021.07.14 20:53:38 INFO  RMSE: 0.3817729869930513[0m
[0m2021.07.14 20:53:38 INFO  r2: 0.013178296985179916[0m
[0m2021.07.14 20:53:39 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
[0m2021.07.14 21:07:04 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 21:07:05 INFO  time: compiled ht3 in 1.46s[0m
Jul 14, 2021 9:07:51 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3904
[0m2021.07.14 21:08:43 INFO  compiling ht3 (1 scala source)[0m
Jul 14, 2021 9:08:44 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3945
[0m2021.07.14 21:08:44 INFO  time: compiled ht3 in 1.26s[0m
[0m2021.07.14 21:12:12 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 21:12:12 INFO  time: compiled ht3 in 0.21s[0m
[0m2021.07.14 21:12:18 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 21:12:19 INFO  time: compiled ht3 in 1.17s[0m
[0m2021.07.14 21:12:38 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 21:12:39 INFO  time: compiled ht3 in 1.17s[0m
[0m2021.07.14 21:13:00 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 21:13:01 INFO  time: compiled ht3 in 1.26s[0m
[0m2021.07.14 21:13:25 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 21:13:26 INFO  time: compiled ht3 in 1.18s[0m
[0m2021.07.14 21:13:29 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 21:13:31 INFO  time: compiled ht3 in 1.22s[0m
[0m2021.07.14 21:13:35 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 21:13:37 INFO  time: compiled ht3 in 2.25s[0m
[0m2021.07.14 21:13:42 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 21:13:44 INFO  time: compiled ht3 in 1.12s[0m
[0m2021.07.14 21:13:54 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 1h 17m 42.896s)[0m
[0m2021.07.14 21:13:54 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 21:13:54 INFO  Listening for transport dt_socket at address: 33453[0m
[0m2021.07.14 21:13:54 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 21:13:54 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
[0m2021.07.14 21:13:54 INFO  Trying to attach to remote debuggee VM localhost:33453 .[0m
[0m2021.07.14 21:13:54 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.14 21:13:55 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 21:13:55 ERROR 21/07/14 21:13:55 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.31.26.139 instead (on interface eth0)[0m
[0m2021.07.14 21:13:55 ERROR 21/07/14 21:13:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 21:13:56 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 21:13:56 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 21:13:56 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 21:13:56 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 21:13:56 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 21:13:56 ERROR 21/07/14 21:13:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 21:14:05 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.14 21:14:07 ERROR 21/07/14 21:14:07 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.14 21:14:13 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 21:14:13 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.14 21:14:13 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 21:14:13 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 21:14:13 INFO  |(179,[0,6,9,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 21:14:13 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 21:14:13 INFO  |(179,[0,5,8,18,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 21:14:13 INFO  |(179,[0,7,8,18,23...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 21:14:13 INFO  |(179,[0,7,8,18,24...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 21:14:13 INFO  |(179,[1,3,9,18,24...|                (3,[1],[1.0])|             (5,[0],[1.0])|             (2,[1],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 21:14:13 INFO  |(179,[0,6,8,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 21:14:13 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 21:14:13 INFO  |(179,[0,5,8,11,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 21:14:13 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 21:14:13 INFO  only showing top 10 rows[0m
[0m2021.07.14 21:14:13 INFO  [0m
[0m2021.07.14 21:14:18 ERROR 21/07/14 21:14:18 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.14 21:14:18 ERROR 21/07/14 21:14:18 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.14 21:14:18 ERROR 21/07/14 21:14:18 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.14 21:14:18 ERROR 21/07/14 21:14:18 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.14 21:14:22 INFO  Coefficients: [-2.5967280171047607E-4,-4.494321362881141E-4,-5.4258303655814775E-5,1.496828991965776E-4,-7.046664354626902E-5,-6.731944201723438E-5,-2.9840412468668286E-5,-0.001341563242940739,-0.006042132729059748,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-3.6025943894617377E-4,-3.734838709209618E-4,-9.110805766567144E-5,1.983981379967166E-4,8.351655751683094E-5,0.009670620656878596,-9.86600939340013E-4,-0.0011069801781385586,-7.091407779834136E-4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5.247108191509612E-4,0.0032973153766204634,5.132567547260887E-4,-0.0017698841368443195,-5.609433170648428E-4,2.0147474212734728E-4,-0.0017104623518882183,-3.101291266562899E-4,5.258280110111335E-5,-0.0017271415061611169,-8.169727913085301E-5,-3.054978207210119E-4,-0.0015006137784203124,7.099771608425882E-5,0.0,1.8363596694750652E-4,9.237518902617039E-5,3.8386030145716254E-5,8.183078904792464E-5,-7.792007242442817E-4,8.603083874078916E-5,-0.0010452079255033947,-0.0010233833120702417,-1.4241909213358675E-4,-1.4241485756656642E-4,0.0,-6.774466534435694E-4,0.0,9.867083263805523E-5,-1.3451743732441652E-4,-1.3291200703640147E-4,2.751741490355995E-4,-1.2410162388225954E-4,-9.666416034505446E-5,-6.490411145985201E-5,-1.451549360629056E-4,-8.355551321407816E-4,0.0,0.0,0.0,-0.0031562963405762237,0.0,0.0,-1.238144421407263E-4,1.4368590970099292E-4,-7.619483523034826E-4,1.2565699447239119E-4,2.4695204335144433E-4,-6.496976945629103E-4,0.010541546737744961,-0.0021519900762538447,0.0,0.0,0.0,0.0,-0.0026972377073756324,-0.0024048975714460975,0.0,-0.0033871176874389294,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0010975938419123558,0.0,-2.253085247785864E-4,0.0,0.0,-1.1412316272077178E-4,4.942675323502692E-5,1.0325801339019078E-4] Intercept: 0.2105413966651437[0m
[0m2021.07.14 21:14:22 INFO  numIterations: 0[0m
[0m2021.07.14 21:14:22 INFO  objectiveHistory: [0.0][0m
[0m2021.07.14 21:14:22 INFO  RMSE: 0.3817729869930512[0m
[0m2021.07.14 21:14:22 INFO  r2: 0.013178296985180693[0m
[0m2021.07.14 21:14:23 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
[0m2021.07.14 21:31:34 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 21:31:36 INFO  time: compiled ht3 in 1.27s[0m
Jul 14, 2021 9:31:40 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4262
[0m2021.07.14 21:32:13 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 21:32:13 INFO  time: compiled ht3 in 0.34s[0m
[0m2021.07.14 21:32:20 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 21:32:21 INFO  time: compiled ht3 in 1.2s[0m
[0m2021.07.14 21:32:34 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 21:32:35 INFO  time: compiled ht3 in 1.2s[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataStreamer.scala in Ordering[java.time.LocalDateTime]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataStreamer.scala, 594, 594, 617)
Jul 14, 2021 9:43:24 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4475
Jul 14, 2021 9:43:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4506
Jul 14, 2021 9:44:01 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4515
[0m2021.07.14 21:44:35 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 21:44:35 INFO  time: compiled ht3 in 0.3s[0m
[0m2021.07.14 21:44:35 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 21:44:35 INFO  time: compiled ht3 in 0.17s[0m
[0m2021.07.14 21:44:56 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 21:44:56 INFO  time: compiled ht3 in 0.23s[0m
Jul 14, 2021 9:45:16 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4691
Jul 14, 2021 9:45:18 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4698
Jul 14, 2021 9:45:22 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4704
Jul 14, 2021 9:45:25 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4713
[0m2021.07.14 21:45:29 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 21:45:29 INFO  time: compiled ht3 in 0.24s[0m
Jul 14, 2021 9:45:58 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4756
Jul 14, 2021 9:46:01 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4763
Jul 14, 2021 9:46:11 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4769
Jul 14, 2021 9:46:18 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4776
Jul 14, 2021 9:46:25 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4794
Jul 14, 2021 9:46:32 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4815
Jul 14, 2021 9:46:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4826
Jul 14, 2021 9:46:42 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4835
Jul 14, 2021 9:46:49 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4849
Jul 14, 2021 9:47:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4884
Jul 14, 2021 9:47:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4894
Jul 14, 2021 9:48:28 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4941
[0m2021.07.14 21:48:30 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.14 21:48:30 INFO  time: compiled ht3 in 0.32s[0m
[0m2021.07.14 21:48:30 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.14 21:48:30 INFO  time: compiled ht3 in 0.25s[0m
Jul 14, 2021 9:48:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4948
Jul 14, 2021 9:48:32 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4954
Jul 14, 2021 9:48:35 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4963
[0m2021.07.14 21:48:42 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.14 21:48:42 INFO  time: compiled ht3 in 0.25s[0m
Jul 14, 2021 9:48:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4991
Jul 14, 2021 9:48:49 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5009
[0m2021.07.14 21:48:54 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.14 21:48:54 INFO  time: compiled ht3 in 0.28s[0m
Jul 14, 2021 9:48:55 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5022
Jul 14, 2021 9:48:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5037
Jul 14, 2021 9:49:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5057
[0m2021.07.14 21:49:21 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.14 21:49:21 INFO  time: compiled ht3 in 0.26s[0m
Jul 14, 2021 9:49:22 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5088
Jul 14, 2021 9:49:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5100
Jul 14, 2021 9:49:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5115
[0m2021.07.14 21:50:19 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.14 21:50:19 INFO  time: compiled ht3 in 0.28s[0m
Jul 14, 2021 9:50:21 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5172
Jul 14, 2021 9:50:40 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5193
[0m2021.07.14 21:50:45 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.14 21:50:45 INFO  time: compiled ht3 in 0.29s[0m
Jul 14, 2021 9:50:49 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5221
[0m2021.07.14 21:51:00 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.14 21:51:00 INFO  time: compiled ht3 in 0.27s[0m
Jul 14, 2021 9:51:01 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5244
Jul 14, 2021 9:51:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5258
[0m2021.07.14 21:51:28 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.14 21:51:28 INFO  time: compiled ht3 in 0.26s[0m
Jul 14, 2021 9:51:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5295
Jul 14, 2021 9:51:34 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5306
Jul 14, 2021 9:51:36 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5312
Jul 14, 2021 9:51:40 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5321
Jul 14, 2021 9:52:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5340
Jul 14, 2021 9:52:07 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5347
[0m2021.07.14 21:52:49 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.14 21:52:50 INFO  time: compiled ht3 in 1.63s[0m
[0m2021.07.14 21:52:57 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 21:52:57 INFO  time: compiled ht3 in 0.24s[0m
[0m2021.07.14 21:53:59 INFO  time: evaluated worksheet 'check.worksheet.sc' in 9.91s[0m
[0m2021.07.14 21:54:04 INFO  time: evaluated worksheet 'check.worksheet.sc' in 0.83s[0m
Jul 14, 2021 9:54:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5456
[0m2021.07.14 21:54:19 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 21:54:20 INFO  time: compiled ht3 in 1.22s[0m
Jul 14, 2021 9:55:41 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5566
[0m2021.07.14 21:56:09 INFO  compiling ht3 (1 scala source)[0m
Jul 14, 2021 9:56:10 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5578
[0m2021.07.14 21:56:11 INFO  time: compiled ht3 in 1.23s[0m
Jul 14, 2021 9:56:46 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5585
Jul 14, 2021 9:56:51 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5593
Jul 14, 2021 9:57:13 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5617
[0m2021.07.14 21:59:58 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 21:59:59 INFO  time: compiled ht3 in 1.27s[0m
[0m2021.07.14 22:00:02 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 2h 3m 51.475s)[0m
[0m2021.07.14 22:00:02 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 22:00:02 INFO  Listening for transport dt_socket at address: 36861[0m
[0m2021.07.14 22:00:02 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 22:00:02 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
[0m2021.07.14 22:00:02 INFO  Trying to attach to remote debuggee VM localhost:36861 .[0m
[0m2021.07.14 22:00:02 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.14 22:00:04 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 22:00:04 ERROR 21/07/14 22:00:04 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.31.26.139 instead (on interface eth0)[0m
[0m2021.07.14 22:00:04 ERROR 21/07/14 22:00:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 22:00:05 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 22:00:05 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 22:00:05 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 22:00:05 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 22:00:05 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 22:00:05 ERROR 21/07/14 22:00:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 22:00:14 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.14 22:00:16 INFO  Data: loaded [360130.0:181][0m
[0m2021.07.14 22:00:16 ERROR 21/07/14 22:00:16 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.14 22:00:22 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:00:22 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.14 22:00:22 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:00:22 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:00:22 INFO  |(179,[0,6,9,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:00:22 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:00:22 INFO  |(179,[0,5,8,18,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:00:22 INFO  |(179,[0,7,8,18,23...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:00:22 INFO  |(179,[0,7,8,18,24...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:00:22 INFO  |(179,[1,3,9,18,24...|                (3,[1],[1.0])|             (5,[0],[1.0])|             (2,[1],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:00:22 INFO  |(179,[0,6,8,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:00:22 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:00:22 INFO  |(179,[0,5,8,11,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:00:22 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:00:22 INFO  only showing top 10 rows[0m
[0m2021.07.14 22:00:22 INFO  [0m
[0m2021.07.14 22:00:28 ERROR 21/07/14 22:00:28 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.14 22:00:28 ERROR 21/07/14 22:00:28 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.14 22:00:28 ERROR 21/07/14 22:00:28 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.14 22:00:28 ERROR 21/07/14 22:00:28 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.14 22:00:32 INFO  Coefficients: [-2.5967280171047607E-4,-4.494321362881141E-4,-5.4258303655814775E-5,1.496828991965776E-4,-7.046664354626902E-5,-6.731944201723438E-5,-2.9840412468668286E-5,-0.001341563242940739,-0.006042132729059748,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-3.6025943894617377E-4,-3.734838709209618E-4,-9.110805766567144E-5,1.983981379967166E-4,8.351655751683094E-5,0.009670620656878596,-9.86600939340013E-4,-0.0011069801781385586,-7.091407779834136E-4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5.247108191509612E-4,0.0032973153766204634,5.132567547260887E-4,-0.0017698841368443195,-5.609433170648428E-4,2.0147474212734728E-4,-0.0017104623518882183,-3.101291266562899E-4,5.258280110111335E-5,-0.0017271415061611169,-8.169727913085301E-5,-3.054978207210119E-4,-0.0015006137784203124,7.099771608425882E-5,0.0,1.8363596694750652E-4,9.237518902617039E-5,3.8386030145716254E-5,8.183078904792464E-5,-7.792007242442817E-4,8.603083874078916E-5,-0.0010452079255033947,-0.0010233833120702417,-1.4241909213358675E-4,-1.4241485756656642E-4,0.0,-6.774466534435694E-4,0.0,9.867083263805523E-5,-1.3451743732441652E-4,-1.3291200703640147E-4,2.751741490355995E-4,-1.2410162388225954E-4,-9.666416034505446E-5,-6.490411145985201E-5,-1.451549360629056E-4,-8.355551321407816E-4,0.0,0.0,0.0,-0.0031562963405762237,0.0,0.0,-1.238144421407263E-4,1.4368590970099292E-4,-7.619483523034826E-4,1.2565699447239119E-4,2.4695204335144433E-4,-6.496976945629103E-4,0.010541546737744961,-0.0021519900762538447,0.0,0.0,0.0,0.0,-0.0026972377073756324,-0.0024048975714460975,0.0,-0.0033871176874389294,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0010975938419123558,0.0,-2.253085247785864E-4,0.0,0.0,-1.1412316272077178E-4,4.942675323502692E-5,1.0325801339019078E-4] Intercept: 0.2105413966651437[0m
[0m2021.07.14 22:00:32 INFO  numIterations: 0[0m
[0m2021.07.14 22:00:32 INFO  objectiveHistory: [0.0][0m
[0m2021.07.14 22:00:32 INFO  RMSE: 0.3817729869930512[0m
[0m2021.07.14 22:00:32 INFO  r2: 0.013178296985180693[0m
[0m2021.07.14 22:00:32 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
Jul 14, 2021 10:00:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5817
[0m2021.07.14 22:01:01 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 22:01:01 INFO  time: compiled ht3 in 0.36s[0m
[0m2021.07.14 22:01:09 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 22:01:11 INFO  time: compiled ht3 in 1.33s[0m
[0m2021.07.14 22:07:11 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 2h 11m 0.014s)[0m
[0m2021.07.14 22:07:11 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 22:07:11 INFO  Listening for transport dt_socket at address: 52161[0m
[0m2021.07.14 22:07:11 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 22:07:11 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka][0m
[0m2021.07.14 22:07:11 INFO  Trying to attach to remote debuggee VM localhost:52161 .[0m
[0m2021.07.14 22:07:11 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.14 22:07:13 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 22:07:13 ERROR 21/07/14 22:07:13 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.31.26.139 instead (on interface eth0)[0m
[0m2021.07.14 22:07:13 ERROR 21/07/14 22:07:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 22:07:14 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 22:07:14 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 22:07:14 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 22:07:14 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 22:07:14 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 22:07:14 ERROR 21/07/14 22:07:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 22:07:15 ERROR 21/07/14 22:07:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.[0m
[0m2021.07.14 22:07:15 ERROR 21/07/14 22:07:16 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.[0m
[0m2021.07.14 22:07:17 ERROR 21/07/14 22:07:17 WARN KafkaUtils: overriding enable.auto.commit to false for executor[0m
[0m2021.07.14 22:07:17 ERROR 21/07/14 22:07:17 WARN KafkaUtils: overriding auto.offset.reset to none for executor[0m
[0m2021.07.14 22:07:17 ERROR 21/07/14 22:07:17 WARN KafkaUtils: overriding executor group.id to spark-executor-spark_ml_testing[0m
[0m2021.07.14 22:07:17 ERROR 21/07/14 22:07:17 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135[0m
[0m2021.07.14 22:07:35 INFO  Data: loaded [34.0:9][0m
[0m2021.07.14 22:07:45 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka][0m
[0m2021.07.14 22:07:45 INFO  Listening for transport dt_socket at address: 47423[0m
Jul 14, 2021 10:08:13 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5877
Jul 14, 2021 10:09:51 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5921
[0m2021.07.14 22:14:18 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 22:14:20 INFO  time: compiled ht3 in 2.43s[0m
Jul 14, 2021 10:15:07 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6120
Jul 14, 2021 10:15:21 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6156
Jul 14, 2021 10:15:27 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6168
[0m2021.07.14 22:15:49 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 22:15:49 INFO  time: compiled ht3 in 0.26s[0m
Jul 14, 2021 10:16:32 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6240
Jul 14, 2021 10:16:34 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6246
Jul 14, 2021 10:16:34 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6253
Jul 14, 2021 10:16:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6266
Jul 14, 2021 10:16:48 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6295
[0m2021.07.14 22:17:37 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 22:17:37 INFO  time: compiled ht3 in 0.23s[0m
[0m2021.07.14 22:17:37 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 22:17:37 INFO  time: compiled ht3 in 0.18s[0m
Jul 14, 2021 10:17:40 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6392
[0m2021.07.14 22:17:47 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 22:17:48 INFO  time: compiled ht3 in 1.37s[0m
Jul 14, 2021 10:18:17 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6427
[0m2021.07.14 22:18:27 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 2h 22m 15.839s)[0m
[0m2021.07.14 22:18:27 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 22:18:27 INFO  Listening for transport dt_socket at address: 59319[0m
[0m2021.07.14 22:18:27 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 22:18:27 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka][0m
[0m2021.07.14 22:18:27 INFO  Trying to attach to remote debuggee VM localhost:59319 .[0m
[0m2021.07.14 22:18:27 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.14 22:18:28 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 22:18:28 ERROR 21/07/14 22:18:28 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.31.26.139 instead (on interface eth0)[0m
[0m2021.07.14 22:18:28 ERROR 21/07/14 22:18:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 22:18:29 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 22:18:29 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 22:18:29 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 22:18:29 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 22:18:29 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 22:18:29 ERROR 21/07/14 22:18:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 22:18:31 ERROR 21/07/14 22:18:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.[0m
[0m2021.07.14 22:18:31 ERROR 21/07/14 22:18:32 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.[0m
[0m2021.07.14 22:18:32 ERROR 21/07/14 22:18:32 WARN KafkaUtils: overriding enable.auto.commit to false for executor[0m
[0m2021.07.14 22:18:32 ERROR 21/07/14 22:18:32 WARN KafkaUtils: overriding auto.offset.reset to none for executor[0m
[0m2021.07.14 22:18:32 ERROR 21/07/14 22:18:32 WARN KafkaUtils: overriding executor group.id to spark-executor-spark_ml_testing[0m
[0m2021.07.14 22:18:32 ERROR 21/07/14 22:18:32 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135[0m
[0m2021.07.14 22:19:05 ERROR 21/07/14 22:19:05 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.14 22:19:05 INFO  Data: loaded [87.0:157][0m
[0m2021.07.14 22:19:30 INFO  Data: loaded [14.0:152][0m
[0m2021.07.14 22:20:00 INFO  Data: loaded [16.0:152][0m
[0m2021.07.14 22:20:06 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka][0m
[0m2021.07.14 22:20:06 INFO  Listening for transport dt_socket at address: 35577[0m
Jul 14, 2021 10:22:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6461
Jul 14, 2021 10:22:26 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6492
Jul 14, 2021 10:22:50 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6526
Jul 14, 2021 10:22:54 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6536
Exception in thread "pool-5-thread-1" java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[0m2021.07.14 22:23:47 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 22:23:47 INFO  time: compiled ht3 in 0.41s[0m
[0m2021.07.14 22:23:47 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 22:23:47 INFO  time: compiled ht3 in 0.29s[0m
Jul 14, 2021 10:23:50 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6634
Jul 14, 2021 10:23:57 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6647
[0m2021.07.14 22:24:04 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.14 22:24:05 INFO  time: compiled ht3 in 1.68s[0m
Jul 14, 2021 10:25:25 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6685
Jul 14, 2021 10:25:28 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6693
[0m2021.07.14 22:25:32 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 2h 29m 20.783s)[0m
[0m2021.07.14 22:25:32 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 22:25:32 INFO  Listening for transport dt_socket at address: 38229[0m
[0m2021.07.14 22:25:32 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 22:25:32 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka][0m
[0m2021.07.14 22:25:32 INFO  Trying to attach to remote debuggee VM localhost:38229 .[0m
[0m2021.07.14 22:25:32 INFO  Attaching to debuggee VM succeeded.[0m
Jul 14, 2021 10:25:33 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6701
[0m2021.07.14 22:25:33 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 22:25:33 ERROR 21/07/14 22:25:33 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.31.26.139 instead (on interface eth0)[0m
[0m2021.07.14 22:25:33 ERROR 21/07/14 22:25:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 22:25:34 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 22:25:34 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 22:25:34 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 22:25:34 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 22:25:34 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 22:25:34 ERROR 21/07/14 22:25:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 22:25:36 ERROR 21/07/14 22:25:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.[0m
[0m2021.07.14 22:25:36 ERROR 21/07/14 22:25:36 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.[0m
[0m2021.07.14 22:25:37 ERROR 21/07/14 22:25:37 WARN KafkaUtils: overriding enable.auto.commit to false for executor[0m
[0m2021.07.14 22:25:37 ERROR 21/07/14 22:25:37 WARN KafkaUtils: overriding auto.offset.reset to none for executor[0m
[0m2021.07.14 22:25:37 ERROR 21/07/14 22:25:37 WARN KafkaUtils: overriding executor group.id to spark-executor-spark_ml_testing[0m
[0m2021.07.14 22:25:37 ERROR 21/07/14 22:25:37 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135[0m
[0m2021.07.14 22:26:05 ERROR 21/07/14 22:26:05 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.14 22:26:05 INFO  Data: loaded [23.0:152][0m
[0m2021.07.14 22:26:06 ERROR 21/07/14 22:26:06 ERROR Instrumentation: org.apache.spark.sql.AnalysisException: cannot resolve '`log_ownerId`' given input columns: [audit_pos, audit_resourceType_num, audit_resourceType_num_enc, auditedHour, auditweights_ageMs, auditweights_closed, auditweights_ctr_gender, auditweights_ctr_high, auditweights_ctr_negative, auditweights_dailyRecency, auditweights_feedOwner_RECOMMENDED_GROUP, auditweights_feedStats, auditweights_friendCommentFeeds, auditweights_friendCommenters, auditweights_friendLikes, auditweights_friendLikes_actors, auditweights_hasDetectedText, auditweights_hasText, auditweights_isPymk, auditweights_isRandom, auditweights_likersFeedStats_hyper, auditweights_likersSvd_prelaunch_hyper, auditweights_likersSvd_spark_hyper, auditweights_matrix, auditweights_notOriginalPhoto, auditweights_numDislikes, auditweights_numLikes, auditweights_numShows, auditweights_onlineVideo, auditweights_partAge, auditweights_partCtr, auditweights_partSvd, auditweights_processedVideo, auditweights_relationMasks, auditweights_source_LIVE_TOP, auditweights_source_MOVIE_TOP, auditweights_source_PROMO, auditweights_svd_prelaunch, auditweights_svd_spark, auditweights_userAge, auditweights_userOwner_CREATE_COMMENT, auditweights_userOwner_CREATE_IMAGE, auditweights_userOwner_CREATE_LIKE, auditweights_userOwner_IMAGE, auditweights_userOwner_MOVIE_COMMENT_CREATE, auditweights_userOwner_PHOTO_COMMENT_CREATE, auditweights_userOwner_PHOTO_MARK_CREATE, auditweights_userOwner_PHOTO_VIEW, auditweights_userOwner_TEXT, auditweights_userOwner_UNKNOWN, auditweights_userOwner_USER_DELETE_MESSAGE, auditweights_userOwner_USER_FEED_REMOVE, auditweights_userOwner_USER_FORUM_MESSAGE_CREATE, auditweights_userOwner_USER_INTERNAL_LIKE, auditweights_userOwner_USER_INTERNAL_UNLIKE, auditweights_userOwner_USER_PRESENT_SEND, auditweights_userOwner_USER_PROFILE_VIEW, auditweights_userOwner_USER_SEND_MESSAGE, auditweights_userOwner_USER_STATUS_COMMENT_CREATE, auditweights_userOwner_VIDEO, auditweights_userOwner_VOTE_POLL, auditweights_x_ActorsRelations, createdHour, instanceId_objectType_num, instanceId_objectType_num_enc, membership_status_num, membership_status_num_enc, metadata_numCompanions, metadata_numPhotos, metadata_numPolls, metadata_numSymbols, metadata_numTokens, metadata_numVideos, metadata_ownerType_num, metadata_ownerType_num_enc, metadata_totalVideoLength, ownerUserCounters_COMMENT_INTERNAL_LIKE, ownerUserCounters_CREATE_COMMENT, ownerUserCounters_CREATE_IMAGE, ownerUserCounters_CREATE_LIKE, ownerUserCounters_CREATE_MOVIE, ownerUserCounters_CREATE_TOPIC, ownerUserCounters_IMAGE, ownerUserCounters_MOVIE_COMMENT_CREATE, ownerUserCounters_PHOTO_COMMENT_CREATE, ownerUserCounters_PHOTO_MARK_CREATE, ownerUserCounters_PHOTO_PIN_BATCH_CREATE, ownerUserCounters_PHOTO_PIN_UPDATE, ownerUserCounters_PHOTO_VIEW, ownerUserCounters_TEXT, ownerUserCounters_UNKNOWN, ownerUserCounters_USER_DELETE_MESSAGE, ownerUserCounters_USER_FEED_REMOVE, ownerUserCounters_USER_FORUM_MESSAGE_CREATE, ownerUserCounters_USER_INTERNAL_LIKE, ownerUserCounters_USER_INTERNAL_UNLIKE, ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE, ownerUserCounters_USER_PRESENT_SEND, ownerUserCounters_USER_PROFILE_VIEW, ownerUserCounters_USER_SEND_MESSAGE, ownerUserCounters_USER_STATUS_COMMENT_CREATE, ownerUserCounters_VIDEO, ownerUserCounters_VOTE_POLL, owner_ID_Location, owner_ID_country, owner_change_datime, owner_gender, owner_is_abused, owner_is_activated, owner_is_active, owner_is_deleted, owner_is_semiactivated, owner_region, owner_status, target, timeDelta, userOwnerCounters_COMMENT_INTERNAL_LIKE, userOwnerCounters_CREATE_COMMENT, userOwnerCounters_CREATE_IMAGE, userOwnerCounters_CREATE_LIKE, userOwnerCounters_CREATE_MOVIE, userOwnerCounters_CREATE_TOPIC, userOwnerCounters_IMAGE, userOwnerCounters_MOVIE_COMMENT_CREATE, userOwnerCounters_PHOTO_COMMENT_CREATE, userOwnerCounters_PHOTO_MARK_CREATE, userOwnerCounters_PHOTO_PIN_BATCH_CREATE, userOwnerCounters_PHOTO_PIN_UPDATE, userOwnerCounters_PHOTO_VIEW, userOwnerCounters_TEXT, userOwnerCounters_UNKNOWN, userOwnerCounters_USER_DELETE_MESSAGE, userOwnerCounters_USER_FEED_REMOVE, userOwnerCounters_USER_FORUM_MESSAGE_CREATE, userOwnerCounters_USER_INTERNAL_LIKE, userOwnerCounters_USER_INTERNAL_UNLIKE, userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE, userOwnerCounters_USER_PRESENT_SEND, userOwnerCounters_USER_PROFILE_VIEW, userOwnerCounters_USER_SEND_MESSAGE, userOwnerCounters_USER_STATUS_COMMENT_CREATE, userOwnerCounters_VIDEO, userOwnerCounters_VOTE_POLL, user_ID_Location, user_ID_country, user_birth_date, user_change_datime, user_create_date, user_gender, user_is_abused, user_is_activated, user_is_active, user_is_deleted, user_is_semiactivated, user_region, user_status];[0m
[0m2021.07.14 22:26:06 ERROR 'Project [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 133 more fields][0m
[0m2021.07.14 22:26:06 ERROR +- Project [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 132 more fields][0m
[0m2021.07.14 22:26:06 ERROR    +- Project [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 131 more fields][0m
[0m2021.07.14 22:26:06 ERROR       +- Project [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 130 more fields][0m
[0m2021.07.14 22:26:06 ERROR          +- Project [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 129 more fields][0m
[0m2021.07.14 22:26:06 ERROR             +- LogicalRDD [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 128 more fields], false[0m
[0m2021.07.14 22:26:06 ERROR [0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:155)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:152)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:341)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:341)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:338)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChild$2(TreeNode.scala:376)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$4(TreeNode.scala:437)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.map(TraversableLike.scala:238)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:437)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:338)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:338)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:338)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:104)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:116)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:116)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:127)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:132)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.immutable.List.foreach(List.scala:392)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.map(TraversableLike.scala:238)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.immutable.List.map(List.scala:298)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:132)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:137)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:137)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:104)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:152)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:93)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:183)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:93)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:90)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:155)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:176)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:3715)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset.select(Dataset.scala:1462)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2427)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2394)[0m
[0m2021.07.14 22:26:06 ERROR 	at ru.otus.bigdataml.ht3.OneHotEncoderToKnownClassesNumber.transform(OneHotEncoderToKnownClassesNumber.scala:46)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$6(Pipeline.scala:160)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:160)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.Iterator.foreach(Iterator.scala:941)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.Iterator.foreach$(Iterator.scala:941)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)[0m
[0m2021.07.14 22:26:06 ERROR 	at ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka$.$anonfun$main$2(SparkMLRegressionFromKafka.scala:76)[0m
[0m2021.07.14 22:26:06 ERROR 	at ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka$.$anonfun$main$2$adapted(SparkMLRegressionFromKafka.scala:47)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)[0m
[0m2021.07.14 22:26:06 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 22:26:06 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 22:26:06 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 22:26:06 ERROR [0m
[0m2021.07.14 22:26:06 ERROR 21/07/14 22:26:06 ERROR JobScheduler: Error running job streaming job 1626279960000 ms.0[0m
[0m2021.07.14 22:26:06 ERROR org.apache.spark.sql.AnalysisException: cannot resolve '`log_ownerId`' given input columns: [audit_pos, audit_resourceType_num, audit_resourceType_num_enc, auditedHour, auditweights_ageMs, auditweights_closed, auditweights_ctr_gender, auditweights_ctr_high, auditweights_ctr_negative, auditweights_dailyRecency, auditweights_feedOwner_RECOMMENDED_GROUP, auditweights_feedStats, auditweights_friendCommentFeeds, auditweights_friendCommenters, auditweights_friendLikes, auditweights_friendLikes_actors, auditweights_hasDetectedText, auditweights_hasText, auditweights_isPymk, auditweights_isRandom, auditweights_likersFeedStats_hyper, auditweights_likersSvd_prelaunch_hyper, auditweights_likersSvd_spark_hyper, auditweights_matrix, auditweights_notOriginalPhoto, auditweights_numDislikes, auditweights_numLikes, auditweights_numShows, auditweights_onlineVideo, auditweights_partAge, auditweights_partCtr, auditweights_partSvd, auditweights_processedVideo, auditweights_relationMasks, auditweights_source_LIVE_TOP, auditweights_source_MOVIE_TOP, auditweights_source_PROMO, auditweights_svd_prelaunch, auditweights_svd_spark, auditweights_userAge, auditweights_userOwner_CREATE_COMMENT, auditweights_userOwner_CREATE_IMAGE, auditweights_userOwner_CREATE_LIKE, auditweights_userOwner_IMAGE, auditweights_userOwner_MOVIE_COMMENT_CREATE, auditweights_userOwner_PHOTO_COMMENT_CREATE, auditweights_userOwner_PHOTO_MARK_CREATE, auditweights_userOwner_PHOTO_VIEW, auditweights_userOwner_TEXT, auditweights_userOwner_UNKNOWN, auditweights_userOwner_USER_DELETE_MESSAGE, auditweights_userOwner_USER_FEED_REMOVE, auditweights_userOwner_USER_FORUM_MESSAGE_CREATE, auditweights_userOwner_USER_INTERNAL_LIKE, auditweights_userOwner_USER_INTERNAL_UNLIKE, auditweights_userOwner_USER_PRESENT_SEND, auditweights_userOwner_USER_PROFILE_VIEW, auditweights_userOwner_USER_SEND_MESSAGE, auditweights_userOwner_USER_STATUS_COMMENT_CREATE, auditweights_userOwner_VIDEO, auditweights_userOwner_VOTE_POLL, auditweights_x_ActorsRelations, createdHour, instanceId_objectType_num, instanceId_objectType_num_enc, membership_status_num, membership_status_num_enc, metadata_numCompanions, metadata_numPhotos, metadata_numPolls, metadata_numSymbols, metadata_numTokens, metadata_numVideos, metadata_ownerType_num, metadata_ownerType_num_enc, metadata_totalVideoLength, ownerUserCounters_COMMENT_INTERNAL_LIKE, ownerUserCounters_CREATE_COMMENT, ownerUserCounters_CREATE_IMAGE, ownerUserCounters_CREATE_LIKE, ownerUserCounters_CREATE_MOVIE, ownerUserCounters_CREATE_TOPIC, ownerUserCounters_IMAGE, ownerUserCounters_MOVIE_COMMENT_CREATE, ownerUserCounters_PHOTO_COMMENT_CREATE, ownerUserCounters_PHOTO_MARK_CREATE, ownerUserCounters_PHOTO_PIN_BATCH_CREATE, ownerUserCounters_PHOTO_PIN_UPDATE, ownerUserCounters_PHOTO_VIEW, ownerUserCounters_TEXT, ownerUserCounters_UNKNOWN, ownerUserCounters_USER_DELETE_MESSAGE, ownerUserCounters_USER_FEED_REMOVE, ownerUserCounters_USER_FORUM_MESSAGE_CREATE, ownerUserCounters_USER_INTERNAL_LIKE, ownerUserCounters_USER_INTERNAL_UNLIKE, ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE, ownerUserCounters_USER_PRESENT_SEND, ownerUserCounters_USER_PROFILE_VIEW, ownerUserCounters_USER_SEND_MESSAGE, ownerUserCounters_USER_STATUS_COMMENT_CREATE, ownerUserCounters_VIDEO, ownerUserCounters_VOTE_POLL, owner_ID_Location, owner_ID_country, owner_change_datime, owner_gender, owner_is_abused, owner_is_activated, owner_is_active, owner_is_deleted, owner_is_semiactivated, owner_region, owner_status, target, timeDelta, userOwnerCounters_COMMENT_INTERNAL_LIKE, userOwnerCounters_CREATE_COMMENT, userOwnerCounters_CREATE_IMAGE, userOwnerCounters_CREATE_LIKE, userOwnerCounters_CREATE_MOVIE, userOwnerCounters_CREATE_TOPIC, userOwnerCounters_IMAGE, userOwnerCounters_MOVIE_COMMENT_CREATE, userOwnerCounters_PHOTO_COMMENT_CREATE, userOwnerCounters_PHOTO_MARK_CREATE, userOwnerCounters_PHOTO_PIN_BATCH_CREATE, userOwnerCounters_PHOTO_PIN_UPDATE, userOwnerCounters_PHOTO_VIEW, userOwnerCounters_TEXT, userOwnerCounters_UNKNOWN, userOwnerCounters_USER_DELETE_MESSAGE, userOwnerCounters_USER_FEED_REMOVE, userOwnerCounters_USER_FORUM_MESSAGE_CREATE, userOwnerCounters_USER_INTERNAL_LIKE, userOwnerCounters_USER_INTERNAL_UNLIKE, userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE, userOwnerCounters_USER_PRESENT_SEND, userOwnerCounters_USER_PROFILE_VIEW, userOwnerCounters_USER_SEND_MESSAGE, userOwnerCounters_USER_STATUS_COMMENT_CREATE, userOwnerCounters_VIDEO, userOwnerCounters_VOTE_POLL, user_ID_Location, user_ID_country, user_birth_date, user_change_datime, user_create_date, user_gender, user_is_abused, user_is_activated, user_is_active, user_is_deleted, user_is_semiactivated, user_region, user_status];[0m
[0m2021.07.14 22:26:06 ERROR 'Project [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 133 more fields][0m
[0m2021.07.14 22:26:06 ERROR +- Project [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 132 more fields][0m
[0m2021.07.14 22:26:06 ERROR    +- Project [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 131 more fields][0m
[0m2021.07.14 22:26:06 ERROR       +- Project [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 130 more fields][0m
[0m2021.07.14 22:26:06 ERROR          +- Project [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 129 more fields][0m
[0m2021.07.14 22:26:06 ERROR             +- LogicalRDD [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 128 more fields], false[0m
[0m2021.07.14 22:26:06 ERROR [0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:155)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:152)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:341)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:341)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:338)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChild$2(TreeNode.scala:376)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$4(TreeNode.scala:437)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.map(TraversableLike.scala:238)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:437)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:338)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:338)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:338)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:104)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:116)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:116)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:127)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:132)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.immutable.List.foreach(List.scala:392)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.map(TraversableLike.scala:238)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.immutable.List.map(List.scala:298)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:132)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:137)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:137)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:104)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:152)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:93)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:183)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:93)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:90)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:155)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:176)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:3715)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset.select(Dataset.scala:1462)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2427)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2394)[0m
[0m2021.07.14 22:26:06 ERROR 	at ru.otus.bigdataml.ht3.OneHotEncoderToKnownClassesNumber.transform(OneHotEncoderToKnownClassesNumber.scala:46)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$6(Pipeline.scala:160)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:160)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.Iterator.foreach(Iterator.scala:941)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.Iterator.foreach$(Iterator.scala:941)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)[0m
[0m2021.07.14 22:26:06 ERROR 	at ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka$.$anonfun$main$2(SparkMLRegressionFromKafka.scala:76)[0m
[0m2021.07.14 22:26:06 ERROR 	at ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka$.$anonfun$main$2$adapted(SparkMLRegressionFromKafka.scala:47)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)[0m
[0m2021.07.14 22:26:06 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 22:26:06 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 22:26:06 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m2021.07.14 22:26:06 ERROR Exception in thread "main" org.apache.spark.sql.AnalysisException: cannot resolve '`log_ownerId`' given input columns: [audit_pos, audit_resourceType_num, audit_resourceType_num_enc, auditedHour, auditweights_ageMs, auditweights_closed, auditweights_ctr_gender, auditweights_ctr_high, auditweights_ctr_negative, auditweights_dailyRecency, auditweights_feedOwner_RECOMMENDED_GROUP, auditweights_feedStats, auditweights_friendCommentFeeds, auditweights_friendCommenters, auditweights_friendLikes, auditweights_friendLikes_actors, auditweights_hasDetectedText, auditweights_hasText, auditweights_isPymk, auditweights_isRandom, auditweights_likersFeedStats_hyper, auditweights_likersSvd_prelaunch_hyper, auditweights_likersSvd_spark_hyper, auditweights_matrix, auditweights_notOriginalPhoto, auditweights_numDislikes, auditweights_numLikes, auditweights_numShows, auditweights_onlineVideo, auditweights_partAge, auditweights_partCtr, auditweights_partSvd, auditweights_processedVideo, auditweights_relationMasks, auditweights_source_LIVE_TOP, auditweights_source_MOVIE_TOP, auditweights_source_PROMO, auditweights_svd_prelaunch, auditweights_svd_spark, auditweights_userAge, auditweights_userOwner_CREATE_COMMENT, auditweights_userOwner_CREATE_IMAGE, auditweights_userOwner_CREATE_LIKE, auditweights_userOwner_IMAGE, auditweights_userOwner_MOVIE_COMMENT_CREATE, auditweights_userOwner_PHOTO_COMMENT_CREATE, auditweights_userOwner_PHOTO_MARK_CREATE, auditweights_userOwner_PHOTO_VIEW, auditweights_userOwner_TEXT, auditweights_userOwner_UNKNOWN, auditweights_userOwner_USER_DELETE_MESSAGE, auditweights_userOwner_USER_FEED_REMOVE, auditweights_userOwner_USER_FORUM_MESSAGE_CREATE, auditweights_userOwner_USER_INTERNAL_LIKE, auditweights_userOwner_USER_INTERNAL_UNLIKE, auditweights_userOwner_USER_PRESENT_SEND, auditweights_userOwner_USER_PROFILE_VIEW, auditweights_userOwner_USER_SEND_MESSAGE, auditweights_userOwner_USER_STATUS_COMMENT_CREATE, auditweights_userOwner_VIDEO, auditweights_userOwner_VOTE_POLL, auditweights_x_ActorsRelations, createdHour, instanceId_objectType_num, instanceId_objectType_num_enc, membership_status_num, membership_status_num_enc, metadata_numCompanions, metadata_numPhotos, metadata_numPolls, metadata_numSymbols, metadata_numTokens, metadata_numVideos, metadata_ownerType_num, metadata_ownerType_num_enc, metadata_totalVideoLength, ownerUserCounters_COMMENT_INTERNAL_LIKE, ownerUserCounters_CREATE_COMMENT, ownerUserCounters_CREATE_IMAGE, ownerUserCounters_CREATE_LIKE, ownerUserCounters_CREATE_MOVIE, ownerUserCounters_CREATE_TOPIC, ownerUserCounters_IMAGE, ownerUserCounters_MOVIE_COMMENT_CREATE, ownerUserCounters_PHOTO_COMMENT_CREATE, ownerUserCounters_PHOTO_MARK_CREATE, ownerUserCounters_PHOTO_PIN_BATCH_CREATE, ownerUserCounters_PHOTO_PIN_UPDATE, ownerUserCounters_PHOTO_VIEW, ownerUserCounters_TEXT, ownerUserCounters_UNKNOWN, ownerUserCounters_USER_DELETE_MESSAGE, ownerUserCounters_USER_FEED_REMOVE, ownerUserCounters_USER_FORUM_MESSAGE_CREATE, ownerUserCounters_USER_INTERNAL_LIKE, ownerUserCounters_USER_INTERNAL_UNLIKE, ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE, ownerUserCounters_USER_PRESENT_SEND, ownerUserCounters_USER_PROFILE_VIEW, ownerUserCounters_USER_SEND_MESSAGE, ownerUserCounters_USER_STATUS_COMMENT_CREATE, ownerUserCounters_VIDEO, ownerUserCounters_VOTE_POLL, owner_ID_Location, owner_ID_country, owner_change_datime, owner_gender, owner_is_abused, owner_is_activated, owner_is_active, owner_is_deleted, owner_is_semiactivated, owner_region, owner_status, target, timeDelta, userOwnerCounters_COMMENT_INTERNAL_LIKE, userOwnerCounters_CREATE_COMMENT, userOwnerCounters_CREATE_IMAGE, userOwnerCounters_CREATE_LIKE, userOwnerCounters_CREATE_MOVIE, userOwnerCounters_CREATE_TOPIC, userOwnerCounters_IMAGE, userOwnerCounters_MOVIE_COMMENT_CREATE, userOwnerCounters_PHOTO_COMMENT_CREATE, userOwnerCounters_PHOTO_MARK_CREATE, userOwnerCounters_PHOTO_PIN_BATCH_CREATE, userOwnerCounters_PHOTO_PIN_UPDATE, userOwnerCounters_PHOTO_VIEW, userOwnerCounters_TEXT, userOwnerCounters_UNKNOWN, userOwnerCounters_USER_DELETE_MESSAGE, userOwnerCounters_USER_FEED_REMOVE, userOwnerCounters_USER_FORUM_MESSAGE_CREATE, userOwnerCounters_USER_INTERNAL_LIKE, userOwnerCounters_USER_INTERNAL_UNLIKE, userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE, userOwnerCounters_USER_PRESENT_SEND, userOwnerCounters_USER_PROFILE_VIEW, userOwnerCounters_USER_SEND_MESSAGE, userOwnerCounters_USER_STATUS_COMMENT_CREATE, userOwnerCounters_VIDEO, userOwnerCounters_VOTE_POLL, user_ID_Location, user_ID_country, user_birth_date, user_change_datime, user_create_date, user_gender, user_is_abused, user_is_activated, user_is_active, user_is_deleted, user_is_semiactivated, user_region, user_status];[0m
[0m2021.07.14 22:26:06 ERROR 'Project [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 133 more fields][0m
[0m2021.07.14 22:26:06 ERROR +- Project [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 132 more fields][0m
[0m2021.07.14 22:26:06 ERROR    +- Project [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 131 more fields][0m
[0m2021.07.14 22:26:06 ERROR       +- Project [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 130 more fields][0m
[0m2021.07.14 22:26:06 ERROR          +- Project [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 129 more fields][0m
[0m2021.07.14 22:26:06 ERROR             +- LogicalRDD [audit_pos#6L, audit_resourceType_num#7L, auditedHour#8L, auditweights_ageMs#9, auditweights_closed#10, auditweights_ctr_gender#11, auditweights_ctr_high#12, auditweights_ctr_negative#13, auditweights_dailyRecency#14, auditweights_feedOwner_RECOMMENDED_GROUP#15, auditweights_feedStats#16, auditweights_friendCommentFeeds#17, auditweights_friendCommenters#18, auditweights_friendLikes#19, auditweights_friendLikes_actors#20, auditweights_hasDetectedText#21, auditweights_hasText#22, auditweights_isPymk#23, auditweights_isRandom#24, auditweights_likersFeedStats_hyper#25, auditweights_likersSvd_prelaunch_hyper#26, auditweights_likersSvd_spark_hyper#27, auditweights_matrix#28, auditweights_notOriginalPhoto#29, ... 128 more fields], false[0m
[0m2021.07.14 22:26:06 ERROR [0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:155)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:152)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:341)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:341)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:338)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChild$2(TreeNode.scala:376)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$4(TreeNode.scala:437)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.map(TraversableLike.scala:238)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:437)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:338)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:338)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:338)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:104)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:116)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:116)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:127)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:132)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.immutable.List.foreach(List.scala:392)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.map(TraversableLike.scala:238)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.immutable.List.map(List.scala:298)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:132)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:137)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:137)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:104)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:152)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:93)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:183)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:93)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:90)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:155)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:176)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:3715)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset.select(Dataset.scala:1462)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2427)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2394)[0m
[0m2021.07.14 22:26:06 ERROR 	at ru.otus.bigdataml.ht3.OneHotEncoderToKnownClassesNumber.transform(OneHotEncoderToKnownClassesNumber.scala:46)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$6(Pipeline.scala:160)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:160)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.Iterator.foreach(Iterator.scala:941)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.Iterator.foreach$(Iterator.scala:941)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)[0m
[0m2021.07.14 22:26:06 ERROR 	at ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka$.$anonfun$main$2(SparkMLRegressionFromKafka.scala:76)[0m
[0m2021.07.14 22:26:06 ERROR 	at ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka$.$anonfun$main$2$adapted(SparkMLRegressionFromKafka.scala:47)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.util.Try$.apply(Try.scala:213)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m2021.07.14 22:26:06 ERROR 	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)[0m
[0m2021.07.14 22:26:06 ERROR 	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)[0m
[0m2021.07.14 22:26:06 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m2021.07.14 22:26:06 ERROR 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m2021.07.14 22:26:06 ERROR 	at java.base/java.lang.Thread.run(Thread.java:829)[0m
Jul 14, 2021 10:26:15 PM org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer fireError
SEVERE: java.net.SocketException: Broken pipe (Write failed)
org.eclipse.lsp4j.jsonrpc.JsonRpcException: java.net.SocketException: Broken pipe (Write failed)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:72)
	at scala.meta.internal.metals.debug.SocketEndpoint.consume(SocketEndpoint.scala:22)
	at scala.meta.internal.metals.debug.MessageIdAdapter.consume(MessageIdAdapter.scala:43)
	at scala.meta.internal.metals.debug.ServerAdapter.send(ServerAdapter.scala:30)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleClientMessage$1(DebugProxy.scala:89)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToClient$1(DebugProxy.scala:54)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketException: Broken pipe (Write failed)
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)
	at java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:69)
	... 21 more

Jul 14, 2021 10:26:15 PM org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer fireError
SEVERE: java.net.SocketException: Broken pipe (Write failed)
org.eclipse.lsp4j.jsonrpc.JsonRpcException: java.net.SocketException: Broken pipe (Write failed)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:72)
	at scala.meta.internal.metals.debug.SocketEndpoint.consume(SocketEndpoint.scala:22)
	at scala.meta.internal.metals.debug.MessageIdAdapter.consume(MessageIdAdapter.scala:43)
	at scala.meta.internal.metals.debug.ServerAdapter.send(ServerAdapter.scala:30)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$handleClientMessage$1(DebugProxy.scala:89)
	at scala.meta.internal.metals.debug.MessageIdAdapter.$anonfun$listen$1(MessageIdAdapter.scala:57)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at scala.meta.internal.metals.debug.SocketEndpoint.listen(SocketEndpoint.scala:26)
	at scala.meta.internal.metals.debug.MessageIdAdapter.listen(MessageIdAdapter.scala:47)
	at scala.meta.internal.metals.debug.DebugProxy.$anonfun$listenToClient$1(DebugProxy.scala:54)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketException: Broken pipe (Write failed)
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)
	at java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:69)
	... 21 more

[0m2021.07.14 22:26:20 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka][0m
Jul 14, 2021 10:26:24 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6702
Jul 14, 2021 10:33:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6736
Jul 14, 2021 10:34:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6763
Jul 14, 2021 10:34:01 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6770
Jul 14, 2021 10:34:02 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6777
Jul 14, 2021 10:34:58 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6794
Jul 14, 2021 10:35:13 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6812
[0m2021.07.14 22:36:01 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 22:36:02 INFO  time: compiled ht3 in 1.26s[0m
Jul 14, 2021 10:36:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6860
[0m2021.07.14 22:36:17 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 2h 40m 5.808s)[0m
[0m2021.07.14 22:36:17 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 22:36:17 INFO  Listening for transport dt_socket at address: 51235[0m
[0m2021.07.14 22:36:17 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 22:36:17 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
[0m2021.07.14 22:36:17 INFO  Trying to attach to remote debuggee VM localhost:51235 .[0m
[0m2021.07.14 22:36:17 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.14 22:36:18 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 22:36:18 ERROR 21/07/14 22:36:18 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.31.26.139 instead (on interface eth0)[0m
[0m2021.07.14 22:36:18 ERROR 21/07/14 22:36:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 22:36:19 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 22:36:19 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 22:36:19 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 22:36:19 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 22:36:19 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 22:36:19 ERROR 21/07/14 22:36:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 22:36:28 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.14 22:36:29 INFO  Data: loaded [360130.0:181][0m
[0m2021.07.14 22:36:29 ERROR 21/07/14 22:36:30 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.14 22:36:36 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:36:36 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.14 22:36:36 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:36:36 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:36:36 INFO  |(179,[0,6,9,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:36:36 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:36:36 INFO  |(179,[0,5,8,18,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:36:36 INFO  |(179,[0,7,8,18,23...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:36:36 INFO  |(179,[0,7,8,18,24...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:36:36 INFO  |(179,[1,3,9,18,24...|                (3,[1],[1.0])|             (5,[0],[1.0])|             (2,[1],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:36:36 INFO  |(179,[0,6,8,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:36:36 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:36:36 INFO  |(179,[0,5,8,11,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:36:36 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:36:36 INFO  only showing top 10 rows[0m
[0m2021.07.14 22:36:36 INFO  [0m
[0m2021.07.14 22:36:41 ERROR 21/07/14 22:36:41 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.14 22:36:41 ERROR 21/07/14 22:36:41 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.14 22:36:41 ERROR 21/07/14 22:36:41 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.14 22:36:41 ERROR 21/07/14 22:36:41 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.14 22:36:45 INFO  Coefficients: [-2.5967280171047596E-4,-4.494321362881141E-4,-5.425830365581507E-5,1.4968289919657746E-4,-7.046664354626916E-5,-6.73194420172346E-5,-2.984041246866848E-5,-0.0013415632429407392,-0.006042132729059745,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-3.6025943894617344E-4,-3.7348387092096173E-4,-9.11080576656715E-5,1.9839813799671675E-4,8.351655751683117E-5,0.009670620656878596,-9.86600939340013E-4,-0.0011069801781385582,-7.091407779834137E-4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5.247108191509582E-4,0.0032973153766204643,5.132567547260851E-4,-0.0017698841368443262,-5.609433170648455E-4,2.0147474212735728E-4,-0.001710462351888302,-3.1012912665629106E-4,5.258280110111161E-5,-0.0017271415061610657,-8.169727913086987E-5,-3.054978207210116E-4,-0.0015006137784203124,7.099771608425771E-5,0.0,1.836359669475063E-4,9.237518902617028E-5,3.8386030145715915E-5,8.183078904792431E-5,-7.792007242442817E-4,8.603083874078905E-5,-0.0010452079255033947,-0.0010233833120702417,-1.4241909213358683E-4,-1.4241485756656664E-4,0.0,-6.774466534435697E-4,0.0,9.867083263805519E-5,-1.3451743732441655E-4,-1.3291200703640147E-4,2.751741490355994E-4,-1.2410162388225965E-4,-9.666416034505463E-5,-6.490411145985212E-5,-1.4515493606290564E-4,-8.355551321407813E-4,0.0,0.0,0.0,-0.0031562963405762237,0.0,0.0,-1.2381444214072627E-4,1.4368590970099287E-4,-7.619483523034829E-4,1.2565699447239102E-4,2.46952043351444E-4,-6.496976945629102E-4,0.010541546737744961,-0.002151990076253846,0.0,0.0,0.0,0.0,-0.0026972377073756345,-0.0024048975714460984,0.0,-0.003387117687438928,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0010975938419123558,0.0,-2.253085247785866E-4,0.0,0.0,-1.1412316272077202E-4,4.9426753235026866E-5,1.0325801339019074E-4] Intercept: 0.21054139666514413[0m
[0m2021.07.14 22:36:45 INFO  numIterations: 0[0m
[0m2021.07.14 22:36:45 INFO  objectiveHistory: [0.0][0m
[0m2021.07.14 22:36:45 INFO  RMSE: 0.3817729869930512[0m
[0m2021.07.14 22:36:45 INFO  r2: 0.013178296985180693[0m
[0m2021.07.14 22:36:51 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
Jul 14, 2021 10:37:04 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6871
Jul 14, 2021 10:38:34 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6880
Jul 14, 2021 10:38:58 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6909
Jul 14, 2021 10:38:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6915
[0m2021.07.14 22:39:02 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.4' (since 2h 42m 50.734s)[0m
[0m2021.07.14 22:39:02 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.14 22:39:02 INFO  Listening for transport dt_socket at address: 42259[0m
[0m2021.07.14 22:39:02 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.14 22:39:02 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka][0m
[0m2021.07.14 22:39:02 INFO  Trying to attach to remote debuggee VM localhost:42259 .[0m
[0m2021.07.14 22:39:02 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.14 22:39:03 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.14 22:39:03 ERROR 21/07/14 22:39:03 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.31.26.139 instead (on interface eth0)[0m
[0m2021.07.14 22:39:03 ERROR 21/07/14 22:39:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.14 22:39:04 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.14 22:39:04 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.14 22:39:04 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.14 22:39:04 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.14 22:39:04 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.14 22:39:04 ERROR 21/07/14 22:39:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.14 22:39:05 ERROR 21/07/14 22:39:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.[0m
[0m2021.07.14 22:39:05 ERROR 21/07/14 22:39:06 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.[0m
[0m2021.07.14 22:39:07 ERROR 21/07/14 22:39:07 WARN KafkaUtils: overriding enable.auto.commit to false for executor[0m
[0m2021.07.14 22:39:07 ERROR 21/07/14 22:39:07 WARN KafkaUtils: overriding auto.offset.reset to none for executor[0m
[0m2021.07.14 22:39:07 ERROR 21/07/14 22:39:07 WARN KafkaUtils: overriding executor group.id to spark-executor-spark_ml_testing[0m
[0m2021.07.14 22:39:07 ERROR 21/07/14 22:39:07 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135[0m
[0m2021.07.14 22:39:35 ERROR 21/07/14 22:39:35 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.14 22:39:35 INFO  Data: loaded [14.0:154][0m
[0m2021.07.14 22:39:39 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:39:39 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.14 22:39:39 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:39:39 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:39:39 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:39:39 INFO  |(179,[0,6,9,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:39:39 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:39:39 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:39:39 INFO  |(179,[0,6,8,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:39:39 INFO  |(179,[0,6,8,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:39:39 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:39:39 INFO  |(179,[0,6,9,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:39:39 INFO  |(179,[0,6,8,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:39:39 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:39:39 INFO  only showing top 10 rows[0m
[0m2021.07.14 22:39:39 INFO  [0m
[0m2021.07.14 22:39:39 ERROR 21/07/14 22:39:39 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.14 22:39:39 ERROR 21/07/14 22:39:39 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.14 22:39:39 ERROR 21/07/14 22:39:40 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.14 22:39:39 ERROR 21/07/14 22:39:40 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.14 22:39:39 INFO  Coefficients: [0.0029238933637508627,0.0014639327511142123,-0.007003161173178064,0.0,-0.003936357981345201,-0.00374499852527431,-0.00389466415770212,0.002343421284932663,0.0,0.018708198020082452,0.0,0.0,-0.005353290176578536,-0.0053532901765785375,0.0,0.0,0.0,0.002415640861648928,-0.00206391552772327,-0.005848896507948503,0.0,-0.0044804733675069385,0.0,-0.006527610182745225,-0.0036847807699898305,-0.007747183776084746,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.007271797327874874,0.0,0.011156753547333621,-0.005305233035916926,0.0,0.0072535391746453345,0.015205060797405783,0.0,0.0,0.0,0.0,0.010923240536085331,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0013477724926041264,-0.006661405890391986,0.0,-0.0023631760281188313,0.0014125989481387148,0.021386427875311524,0.021121083765896678,0.030412432772993225,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.002609394869591936,0.0,-0.005305233035916922,0.0,-0.005602984519783642,0.0,0.0,0.006937841489873103,0.0,0.0,0.0,0.0,0.0,0.0,0.005435950782871431,0.0,0.0,-0.0033011491368192844,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.009500308939732922,0.0014535967189495554,-0.01457070264543053,0.0027928071673596266,-0.0067167114923115554,0.00501451557736828,0.0,0.001463932751114164,0.001463932751114164,0.0,0.0,-0.008021171588704473,0.019353180287721816] Intercept: -0.07996340690946563[0m
[0m2021.07.14 22:39:39 INFO  numIterations: 0[0m
[0m2021.07.14 22:39:39 INFO  objectiveHistory: [0.0][0m
[0m2021.07.14 22:39:39 INFO  RMSE: 0.1458174445384465[0m
[0m2021.07.14 22:39:39 INFO  r2: 0.6794234986294083[0m
[0m2021.07.14 22:40:00 INFO  Data: loaded [15.0:154][0m
[0m2021.07.14 22:40:02 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:40:02 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.14 22:40:02 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:40:02 INFO  |(179,[0,6,9,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:02 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:02 INFO  |(179,[0,5,8,18,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:02 INFO  |(179,[0,5,8,11,23...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[4],[1.0])|[0m
[0m2021.07.14 22:40:02 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:02 INFO  |(179,[0,5,8,11,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:02 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:02 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:02 INFO  |(179,[0,6,8,11,22...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[3],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:02 INFO  |(179,[0,6,8,18,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:02 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:40:02 INFO  only showing top 10 rows[0m
[0m2021.07.14 22:40:02 INFO  [0m
[0m2021.07.14 22:40:02 INFO  Coefficients: [-0.017099155954133475,0.002146160439804972,-0.011043063024032308,0.0,0.02253027151368304,0.018243701404879784,0.02250651482266948,0.014018088311140564,0.02267098447960759,-0.0051638678954664405,-0.00527648447032999,-0.005276484470329978,-0.009846105899831889,-0.009846105899831883,0.0,0.022670984479607595,0.0,0.0021461604398061423,-0.006943770398562071,-0.009707039605024479,0.0,0.016343419595378328,0.0,-0.005539530333694053,-0.005433715134879613,-0.006110400346375271,0.0,0.0,0.0,0.0,-0.006623459804947202,0.0,0.0,0.0,0.0,-0.00952283136149239,0.02267098447960758,-0.011378411041213407,0.0,0.0,-0.0047849458085293525,0.02325783303684936,0.0,0.0,0.0,0.0,-0.01067568198841941,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0362968856273362,0.0,0.003974092058272422,0.0,-0.010726437670000281,-0.011160399353820929,-0.009082481136458865,-0.006450127227686932,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.006737179848729701,0.0,0.0,0.0,0.02181378492854368,0.0,-0.009082481136458863,0.040519790442007986,0.0,0.0,0.0,0.0,0.0,0.0,-0.008206394543312687,-0.009082481136458875,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.009082481136458879,0.0,0.054569335176277975,0.0015955096927930468,0.013519877005124917,0.011314939132094972,-0.011428804276679501,-0.014874198482617925,0.0,0.0021461604398059684,0.002146160439805877,0.0,0.0,0.03688965750677385,0.020850130375289946] Intercept: -0.4870454073379278[0m
[0m2021.07.14 22:40:02 INFO  numIterations: 0[0m
[0m2021.07.14 22:40:02 INFO  objectiveHistory: [0.0][0m
[0m2021.07.14 22:40:02 INFO  RMSE: 0.16787877610299493[0m
[0m2021.07.14 22:40:02 INFO  r2: 0.8238544783385031[0m
[0m2021.07.14 22:40:30 INFO  Data: loaded [17.0:154][0m
[0m2021.07.14 22:40:32 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:40:32 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.14 22:40:32 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:40:32 INFO  |(179,[0,6,8,18,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:32 INFO  |(179,[0,6,9,18,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:32 INFO  |(179,[0,6,8,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:32 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:32 INFO  |(179,[2,4,8,18,23...|                (3,[2],[1.0])|             (5,[1],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:32 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:32 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:32 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:32 INFO  |(179,[0,6,8,18,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:40:32 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[5],[1.0])|[0m
[0m2021.07.14 22:40:32 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:40:32 INFO  only showing top 10 rows[0m
[0m2021.07.14 22:40:32 INFO  [0m
[0m2021.07.14 22:40:32 INFO  Coefficients: [0.005745251065752837,0.009262398687201715,0.01676193578476412,0.0,-0.00294111350022188,2.4139747875176793E-4,0.01645981660609009,0.008161685067219744,0.0,0.028820123180048163,0.0,0.0,0.005106958069830676,0.005106958069830678,0.0,0.0,0.0,0.008720422057460363,0.023024450860195365,0.01380517304300375,0.0,0.006444840937040525,0.0,-0.012375679407852357,-0.0039045163037124413,-0.010939369877701069,0.0,0.0,0.0,0.0,-0.0073365550692638766,0.0,0.0,0.0,0.0,-0.0016034915689622478,0.0,0.005840359095208807,-0.0023830607283724134,0.0,0.0037049249928250328,0.018859584931792968,0.0,0.0,0.0,0.0,-0.01966677957668397,0.0,0.0,-0.004955679154603655,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0013488889101788105,-0.01045856838405935,0.0,-0.004556837335203154,-0.004454948848785835,-0.007234049676383912,-0.007102969958704409,6.303307258001394E-4,-0.0073365550692638705,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0016729981108523522,0.0,-0.004589585015372137,0.0,0.004858481683126791,0.0,0.0,0.048965399255050855,0.0,0.0,0.0,0.0,0.0,0.0,-0.004039539531036204,0.0,0.0,-0.007221795106680698,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0048432932518922825,0.009285687025943424,-0.0016371973586377015,-0.005862741299711988,0.00652862480441305,0.011888422336044072,0.0,0.009262398687201788,0.009262398687201787,0.0,0.0,-0.007829950701521753,0.011922321494856555] Intercept: -0.25755639615612475[0m
[0m2021.07.14 22:40:32 INFO  numIterations: 0[0m
[0m2021.07.14 22:40:32 INFO  objectiveHistory: [0.0][0m
[0m2021.07.14 22:40:32 INFO  RMSE: 0.21226609373995808[0m
[0m2021.07.14 22:40:32 INFO  r2: 0.5659519158193875[0m
[0m2021.07.14 22:41:01 INFO  Data: loaded [18.0:154][0m
[0m2021.07.14 22:41:02 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:41:02 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.14 22:41:02 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:41:02 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:41:02 INFO  |(179,[0,6,8,15,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[5],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:41:02 INFO  |(179,[0,6,8,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:41:02 INFO  |(179,[0,6,8,18,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[5],[1.0])|[0m
[0m2021.07.14 22:41:02 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:41:02 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:41:02 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[5],[1.0])|[0m
[0m2021.07.14 22:41:02 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:41:02 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.14 22:41:02 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[5],[1.0])|[0m
[0m2021.07.14 22:41:02 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.14 22:41:02 INFO  only showing top 10 rows[0m
[0m2021.07.14 22:41:02 INFO  [0m
[0m2021.07.14 22:41:03 INFO  Coefficients: [-0.006524244083339168,0.012214666605990014,0.04082109471533316,0.0,0.008990616534258597,0.007593132200008835,-0.005446546236618016,-0.04421912431191033,0.0,-4.45035526557832E-4,0.0,0.0,4.24474985500871E-4,4.244749855008729E-4,0.0,0.0,0.0,4.4030974637814814E-4,0.03026948363367072,0.005642507950712074,0.0,0.008806397955766572,0.0,-0.006229211363844132,-0.006614958825117186,-0.006044132719583599,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007742994281210678,0.0,-0.0022792043130581393,-0.007201827154893498,0.0,0.02085192062187318,-0.0020768630516670434,0.0,0.0,0.0,0.0,0.004653622008744993,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.004646319517382551,-9.017286589468715E-4,0.0,0.019666093636357105,-0.008326908812582865,0.00424217072631872,0.0053321363962221015,-0.00527902929443404,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.011925074589445497,0.0,-0.007201827154893497,0.0,-0.006517785968240177,0.0,0.0,8.773069994728736E-4,0.0,0.0,0.0,0.0,0.0,0.0,-3.2749487914891486E-4,-0.005328891369204837,0.0,-0.0032581106728529134,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.003776040237758462,0.012674316754911633,0.0030452862760592184,0.0026443460426843657,-0.003238240153632428,-7.308000694960944E-4,0.0,0.012214666605989787,0.012214666605989964,0.0,0.0,-0.0034599020600209252,0.012214666605989664] Intercept: -1.1470534390698242[0m
[0m2021.07.14 22:41:03 INFO  numIterations: 0[0m
[0m2021.07.14 22:41:03 INFO  objectiveHistory: [0.0][0m
[0m2021.07.14 22:41:03 INFO  RMSE: 0.15932747607471284[0m
[0m2021.07.14 22:41:03 INFO  r2: 0.7429743980975758[0m
[0m2021.07.14 22:41:11 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka][0m
[0m2021.07.14 22:41:11 INFO  Listening for transport dt_socket at address: 48891[0m
[0m2021.07.14 22:44:24 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 22:44:28 INFO  time: compiled ht3 in 4.04s[0m
Jul 14, 2021 11:14:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7120
Jul 14, 2021 11:15:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7126
Jul 14, 2021 11:17:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7193
Jul 14, 2021 11:17:58 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7199
Jul 14, 2021 11:18:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7218
Jul 14, 2021 11:32:32 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7235
[0m2021.07.14 23:32:33 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 23:32:36 INFO  time: compiled ht3 in 3.24s[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala in reflect.runtime.universe.TypeTag[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala, 863, 863, 873)
Jul 14, 2021 11:33:46 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7276
Jul 14, 2021 11:34:02 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7284
[0m2021.07.14 23:38:57 INFO  time: evaluated worksheet 'check.worksheet.sc' in 1.15s[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala in List[T]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataProvider.scala, 836, 836, 843)
Jul 14, 2021 11:43:35 PM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTrace
Jul 14, 2021 11:46:23 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7383
Jul 14, 2021 11:46:33 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7398
Jul 14, 2021 11:46:34 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7404
Jul 14, 2021 11:46:45 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7420
Jul 14, 2021 11:52:17 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7547
Jul 14, 2021 11:52:19 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7554
Jul 14, 2021 11:52:25 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7561
[0m2021.07.14 23:53:51 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 23:53:51 INFO  time: compiled ht3 in 0.82s[0m
Jul 14, 2021 11:54:07 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7601
Jul 14, 2021 11:55:27 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7629
Jul 14, 2021 11:55:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7635
[0m2021.07.14 23:55:52 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 23:55:52 INFO  time: compiled ht3 in 0.9s[0m
Jul 14, 2021 11:57:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7669
[0m2021.07.14 23:57:24 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.14 23:57:24 INFO  time: compiled ht3 in 0.26s[0m
Jul 14, 2021 11:57:49 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7725
Jul 14, 2021 11:58:01 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7737
[0m2021.07.15 00:53:01 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 00:53:04 INFO  time: compiled ht3 in 3.53s[0m
[0m2021.07.15 00:53:05 INFO  shutting down Metals[0m
[0m2021.07.15 00:53:05 INFO  Shut down connection with build server.[0m
[0m2021.07.15 00:53:05 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.07.15 06:40:34 INFO  Started: Metals version 0.10.5 in workspace '/home/hwait/dev/ht3' for client Visual Studio Code 1.58.1.[0m
[0m2021.07.15 06:40:35 INFO  time: initialize in 1.81s[0m
[0m2021.07.15 06:40:36 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.07.15 06:40:36 WARN  no build target for: /home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/SparkMLModelTraining.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5733920902607596576/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.07.15 06:40:37 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.07.15 06:40:40 INFO  no build target: using presentation compiler with only scala-library: 2.12.14[0m
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8-19-4d9f966b...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/hwait/dev/ht3/.bloop'...
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3-test.json'
[0m[32m[D][0m Loading project from '/home/hwait/dev/ht3/.bloop/ht3.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.12.10.
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.6/jline-2.14.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.12/jansi-1.12.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.10/scala-compiler-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar
[0m[32m[D][0m   => /home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.10/scala-reflect-2.12.10.jar
[0m[32m[D][0m Configured SemanticDB in projects 'ht3', 'ht3-test'
[0m[32m[D][0m Loading previous analysis for 'ht3-test' from '/home/hwait/dev/ht3/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Loading previous analysis for 'ht3' from '/home/hwait/dev/ht3/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.12.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5733920902607596576/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5733920902607596576/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.15 06:40:41 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.15 06:40:42 INFO  time: code lens generation in 5.49s[0m
[0m2021.07.15 06:40:45 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher10475948864806401810/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher10475948864806401810/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher10475948864806401810/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.07.15 06:40:45 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/bsp.trace.json[0m
[0m2021.07.15 06:40:46 INFO  time: Connected to build server in 9.98s[0m
[0m2021.07.15 06:40:46 INFO  Connected to Build server: Bloop v1.4.8-19-4d9f966b[0m
[0m2021.07.15 06:40:47 INFO  time: Imported build in 0.31s[0m
[0m2021.07.15 06:40:50 WARN  Could not find java sources in None. Java symbols will not be available.[0m
[0m2021.07.15 06:40:50 INFO  time: indexed workspace in 3.92s[0m
[0m2021.07.15 06:40:51 INFO  compiling ht3 (8 scala sources)[0m
[0m2021.07.15 06:40:59 INFO  time: compiled ht3 in 7.95s[0m
Jul 15, 2021 6:49:50 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 29
Jul 15, 2021 6:49:50 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 27
[0m2021.07.15 06:50:38 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 06:50:41 INFO  time: compiled ht3 in 2.33s[0m
[0m2021.07.15 06:50:41 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.5' (since 9m 59.271s)[0m
[0m2021.07.15 06:50:41 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.15 06:50:41 INFO  Listening for transport dt_socket at address: 41443[0m
[0m2021.07.15 06:50:41 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.15 06:50:41 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
[0m2021.07.15 06:50:41 INFO  Trying to attach to remote debuggee VM localhost:41443 .[0m
[0m2021.07.15 06:50:41 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.15 06:50:42 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.15 06:50:42 ERROR 21/07/15 06:50:42 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.29.131.255 instead (on interface eth0)[0m
[0m2021.07.15 06:50:42 ERROR 21/07/15 06:50:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.15 06:50:43 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.15 06:50:43 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.15 06:50:43 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.15 06:50:43 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.15 06:50:43 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.15 06:50:43 ERROR 21/07/15 06:50:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.15 06:50:52 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.15 06:50:53 INFO  instanceId_userId,instanceId_objectType,instanceId_objectId,audit_pos,audit_clientType,audit_timestamp,audit_timePassed,audit_experiment,audit_resourceType,metadata_ownerId,metadata_ownerType,metadata_createdAt,metadata_authorId,metadata_applicationId,metadata_numCompanions,metadata_numPhotos,metadata_numPolls,metadata_numSymbols,metadata_numTokens,metadata_numVideos,metadata_platform,metadata_totalVideoLength,metadata_options,relationsMask,userOwnerCounters_USER_FEED_REMOVE,userOwnerCounters_USER_PROFILE_VIEW,userOwnerCounters_VOTE_POLL,userOwnerCounters_USER_SEND_MESSAGE,userOwnerCounters_USER_DELETE_MESSAGE,userOwnerCounters_USER_INTERNAL_LIKE,userOwnerCounters_USER_INTERNAL_UNLIKE,userOwnerCounters_USER_STATUS_COMMENT_CREATE,userOwnerCounters_PHOTO_COMMENT_CREATE,userOwnerCounters_MOVIE_COMMENT_CREATE,userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE,userOwnerCounters_COMMENT_INTERNAL_LIKE,userOwnerCounters_USER_FORUM_MESSAGE_CREATE,userOwnerCounters_PHOTO_MARK_CREATE,userOwnerCounters_PHOTO_VIEW,userOwnerCounters_PHOTO_PIN_BATCH_CREATE,userOwnerCounters_PHOTO_PIN_UPDATE,userOwnerCounters_USER_PRESENT_SEND,userOwnerCounters_UNKNOWN,userOwnerCounters_CREATE_TOPIC,userOwnerCounters_CREATE_IMAGE,userOwnerCounters_CREATE_MOVIE,userOwnerCounters_CREATE_COMMENT,userOwnerCounters_CREATE_LIKE,userOwnerCounters_TEXT,userOwnerCounters_IMAGE,userOwnerCounters_VIDEO,ownerUserCounters_USER_FEED_REMOVE,ownerUserCounters_USER_PROFILE_VIEW,ownerUserCounters_VOTE_POLL,ownerUserCounters_USER_SEND_MESSAGE,ownerUserCounters_USER_DELETE_MESSAGE,ownerUserCounters_USER_INTERNAL_LIKE,ownerUserCounters_USER_INTERNAL_UNLIKE,ownerUserCounters_USER_STATUS_COMMENT_CREATE,ownerUserCounters_PHOTO_COMMENT_CREATE,ownerUserCounters_MOVIE_COMMENT_CREATE,ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE,ownerUserCounters_COMMENT_INTERNAL_LIKE,ownerUserCounters_USER_FORUM_MESSAGE_CREATE,ownerUserCounters_PHOTO_MARK_CREATE,ownerUserCounters_PHOTO_VIEW,ownerUserCounters_PHOTO_PIN_BATCH_CREATE,ownerUserCounters_PHOTO_PIN_UPDATE,ownerUserCounters_USER_PRESENT_SEND,ownerUserCounters_UNKNOWN,ownerUserCounters_CREATE_TOPIC,ownerUserCounters_CREATE_IMAGE,ownerUserCounters_CREATE_MOVIE,ownerUserCounters_CREATE_COMMENT,ownerUserCounters_CREATE_LIKE,ownerUserCounters_TEXT,ownerUserCounters_IMAGE,ownerUserCounters_VIDEO,membership_status,membership_statusUpdateDate,membership_joinDate,membership_joinRequestDate,owner_create_date,owner_birth_date,owner_gender,owner_status,owner_ID_country,owner_ID_Location,owner_is_active,owner_is_deleted,owner_is_abused,owner_is_activated,owner_change_datime,owner_is_semiactivated,owner_region,user_create_date,user_birth_date,user_gender,user_status,user_ID_country,user_ID_Location,user_is_active,user_is_deleted,user_is_abused,user_is_activated,user_change_datime,user_is_semiactivated,user_region,feedback,objectId,auditweights_ageMs,auditweights_closed,auditweights_ctr_gender,auditweights_ctr_high,auditweights_ctr_negative,auditweights_dailyRecency,auditweights_feedOwner_RECOMMENDED_GROUP,auditweights_feedStats,auditweights_friendCommentFeeds,auditweights_friendCommenters,auditweights_friendLikes,auditweights_friendLikes_actors,auditweights_hasDetectedText,auditweights_hasText,auditweights_isPymk,auditweights_isRandom,auditweights_likersFeedStats_hyper,auditweights_likersSvd_prelaunch_hyper,auditweights_matrix,auditweights_notOriginalPhoto,auditweights_numDislikes,auditweights_numLikes,auditweights_numShows,auditweights_onlineVideo,auditweights_partAge,auditweights_partCtr,auditweights_partSvd,auditweights_processedVideo,auditweights_relationMasks,auditweights_source_LIVE_TOP,auditweights_source_MOVIE_TOP,auditweights_svd_prelaunch,auditweights_svd_spark,auditweights_userAge,auditweights_userOwner_CREATE_COMMENT,auditweights_userOwner_CREATE_IMAGE,auditweights_userOwner_CREATE_LIKE,auditweights_userOwner_IMAGE,auditweights_userOwner_MOVIE_COMMENT_CREATE,auditweights_userOwner_PHOTO_COMMENT_CREATE,auditweights_userOwner_PHOTO_MARK_CREATE,auditweights_userOwner_PHOTO_VIEW,auditweights_userOwner_TEXT,auditweights_userOwner_UNKNOWN,auditweights_userOwner_USER_DELETE_MESSAGE,auditweights_userOwner_USER_FEED_REMOVE,auditweights_userOwner_USER_FORUM_MESSAGE_CREATE,auditweights_userOwner_USER_INTERNAL_LIKE,auditweights_userOwner_USER_INTERNAL_UNLIKE,auditweights_userOwner_USER_PRESENT_SEND,auditweights_userOwner_USER_PROFILE_VIEW,auditweights_userOwner_USER_SEND_MESSAGE,auditweights_userOwner_USER_STATUS_COMMENT_CREATE,auditweights_userOwner_VIDEO,auditweights_userOwner_VOTE_POLL,auditweights_x_ActorsRelations,auditweights_likersSvd_spark_hyper,auditweights_source_PROMO,date,target,createdTime,auditedTime,timeDelta,createdHour,auditedHour,log_ownerId,log_authorId,instanceId_objectType_num,audit_resourceType_num,metadata_ownerType_num,membership_status_num[0m
[0m2021.07.15 06:50:53 INFO  Data: loaded [360130.0:181][0m
[0m2021.07.15 06:50:53 ERROR 21/07/15 06:50:54 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.15 06:51:00 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 06:51:00 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.15 06:51:00 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 06:51:00 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:51:00 INFO  |(179,[0,6,9,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:51:00 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:51:00 INFO  |(179,[0,5,8,18,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:51:00 INFO  |(179,[0,7,8,18,23...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:51:00 INFO  |(179,[0,7,8,18,24...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:51:00 INFO  |(179,[1,3,9,18,24...|                (3,[1],[1.0])|             (5,[0],[1.0])|             (2,[1],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:51:00 INFO  |(179,[0,6,8,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:51:00 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:51:00 INFO  |(179,[0,5,8,11,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:51:00 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 06:51:00 INFO  only showing top 10 rows[0m
[0m2021.07.15 06:51:00 INFO  [0m
[0m2021.07.15 06:51:05 ERROR 21/07/15 06:51:05 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.15 06:51:05 ERROR 21/07/15 06:51:05 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.15 06:51:05 ERROR 21/07/15 06:51:05 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.15 06:51:05 ERROR 21/07/15 06:51:05 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.15 06:51:10 INFO  Coefficients: [-2.5967280171047705E-4,-4.4943213628811436E-4,-5.4258303655815764E-5,1.496828991965769E-4,-7.046664354626967E-5,-6.731944201723519E-5,-2.984041246866912E-5,-0.0013415632429407394,-0.006042132729059748,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-3.602594389461736E-4,-3.734838709209616E-4,-9.110805766567136E-5,1.9839813799671659E-4,8.351655751683134E-5,0.009670620656878591,-9.866009393400136E-4,-0.0011069801781385593,-7.091407779834133E-4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5.247108191509057E-4,0.003297315376620463,5.132567547260736E-4,-0.0017698841368443362,-5.609433170648613E-4,2.01474742127341E-4,-0.0017104623518883536,-3.10129126656292E-4,5.258280110111026E-5,-0.0017271415061610733,-8.169727913092738E-5,-3.054978207210119E-4,-0.0015006137784203271,7.099771608425419E-5,0.0,1.836359669475063E-4,9.237518902616978E-5,3.838603014571523E-5,8.18307890479239E-5,-7.792007242442824E-4,8.603083874078855E-5,-0.0010452079255033953,-0.0010233833120702417,-1.4241909213358708E-4,-1.424148575665667E-4,0.0,-6.774466534435696E-4,0.0,9.867083263805458E-5,-1.345174373244168E-4,-1.3291200703640155E-4,2.751741490355994E-4,-1.241016238822601E-4,-9.666416034505548E-5,-6.490411145985246E-5,-1.4515493606290572E-4,-8.355551321407808E-4,0.0,0.0,0.0,-0.0031562963405762246,0.0,0.0,-1.2381444214072632E-4,1.4368590970099238E-4,-7.619483523034831E-4,1.256569944723904E-4,2.4695204335144303E-4,-6.496976945629104E-4,0.010541546737744958,-0.002151990076253845,0.0,0.0,0.0,0.0,-0.002697237707375636,-0.002404897571446099,0.0,-0.00338711768743893,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0010975938419123558,0.0,-2.2530852477858668E-4,0.0,0.0,-1.1412316272077245E-4,4.9426753235026324E-5,1.0325801339019014E-4] Intercept: 0.21054139666514562[0m
[0m2021.07.15 06:51:10 INFO  numIterations: 0[0m
[0m2021.07.15 06:51:10 INFO  objectiveHistory: [0.0][0m
[0m2021.07.15 06:51:10 INFO  RMSE: 0.3817729869930512[0m
[0m2021.07.15 06:51:10 INFO  r2: 0.013178296985180693[0m
[0m2021.07.15 06:51:10 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
Jul 15, 2021 6:53:07 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 133
[0m2021.07.15 06:53:28 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.5' (since 12m 46.319s)[0m
[0m2021.07.15 06:53:28 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 06:53:28 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 06:53:29 INFO  time: compiled ht3 in 1.59s[0m
[0m2021.07.15 06:53:29 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.15 06:53:29 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.15 06:53:29 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
[0m2021.07.15 06:53:29 INFO  Listening for transport dt_socket at address: 35661[0m
[0m2021.07.15 06:53:29 INFO  Trying to attach to remote debuggee VM localhost:35661 .[0m
[0m2021.07.15 06:53:29 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.15 06:53:31 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.15 06:53:31 ERROR 21/07/15 06:53:31 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.29.131.255 instead (on interface eth0)[0m
[0m2021.07.15 06:53:31 ERROR 21/07/15 06:53:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.15 06:53:32 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.15 06:53:32 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.15 06:53:32 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.15 06:53:32 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.15 06:53:32 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.15 06:53:32 ERROR 21/07/15 06:53:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.15 06:53:40 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.15 06:53:41 INFO  instanceId_userId","instanceId_objectType","instanceId_objectId","audit_pos","audit_clientType","audit_timestamp","audit_timePassed","audit_experiment","audit_resourceType","metadata_ownerId","metadata_ownerType","metadata_createdAt","metadata_authorId","metadata_applicationId","metadata_numCompanions","metadata_numPhotos","metadata_numPolls","metadata_numSymbols","metadata_numTokens","metadata_numVideos","metadata_platform","metadata_totalVideoLength","metadata_options","relationsMask","userOwnerCounters_USER_FEED_REMOVE","userOwnerCounters_USER_PROFILE_VIEW","userOwnerCounters_VOTE_POLL","userOwnerCounters_USER_SEND_MESSAGE","userOwnerCounters_USER_DELETE_MESSAGE","userOwnerCounters_USER_INTERNAL_LIKE","userOwnerCounters_USER_INTERNAL_UNLIKE","userOwnerCounters_USER_STATUS_COMMENT_CREATE","userOwnerCounters_PHOTO_COMMENT_CREATE","userOwnerCounters_MOVIE_COMMENT_CREATE","userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE","userOwnerCounters_COMMENT_INTERNAL_LIKE","userOwnerCounters_USER_FORUM_MESSAGE_CREATE","userOwnerCounters_PHOTO_MARK_CREATE","userOwnerCounters_PHOTO_VIEW","userOwnerCounters_PHOTO_PIN_BATCH_CREATE","userOwnerCounters_PHOTO_PIN_UPDATE","userOwnerCounters_USER_PRESENT_SEND","userOwnerCounters_UNKNOWN","userOwnerCounters_CREATE_TOPIC","userOwnerCounters_CREATE_IMAGE","userOwnerCounters_CREATE_MOVIE","userOwnerCounters_CREATE_COMMENT","userOwnerCounters_CREATE_LIKE","userOwnerCounters_TEXT","userOwnerCounters_IMAGE","userOwnerCounters_VIDEO","ownerUserCounters_USER_FEED_REMOVE","ownerUserCounters_USER_PROFILE_VIEW","ownerUserCounters_VOTE_POLL","ownerUserCounters_USER_SEND_MESSAGE","ownerUserCounters_USER_DELETE_MESSAGE","ownerUserCounters_USER_INTERNAL_LIKE","ownerUserCounters_USER_INTERNAL_UNLIKE","ownerUserCounters_USER_STATUS_COMMENT_CREATE","ownerUserCounters_PHOTO_COMMENT_CREATE","ownerUserCounters_MOVIE_COMMENT_CREATE","ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE","ownerUserCounters_COMMENT_INTERNAL_LIKE","ownerUserCounters_USER_FORUM_MESSAGE_CREATE","ownerUserCounters_PHOTO_MARK_CREATE","ownerUserCounters_PHOTO_VIEW","ownerUserCounters_PHOTO_PIN_BATCH_CREATE","ownerUserCounters_PHOTO_PIN_UPDATE","ownerUserCounters_USER_PRESENT_SEND","ownerUserCounters_UNKNOWN","ownerUserCounters_CREATE_TOPIC","ownerUserCounters_CREATE_IMAGE","ownerUserCounters_CREATE_MOVIE","ownerUserCounters_CREATE_COMMENT","ownerUserCounters_CREATE_LIKE","ownerUserCounters_TEXT","ownerUserCounters_IMAGE","ownerUserCounters_VIDEO","membership_status","membership_statusUpdateDate","membership_joinDate","membership_joinRequestDate","owner_create_date","owner_birth_date","owner_gender","owner_status","owner_ID_country","owner_ID_Location","owner_is_active","owner_is_deleted","owner_is_abused","owner_is_activated","owner_change_datime","owner_is_semiactivated","owner_region","user_create_date","user_birth_date","user_gender","user_status","user_ID_country","user_ID_Location","user_is_active","user_is_deleted","user_is_abused","user_is_activated","user_change_datime","user_is_semiactivated","user_region","feedback","objectId","auditweights_ageMs","auditweights_closed","auditweights_ctr_gender","auditweights_ctr_high","auditweights_ctr_negative","auditweights_dailyRecency","auditweights_feedOwner_RECOMMENDED_GROUP","auditweights_feedStats","auditweights_friendCommentFeeds","auditweights_friendCommenters","auditweights_friendLikes","auditweights_friendLikes_actors","auditweights_hasDetectedText","auditweights_hasText","auditweights_isPymk","auditweights_isRandom","auditweights_likersFeedStats_hyper","auditweights_likersSvd_prelaunch_hyper","auditweights_matrix","auditweights_notOriginalPhoto","auditweights_numDislikes","auditweights_numLikes","auditweights_numShows","auditweights_onlineVideo","auditweights_partAge","auditweights_partCtr","auditweights_partSvd","auditweights_processedVideo","auditweights_relationMasks","auditweights_source_LIVE_TOP","auditweights_source_MOVIE_TOP","auditweights_svd_prelaunch","auditweights_svd_spark","auditweights_userAge","auditweights_userOwner_CREATE_COMMENT","auditweights_userOwner_CREATE_IMAGE","auditweights_userOwner_CREATE_LIKE","auditweights_userOwner_IMAGE","auditweights_userOwner_MOVIE_COMMENT_CREATE","auditweights_userOwner_PHOTO_COMMENT_CREATE","auditweights_userOwner_PHOTO_MARK_CREATE","auditweights_userOwner_PHOTO_VIEW","auditweights_userOwner_TEXT","auditweights_userOwner_UNKNOWN","auditweights_userOwner_USER_DELETE_MESSAGE","auditweights_userOwner_USER_FEED_REMOVE","auditweights_userOwner_USER_FORUM_MESSAGE_CREATE","auditweights_userOwner_USER_INTERNAL_LIKE","auditweights_userOwner_USER_INTERNAL_UNLIKE","auditweights_userOwner_USER_PRESENT_SEND","auditweights_userOwner_USER_PROFILE_VIEW","auditweights_userOwner_USER_SEND_MESSAGE","auditweights_userOwner_USER_STATUS_COMMENT_CREATE","auditweights_userOwner_VIDEO","auditweights_userOwner_VOTE_POLL","auditweights_x_ActorsRelations","auditweights_likersSvd_spark_hyper","auditweights_source_PROMO","date","target","createdTime","auditedTime","timeDelta","createdHour","auditedHour","log_ownerId","log_authorId","instanceId_objectType_num","audit_resourceType_num","metadata_ownerType_num","membership_status_num[0m
[0m2021.07.15 06:53:41 INFO  Data: loaded [360130.0:181][0m
[0m2021.07.15 06:53:42 ERROR 21/07/15 06:53:42 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.15 06:53:48 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 06:53:48 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.15 06:53:48 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 06:53:48 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:53:48 INFO  |(179,[0,6,9,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:53:48 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:53:48 INFO  |(179,[0,5,8,18,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:53:48 INFO  |(179,[0,7,8,18,23...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:53:48 INFO  |(179,[0,7,8,18,24...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:53:48 INFO  |(179,[1,3,9,18,24...|                (3,[1],[1.0])|             (5,[0],[1.0])|             (2,[1],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:53:48 INFO  |(179,[0,6,8,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:53:48 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:53:48 INFO  |(179,[0,5,8,11,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 06:53:48 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 06:53:48 INFO  only showing top 10 rows[0m
[0m2021.07.15 06:53:48 INFO  [0m
[0m2021.07.15 06:53:53 ERROR 21/07/15 06:53:53 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.15 06:53:53 ERROR 21/07/15 06:53:53 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.15 06:53:53 ERROR 21/07/15 06:53:53 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.15 06:53:53 ERROR 21/07/15 06:53:53 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
Jul 15, 2021 6:53:54 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 179
[0m2021.07.15 06:53:57 INFO  Coefficients: [-2.5967280171047596E-4,-4.494321362881141E-4,-5.425830365581507E-5,1.4968289919657746E-4,-7.046664354626916E-5,-6.73194420172346E-5,-2.984041246866848E-5,-0.0013415632429407392,-0.006042132729059745,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-3.6025943894617344E-4,-3.7348387092096173E-4,-9.11080576656715E-5,1.9839813799671675E-4,8.351655751683117E-5,0.009670620656878596,-9.86600939340013E-4,-0.0011069801781385582,-7.091407779834137E-4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5.247108191509582E-4,0.0032973153766204643,5.132567547260851E-4,-0.0017698841368443262,-5.609433170648455E-4,2.0147474212735728E-4,-0.001710462351888302,-3.1012912665629106E-4,5.258280110111161E-5,-0.0017271415061610657,-8.169727913086987E-5,-3.054978207210116E-4,-0.0015006137784203124,7.099771608425771E-5,0.0,1.836359669475063E-4,9.237518902617028E-5,3.8386030145715915E-5,8.183078904792431E-5,-7.792007242442817E-4,8.603083874078905E-5,-0.0010452079255033947,-0.0010233833120702417,-1.4241909213358683E-4,-1.4241485756656664E-4,0.0,-6.774466534435697E-4,0.0,9.867083263805519E-5,-1.3451743732441655E-4,-1.3291200703640147E-4,2.751741490355994E-4,-1.2410162388225965E-4,-9.666416034505463E-5,-6.490411145985212E-5,-1.4515493606290564E-4,-8.355551321407813E-4,0.0,0.0,0.0,-0.0031562963405762237,0.0,0.0,-1.2381444214072627E-4,1.4368590970099287E-4,-7.619483523034829E-4,1.2565699447239102E-4,2.46952043351444E-4,-6.496976945629102E-4,0.010541546737744961,-0.002151990076253846,0.0,0.0,0.0,0.0,-0.0026972377073756345,-0.0024048975714460984,0.0,-0.003387117687438928,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0010975938419123558,0.0,-2.253085247785866E-4,0.0,0.0,-1.1412316272077202E-4,4.9426753235026866E-5,1.0325801339019074E-4] Intercept: 0.21054139666514413[0m
[0m2021.07.15 06:53:57 INFO  numIterations: 0[0m
[0m2021.07.15 06:53:57 INFO  objectiveHistory: [0.0][0m
[0m2021.07.15 06:53:57 INFO  RMSE: 0.3817729869930512[0m
[0m2021.07.15 06:53:57 INFO  r2: 0.013178296985180693[0m
[0m2021.07.15 06:53:57 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
Jul 15, 2021 6:54:30 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 299
[0m2021.07.15 06:55:23 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 06:55:24 INFO  time: compiled ht3 in 1.25s[0m
[0m2021.07.15 06:55:24 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 06:55:24 INFO  time: compiled ht3 in 0.4s[0m
[0m2021.07.15 06:55:33 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 06:55:34 INFO  time: compiled ht3 in 1.1s[0m
[0m2021.07.15 06:55:34 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 06:55:34 INFO  time: compiled ht3 in 0.28s[0m
Jul 15, 2021 6:57:51 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 543
[0m2021.07.15 06:58:01 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 06:58:01 INFO  time: compiled ht3 in 0.99s[0m
[0m2021.07.15 06:58:01 INFO  compiling ht3 (3 scala sources)[0m
Jul 15, 2021 6:58:02 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 563
[0m2021.07.15 06:58:03 INFO  time: compiled ht3 in 0.3s[0m
[0m2021.07.15 06:58:03 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 06:58:03 INFO  time: compiled ht3 in 0.29s[0m
[0m2021.07.15 06:58:03 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 06:58:03 INFO  time: compiled ht3 in 0.27s[0m
Jul 15, 2021 6:58:04 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 572
Jul 15, 2021 6:58:26 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 597
Jul 15, 2021 6:58:32 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 611
Jul 15, 2021 6:58:45 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 636
[0m2021.07.15 07:01:34 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:01:34 INFO  time: compiled ht3 in 0.7s[0m
[0m2021.07.15 07:01:34 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:01:35 INFO  time: compiled ht3 in 0.4s[0m
[0m2021.07.15 07:01:42 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:01:42 INFO  time: compiled ht3 in 0.35s[0m
Jul 15, 2021 7:01:50 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1018
Jul 15, 2021 7:01:57 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1028
Jul 15, 2021 7:01:59 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1037
[0m2021.07.15 07:02:06 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:02:06 INFO  time: compiled ht3 in 0.15s[0m
Jul 15, 2021 7:02:08 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1073
Jul 15, 2021 7:02:08 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1074
Jul 15, 2021 7:02:26 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1100
Jul 15, 2021 7:02:39 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1113
Jul 15, 2021 7:03:34 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1137
Jul 15, 2021 7:04:19 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1143
[0m2021.07.15 07:06:21 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:06:21 INFO  time: compiled ht3 in 0.33s[0m
[0m2021.07.15 07:06:25 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:06:25 INFO  time: compiled ht3 in 0.39s[0m
[0m2021.07.15 07:06:36 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:06:36 INFO  time: compiled ht3 in 0.37s[0m
Jul 15, 2021 7:06:56 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1217
[0m2021.07.15 07:07:04 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:07:04 INFO  time: compiled ht3 in 0.35s[0m
[0m2021.07.15 07:07:04 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:07:04 INFO  time: compiled ht3 in 0.28s[0m
Jul 15, 2021 7:07:04 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1233
Jul 15, 2021 7:07:12 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1249
[0m2021.07.15 07:07:15 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:07:15 INFO  time: compiled ht3 in 0.28s[0m
[0m2021.07.15 07:07:15 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:07:15 INFO  time: compiled ht3 in 0.28s[0m
Jul 15, 2021 7:07:18 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1269
[0m2021.07.15 07:08:05 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:08:05 INFO  time: compiled ht3 in 0.35s[0m
[0m2021.07.15 07:08:11 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.5' (since 27m 29.776s)[0m
[0m2021.07.15 07:08:11 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:08:11 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:08:11 INFO  time: compiled ht3 in 0.32s[0m
[0m2021.07.15 07:08:11 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:08:11 INFO  time: compiled ht3 in 0.25s[0m
Jul 15, 2021 7:08:12 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
java.util.concurrent.CompletionException: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:29)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:26)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:237)
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:229)
	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417)
	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
	... 4 more

Jul 15, 2021 7:08:24 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1388
Jul 15, 2021 7:08:29 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1398
Jul 15, 2021 7:08:37 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1408
[0m2021.07.15 07:08:44 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:08:44 INFO  time: compiled ht3 in 0.3s[0m
Jul 15, 2021 7:09:09 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1472
Jul 15, 2021 7:09:14 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1487
Jul 15, 2021 7:09:15 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1493
[0m2021.07.15 07:09:17 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:09:17 INFO  time: compiled ht3 in 0.32s[0m
[0m2021.07.15 07:09:40 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:09:41 INFO  time: compiled ht3 in 1.57s[0m
[0m2021.07.15 07:09:41 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 07:09:41 INFO  time: compiled ht3 in 0.13s[0m
[0m2021.07.15 07:09:46 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:09:48 INFO  time: compiled ht3 in 1.51s[0m
[0m2021.07.15 07:09:48 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 07:09:48 INFO  time: compiled ht3 in 0.1s[0m
[0m2021.07.15 07:09:58 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 07:09:59 INFO  time: compiled ht3 in 1.47s[0m
[0m2021.07.15 07:09:59 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 07:09:59 INFO  time: compiled ht3 in 0.11s[0m
Jul 15, 2021 7:10:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1585
Jul 15, 2021 7:10:06 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1595
[0m2021.07.15 07:10:18 INFO  compiling ht3 (4 scala sources)[0m
[0m2021.07.15 07:10:18 INFO  time: compiled ht3 in 0.41s[0m
Jul 15, 2021 7:10:35 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1657
Jul 15, 2021 7:10:39 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1669
[0m2021.07.15 07:10:42 INFO  compiling ht3 (4 scala sources)[0m
[0m2021.07.15 07:10:44 INFO  time: compiled ht3 in 1.54s[0m
Jul 15, 2021 7:11:10 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1687
[0m2021.07.15 07:11:11 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.5' (since 30m 30.007s)[0m
[0m2021.07.15 07:11:11 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.15 07:11:11 INFO  Listening for transport dt_socket at address: 54535[0m
[0m2021.07.15 07:11:11 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.15 07:11:12 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
[0m2021.07.15 07:11:11 INFO  Trying to attach to remote debuggee VM localhost:54535 .[0m
[0m2021.07.15 07:11:11 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.15 07:11:13 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.15 07:11:13 ERROR 21/07/15 07:11:13 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.29.131.255 instead (on interface eth0)[0m
[0m2021.07.15 07:11:13 ERROR 21/07/15 07:11:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.15 07:11:14 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.15 07:11:14 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.15 07:11:14 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.15 07:11:14 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.15 07:11:14 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.15 07:11:14 ERROR 21/07/15 07:11:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.15 07:11:23 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.15 07:11:24 INFO  Data: loaded [360130.0:181][0m
[0m2021.07.15 07:11:24 ERROR 21/07/15 07:11:25 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.15 07:11:31 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 07:11:31 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.15 07:11:31 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 07:11:31 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 07:11:31 INFO  |(179,[0,6,9,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 07:11:31 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 07:11:31 INFO  |(179,[0,5,8,18,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 07:11:31 INFO  |(179,[0,7,8,18,23...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 07:11:31 INFO  |(179,[0,7,8,18,24...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 07:11:31 INFO  |(179,[1,3,9,18,24...|                (3,[1],[1.0])|             (5,[0],[1.0])|             (2,[1],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 07:11:31 INFO  |(179,[0,6,8,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 07:11:31 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 07:11:31 INFO  |(179,[0,5,8,11,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 07:11:31 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 07:11:31 INFO  only showing top 10 rows[0m
[0m2021.07.15 07:11:31 INFO  [0m
[0m2021.07.15 07:11:35 ERROR 21/07/15 07:11:35 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.15 07:11:35 ERROR 21/07/15 07:11:35 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.15 07:11:35 ERROR 21/07/15 07:11:36 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.15 07:11:35 ERROR 21/07/15 07:11:36 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.15 07:11:40 INFO  Coefficients: [-2.5967280171047596E-4,-4.494321362881141E-4,-5.425830365581507E-5,1.4968289919657746E-4,-7.046664354626916E-5,-6.73194420172346E-5,-2.984041246866848E-5,-0.0013415632429407392,-0.006042132729059745,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-3.6025943894617344E-4,-3.7348387092096173E-4,-9.11080576656715E-5,1.9839813799671675E-4,8.351655751683117E-5,0.009670620656878596,-9.86600939340013E-4,-0.0011069801781385582,-7.091407779834137E-4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5.247108191509582E-4,0.0032973153766204643,5.132567547260851E-4,-0.0017698841368443262,-5.609433170648455E-4,2.0147474212735728E-4,-0.001710462351888302,-3.1012912665629106E-4,5.258280110111161E-5,-0.0017271415061610657,-8.169727913086987E-5,-3.054978207210116E-4,-0.0015006137784203124,7.099771608425771E-5,0.0,1.836359669475063E-4,9.237518902617028E-5,3.8386030145715915E-5,8.183078904792431E-5,-7.792007242442817E-4,8.603083874078905E-5,-0.0010452079255033947,-0.0010233833120702417,-1.4241909213358683E-4,-1.4241485756656664E-4,0.0,-6.774466534435697E-4,0.0,9.867083263805519E-5,-1.3451743732441655E-4,-1.3291200703640147E-4,2.751741490355994E-4,-1.2410162388225965E-4,-9.666416034505463E-5,-6.490411145985212E-5,-1.4515493606290564E-4,-8.355551321407813E-4,0.0,0.0,0.0,-0.0031562963405762237,0.0,0.0,-1.2381444214072627E-4,1.4368590970099287E-4,-7.619483523034829E-4,1.2565699447239102E-4,2.46952043351444E-4,-6.496976945629102E-4,0.010541546737744961,-0.002151990076253846,0.0,0.0,0.0,0.0,-0.0026972377073756345,-0.0024048975714460984,0.0,-0.003387117687438928,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0010975938419123558,0.0,-2.253085247785866E-4,0.0,0.0,-1.1412316272077202E-4,4.9426753235026866E-5,1.0325801339019074E-4] Intercept: 0.21054139666514413[0m
[0m2021.07.15 07:11:40 INFO  numIterations: 0[0m
[0m2021.07.15 07:11:40 INFO  objectiveHistory: [0.0][0m
[0m2021.07.15 07:11:40 INFO  RMSE: 0.3817729869930512[0m
[0m2021.07.15 07:11:40 INFO  r2: 0.013178296985180693[0m
[0m2021.07.15 07:11:40 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
[0m2021.07.15 07:14:32 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 07:14:33 INFO  time: compiled ht3 in 1.16s[0m
Jul 15, 2021 7:20:31 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1747
Jul 15, 2021 7:22:35 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1838
Jul 15, 2021 7:23:02 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1849
Jul 15, 2021 7:23:20 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1901
Jul 15, 2021 7:23:34 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1913
Jul 15, 2021 7:23:47 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1933
Jul 15, 2021 7:23:50 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1942
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in Array[<error>]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 87, 87, 102)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in Array[<error>]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 87, 87, 104)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in Array[<error>]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 87, 87, 97)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in Array[<error>]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 87, 87, 104)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in Array[<error>]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 87, 87, 97)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in Array[<error>]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 87, 87, 98)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in Array[<error>]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 87, 87, 99)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in Array[<error>]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 87, 87, 100)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in Array[<error>]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 87, 87, 101)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in Array[String]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 87, 87, 102)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in (String, <error>)RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 93, 93, 110)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in (String, <error>)RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 93, 93, 111)
Jul 15, 2021 7:24:13 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2084
Jul 15, 2021 7:24:22 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2099
[0m2021.07.15 07:24:38 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 07:24:38 INFO  time: compiled ht3 in 0.25s[0m
[0m2021.07.15 07:24:38 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 07:24:38 INFO  time: compiled ht3 in 0.17s[0m
Jul 15, 2021 7:24:41 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2152
Jul 15, 2021 7:24:55 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2178
Jul 15, 2021 7:25:15 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2245
Jul 15, 2021 7:25:49 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2266
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in Array[String]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 183, 183, 196)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in Array[(String, Int)]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 87, 87, 107)
[0m2021.07.15 07:26:11 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 07:26:11 INFO  time: compiled ht3 in 0.24s[0m
Jul 15, 2021 7:26:19 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2348
Jul 15, 2021 7:26:56 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2370
Jul 15, 2021 7:27:19 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2456
Jul 15, 2021 7:27:25 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2468
Jul 15, 2021 7:27:35 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2501
Jul 15, 2021 7:27:38 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2512
Jul 15, 2021 7:27:46 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2538
Jul 15, 2021 7:27:57 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2555
Jul 15, 2021 7:28:09 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2594
Jul 15, 2021 7:28:25 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2610
Jul 15, 2021 7:28:43 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2642
Jul 15, 2021 7:29:02 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2662
Jul 15, 2021 7:29:21 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2710
Jul 15, 2021 7:29:35 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2730
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in List[String]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 182, 182, 194)
Jul 15, 2021 7:30:05 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2747
Jul 15, 2021 7:30:13 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2758
Jul 15, 2021 7:30:38 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2826
Jul 15, 2021 7:30:46 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2837
Jul 15, 2021 7:30:58 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2843
Jul 15, 2021 7:31:02 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2853
Jul 15, 2021 7:31:40 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2965
Jul 15, 2021 7:31:50 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2983
Jul 15, 2021 7:31:56 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3005
Jul 15, 2021 7:32:01 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3017
Jul 15, 2021 7:32:06 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3033
Jul 15, 2021 7:32:12 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3047
Jul 15, 2021 7:32:19 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3067
Jul 15, 2021 7:32:25 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3078
[0m2021.07.15 07:32:28 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 07:32:28 INFO  time: compiled ht3 in 0.25s[0m
[0m2021.07.15 07:33:53 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 07:33:53 INFO  time: compiled ht3 in 0.26s[0m
Jul 15, 2021 7:34:55 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3187
Jul 15, 2021 7:35:15 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3203
[0m2021.07.15 07:35:29 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 07:35:29 INFO  time: compiled ht3 in 0.22s[0m
[0m2021.07.15 07:35:45 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 07:35:45 INFO  time: compiled ht3 in 0.25s[0m
[0m2021.07.15 07:36:23 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 07:36:23 INFO  time: compiled ht3 in 0.24s[0m
[0m2021.07.15 07:36:25 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 07:36:25 INFO  time: compiled ht3 in 0.23s[0m
Jul 15, 2021 7:36:28 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3287
Jul 15, 2021 7:36:56 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3320
Jul 15, 2021 7:37:36 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3416
[0m2021.07.15 07:37:46 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 07:37:46 INFO  time: compiled ht3 in 0.24s[0m
Jul 15, 2021 7:37:47 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3433
Jul 15, 2021 7:38:19 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3510
Jul 15, 2021 7:38:24 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3520
[0m2021.07.15 07:39:18 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 07:39:18 INFO  time: compiled ht3 in 0.24s[0m
Jul 15, 2021 7:39:35 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3656
Jul 15, 2021 7:39:41 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3674
[0m2021.07.15 07:40:18 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 07:40:18 INFO  time: compiled ht3 in 0.25s[0m
[0m2021.07.15 07:40:34 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 07:40:34 INFO  time: compiled ht3 in 0.25s[0m
Jul 15, 2021 8:30:47 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3812
Jul 15, 2021 8:31:07 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3831
[0m2021.07.15 08:31:22 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 08:31:22 INFO  time: compiled ht3 in 0.87s[0m
Jul 15, 2021 8:35:38 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4048
Jul 15, 2021 8:35:44 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4059
Jul 15, 2021 8:35:50 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4080
Jul 15, 2021 8:35:56 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4088
Jul 15, 2021 8:36:01 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4096
[0m2021.07.15 08:36:16 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 08:36:16 INFO  time: compiled ht3 in 0.49s[0m
Jul 15, 2021 8:36:30 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4144
Jul 15, 2021 8:37:16 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4162
[0m2021.07.15 08:39:16 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 08:39:16 INFO  time: compiled ht3 in 0.89s[0m
Jul 15, 2021 8:39:24 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4316
Jul 15, 2021 8:39:43 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4335
[0m2021.07.15 08:39:46 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 08:39:46 INFO  time: compiled ht3 in 0.36s[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in (String, Int)RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 453, 453, 466)
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in Array[(String, Int)]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 447, 447, 467)
[0m2021.07.15 08:40:54 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 08:40:54 INFO  time: compiled ht3 in 0.27s[0m
something's wrong: no file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala in Array[String]RangePosition(file:///home/hwait/dev/ht3/src/main/scala/ru.otus.bigdataml.ht3/DataTransformer.scala, 1019, 1019, 1032)
Jul 15, 2021 8:41:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4426
Jul 15, 2021 8:41:10 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4442
[0m2021.07.15 08:41:24 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 08:41:24 INFO  time: compiled ht3 in 0.25s[0m
Jul 15, 2021 8:41:29 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4473
Jul 15, 2021 8:41:35 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4486
Jul 15, 2021 8:41:55 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4521
Jul 15, 2021 8:42:03 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4538
[0m2021.07.15 08:42:07 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 08:42:07 INFO  time: compiled ht3 in 0.25s[0m
[0m2021.07.15 08:42:22 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 08:42:22 INFO  time: compiled ht3 in 0.32s[0m
[0m2021.07.15 08:43:20 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 08:43:20 INFO  time: compiled ht3 in 0.27s[0m
[0m2021.07.15 08:46:37 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 08:46:37 INFO  time: compiled ht3 in 0.74s[0m
[0m2021.07.15 08:47:02 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 08:47:02 INFO  time: compiled ht3 in 0.62s[0m
[0m2021.07.15 08:48:31 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 08:48:31 INFO  time: compiled ht3 in 0.27s[0m
[0m2021.07.15 08:49:23 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 08:49:26 INFO  time: compiled ht3 in 2.7s[0m
Jul 15, 2021 8:51:31 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5126
Jul 15, 2021 8:52:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5201
Jul 15, 2021 8:52:04 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5210
Jul 15, 2021 8:52:14 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5224
Jul 15, 2021 8:52:16 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5234
[0m2021.07.15 08:53:25 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 08:53:26 INFO  time: compiled ht3 in 1.24s[0m
[0m2021.07.15 08:53:37 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.5' (since 2h 12m 55.242s)[0m
[0m2021.07.15 08:53:37 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.15 08:53:37 INFO  Listening for transport dt_socket at address: 52975[0m
[0m2021.07.15 08:53:37 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.15 08:53:37 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
[0m2021.07.15 08:53:37 INFO  Trying to attach to remote debuggee VM localhost:52975 .[0m
[0m2021.07.15 08:53:37 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.15 08:53:38 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.15 08:53:38 ERROR 21/07/15 08:53:38 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.29.131.255 instead (on interface eth0)[0m
[0m2021.07.15 08:53:38 ERROR 21/07/15 08:53:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.15 08:53:39 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.15 08:53:39 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.15 08:53:39 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.15 08:53:39 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.15 08:53:39 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.15 08:53:39 ERROR 21/07/15 08:53:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.15 08:53:49 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.15 08:53:51 INFO  Data: loaded [360130.0:181][0m
[0m2021.07.15 08:53:51 ERROR 21/07/15 08:53:51 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.15 08:53:59 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 08:53:59 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.15 08:53:59 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 08:53:59 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 08:53:59 INFO  |(179,[0,6,9,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 08:53:59 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 08:53:59 INFO  |(179,[0,5,8,18,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 08:53:59 INFO  |(179,[0,7,8,18,23...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 08:53:59 INFO  |(179,[0,7,8,18,24...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 08:53:59 INFO  |(179,[1,3,9,18,24...|                (3,[1],[1.0])|             (5,[0],[1.0])|             (2,[1],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 08:53:59 INFO  |(179,[0,6,8,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 08:53:59 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 08:53:59 INFO  |(179,[0,5,8,11,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 08:53:59 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 08:53:59 INFO  only showing top 10 rows[0m
[0m2021.07.15 08:53:59 INFO  [0m
[0m2021.07.15 08:54:04 ERROR 21/07/15 08:54:04 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.15 08:54:04 ERROR 21/07/15 08:54:04 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.15 08:54:04 ERROR 21/07/15 08:54:05 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.15 08:54:04 ERROR 21/07/15 08:54:05 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.15 08:54:09 INFO  Coefficients: [-2.5967280171047705E-4,-4.4943213628811436E-4,-5.4258303655815764E-5,1.496828991965769E-4,-7.046664354626967E-5,-6.731944201723519E-5,-2.984041246866912E-5,-0.0013415632429407394,-0.006042132729059748,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-3.602594389461736E-4,-3.734838709209616E-4,-9.110805766567136E-5,1.9839813799671659E-4,8.351655751683134E-5,0.009670620656878591,-9.866009393400136E-4,-0.0011069801781385593,-7.091407779834133E-4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5.247108191509057E-4,0.003297315376620463,5.132567547260736E-4,-0.0017698841368443362,-5.609433170648613E-4,2.01474742127341E-4,-0.0017104623518883536,-3.10129126656292E-4,5.258280110111026E-5,-0.0017271415061610733,-8.169727913092738E-5,-3.054978207210119E-4,-0.0015006137784203271,7.099771608425419E-5,0.0,1.836359669475063E-4,9.237518902616978E-5,3.838603014571523E-5,8.18307890479239E-5,-7.792007242442824E-4,8.603083874078855E-5,-0.0010452079255033953,-0.0010233833120702417,-1.4241909213358708E-4,-1.424148575665667E-4,0.0,-6.774466534435696E-4,0.0,9.867083263805458E-5,-1.345174373244168E-4,-1.3291200703640155E-4,2.751741490355994E-4,-1.241016238822601E-4,-9.666416034505548E-5,-6.490411145985246E-5,-1.4515493606290572E-4,-8.355551321407808E-4,0.0,0.0,0.0,-0.0031562963405762246,0.0,0.0,-1.2381444214072632E-4,1.4368590970099238E-4,-7.619483523034831E-4,1.256569944723904E-4,2.4695204335144303E-4,-6.496976945629104E-4,0.010541546737744958,-0.002151990076253845,0.0,0.0,0.0,0.0,-0.002697237707375636,-0.002404897571446099,0.0,-0.00338711768743893,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0010975938419123558,0.0,-2.2530852477858668E-4,0.0,0.0,-1.1412316272077245E-4,4.9426753235026324E-5,1.0325801339019014E-4] Intercept: 0.21054139666514562[0m
[0m2021.07.15 08:54:09 INFO  numIterations: 0[0m
[0m2021.07.15 08:54:09 INFO  objectiveHistory: [0.0][0m
[0m2021.07.15 08:54:09 INFO  RMSE: 0.3817729869930512[0m
[0m2021.07.15 08:54:09 INFO  r2: 0.013178296985180693[0m
[0m2021.07.15 08:54:09 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
[0m2021.07.15 08:55:21 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 08:55:23 INFO  time: compiled ht3 in 1.8s[0m
[0m2021.07.15 08:55:58 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 08:55:59 INFO  time: compiled ht3 in 1.03s[0m
[0m2021.07.15 08:56:03 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 08:56:04 INFO  time: compiled ht3 in 1.09s[0m
[0m2021.07.15 08:56:13 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 08:56:13 INFO  time: compiled ht3 in 0.95s[0m
Jul 15, 2021 8:56:24 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5523
Jul 15, 2021 8:56:34 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5531
[0m2021.07.15 08:56:58 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 08:56:59 INFO  time: compiled ht3 in 1.29s[0m
[0m2021.07.15 08:56:59 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 08:56:59 INFO  time: compiled ht3 in 0.11s[0m
Jul 15, 2021 8:57:01 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5565
Jul 15, 2021 8:57:15 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5574
Jul 15, 2021 8:57:18 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5580
Jul 15, 2021 8:57:34 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5598
Jul 15, 2021 8:57:48 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5613
Jul 15, 2021 8:57:48 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5619
Jul 15, 2021 8:59:25 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5674
Jul 15, 2021 8:59:27 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5680
Jul 15, 2021 9:00:13 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5687
Jul 15, 2021 9:00:25 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5695
Jul 15, 2021 9:01:50 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5701
Jul 15, 2021 9:02:20 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5713
Jul 15, 2021 9:05:41 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5772
Jul 15, 2021 9:05:43 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5778
Jul 15, 2021 9:05:59 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5791
Jul 15, 2021 9:07:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5846
Jul 15, 2021 9:07:07 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5858
Jul 15, 2021 9:07:10 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 33 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 33 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 33 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:07:12 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5878
Jul 15, 2021 9:07:19 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5885
Jul 15, 2021 9:07:19 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 33 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 33 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 33 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:07:26 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:07:28 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:07:34 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:07:36 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:07:37 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:07:38 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:07:39 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:07:39 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:07:40 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:07:42 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:07:45 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:07:46 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 29 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:07:52 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5962
Jul 15, 2021 9:07:54 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5968
Jul 15, 2021 9:08:15 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:08:19 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:08:21 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:08:21 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:08:28 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:08:34 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:08:35 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:08:37 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Jul 15, 2021 9:08:39 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.07.15 09:08:40 INFO  compiling ht3 (1 scala source)[0m
Jul 15, 2021 9:08:41 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.07.15 09:08:40 INFO  time: compiled ht3 in 0.48s[0m
Jul 15, 2021 9:08:41 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: 30 is not a valid line number, allowed [0..28]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:37)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:32)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.MtagsEnrichments$XtensionPositionLspInverse.toMeta(MtagsEnrichments.scala:121)
	at scala.meta.internal.parsing.Trees.$anonfun$findLastEnclosingAt$5(Trees.scala:71)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.Trees.findLastEnclosingAt(Trees.scala:70)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:70)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.07.15 09:09:14 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:09:16 INFO  time: compiled ht3 in 1.32s[0m
[0m2021.07.15 09:09:16 INFO  compiling ht3 (4 scala sources)[0m
[0m2021.07.15 09:09:18 INFO  time: compiled ht3 in 2.16s[0m
Jul 15, 2021 9:09:39 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6151
[0m2021.07.15 09:09:56 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:09:56 INFO  time: compiled ht3 in 0.14s[0m
[0m2021.07.15 09:09:56 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:09:56 INFO  time: compiled ht3 in 30ms[0m
[0m2021.07.15 09:12:31 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:12:31 INFO  time: compiled ht3 in 0.22s[0m
Jul 15, 2021 9:12:51 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6236
Jul 15, 2021 9:12:57 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6250
[0m2021.07.15 09:13:00 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:13:00 INFO  time: compiled ht3 in 0.19s[0m
Jul 15, 2021 9:13:12 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6272
Jul 15, 2021 9:13:20 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6287
Jul 15, 2021 9:13:22 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6293
Jul 15, 2021 9:13:49 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6306
Jul 15, 2021 9:14:02 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6318
Jul 15, 2021 9:14:05 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6326
[0m2021.07.15 09:14:16 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:14:17 INFO  time: compiled ht3 in 1.24s[0m
[0m2021.07.15 09:14:17 INFO  compiling ht3 (4 scala sources)[0m
[0m2021.07.15 09:14:17 INFO  time: compiled ht3 in 0.24s[0m
[0m2021.07.15 09:14:25 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:14:26 INFO  time: compiled ht3 in 1.19s[0m
[0m2021.07.15 09:14:26 INFO  compiling ht3 (4 scala sources)[0m
[0m2021.07.15 09:14:26 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.5' (since 2h 33m 45.102s)[0m
[0m2021.07.15 09:14:26 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:14:26 INFO  time: compiled ht3 in 12ms[0m
[0m2021.07.15 09:14:26 INFO  compiling ht3 (4 scala sources)[0m
[0m2021.07.15 09:14:27 INFO  time: compiled ht3 in 0.3s[0m
[0m2021.07.15 09:14:27 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.5' (since 2h 33m 45.454s)[0m
[0m2021.07.15 09:14:27 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:14:27 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:14:27 INFO  time: compiled ht3 in 0.39s[0m
[0m2021.07.15 09:14:27 INFO  compiling ht3 (4 scala sources)[0m
[0m2021.07.15 09:14:27 INFO  time: compiled ht3 in 3ms[0m
[0m2021.07.15 09:14:27 INFO  compiling ht3 (4 scala sources)[0m
[0m2021.07.15 09:14:27 INFO  time: compiled ht3 in 0.26s[0m
Jul 15, 2021 9:14:27 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
java.util.concurrent.CompletionException: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:29)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:26)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:237)
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:229)
	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417)
	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
	... 4 more

[0m2021.07.15 09:14:30 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.5' (since 2h 33m 48.522s)[0m
[0m2021.07.15 09:14:30 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:14:30 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:14:31 INFO  time: compiled ht3 in 1.14s[0m
[0m2021.07.15 09:14:31 INFO  compiling ht3 (4 scala sources)[0m
[0m2021.07.15 09:14:31 INFO  time: compiled ht3 in 2ms[0m
[0m2021.07.15 09:14:31 INFO  compiling ht3 (4 scala sources)[0m
[0m2021.07.15 09:14:31 INFO  time: compiled ht3 in 0.23s[0m
[0m2021.07.15 09:14:31 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:14:31 INFO  time: compiled ht3 in 0.32s[0m
[0m2021.07.15 09:14:31 INFO  compiling ht3 (4 scala sources)[0m
[0m2021.07.15 09:14:31 INFO  time: compiled ht3 in 0.25s[0m
Jul 15, 2021 9:14:32 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
java.util.concurrent.CompletionException: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:29)
	at scala.concurrent.java8.FuturesConvertersImpl$CF.apply(FutureConvertersImpl.scala:26)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: scala.meta.internal.metals.MetalsBspException: BSP connection failed in the attempt to get: DebugSessionAddress.  Compilation not successful
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:237)
	at scala.meta.internal.metals.BuildServerConnection$$anonfun$1.applyOrElse(BuildServerConnection.scala:229)
	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417)
	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
	... 4 more

Jul 15, 2021 9:14:33 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6385
Jul 15, 2021 9:14:42 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6396
Jul 15, 2021 9:14:47 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6406
Jul 15, 2021 9:14:52 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6422
[0m2021.07.15 09:15:01 INFO  compiling ht3 (4 scala sources)[0m
[0m2021.07.15 09:15:03 INFO  time: compiled ht3 in 1.78s[0m
[0m2021.07.15 09:15:03 INFO  compiling ht3 (4 scala sources)[0m
[0m2021.07.15 09:15:04 INFO  time: compiled ht3 in 1.14s[0m
[0m2021.07.15 09:15:04 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 09:15:04 INFO  time: compiled ht3 in 0.45s[0m
Jul 15, 2021 9:15:05 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6466
[0m2021.07.15 09:15:07 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.5' (since 2h 34m 25.82s)[0m
[0m2021.07.15 09:15:07 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.15 09:15:07 INFO  Listening for transport dt_socket at address: 59305[0m
[0m2021.07.15 09:15:07 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.15 09:15:07 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
[0m2021.07.15 09:15:07 INFO  Trying to attach to remote debuggee VM localhost:59305 .[0m
[0m2021.07.15 09:15:07 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.15 09:15:09 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.15 09:15:09 ERROR 21/07/15 09:15:09 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.29.131.255 instead (on interface eth0)[0m
[0m2021.07.15 09:15:09 ERROR 21/07/15 09:15:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.15 09:15:10 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.15 09:15:10 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.15 09:15:10 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.15 09:15:10 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.15 09:15:10 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.15 09:15:10 ERROR 21/07/15 09:15:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.15 09:15:19 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.15 09:15:21 INFO  Data: loaded [360130.0:181][0m
[0m2021.07.15 09:15:21 ERROR 21/07/15 09:15:22 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.15 09:15:28 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 09:15:28 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.15 09:15:28 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 09:15:28 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:15:28 INFO  |(179,[0,6,9,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:15:28 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:15:28 INFO  |(179,[0,5,8,18,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:15:28 INFO  |(179,[0,7,8,18,23...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:15:28 INFO  |(179,[0,7,8,18,24...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:15:28 INFO  |(179,[1,3,9,18,24...|                (3,[1],[1.0])|             (5,[0],[1.0])|             (2,[1],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:15:28 INFO  |(179,[0,6,8,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:15:28 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:15:28 INFO  |(179,[0,5,8,11,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:15:28 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 09:15:28 INFO  only showing top 10 rows[0m
[0m2021.07.15 09:15:28 INFO  [0m
[0m2021.07.15 09:15:33 ERROR 21/07/15 09:15:33 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.15 09:15:33 ERROR 21/07/15 09:15:33 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.15 09:15:33 ERROR 21/07/15 09:15:34 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.15 09:15:33 ERROR 21/07/15 09:15:34 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.15 09:15:37 INFO  Coefficients: [-2.5967280171047705E-4,-4.4943213628811436E-4,-5.4258303655815764E-5,1.496828991965769E-4,-7.046664354626967E-5,-6.731944201723519E-5,-2.984041246866912E-5,-0.0013415632429407394,-0.006042132729059748,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-3.602594389461736E-4,-3.734838709209616E-4,-9.110805766567136E-5,1.9839813799671659E-4,8.351655751683134E-5,0.009670620656878591,-9.866009393400136E-4,-0.0011069801781385593,-7.091407779834133E-4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5.247108191509057E-4,0.003297315376620463,5.132567547260736E-4,-0.0017698841368443362,-5.609433170648613E-4,2.01474742127341E-4,-0.0017104623518883536,-3.10129126656292E-4,5.258280110111026E-5,-0.0017271415061610733,-8.169727913092738E-5,-3.054978207210119E-4,-0.0015006137784203271,7.099771608425419E-5,0.0,1.836359669475063E-4,9.237518902616978E-5,3.838603014571523E-5,8.18307890479239E-5,-7.792007242442824E-4,8.603083874078855E-5,-0.0010452079255033953,-0.0010233833120702417,-1.4241909213358708E-4,-1.424148575665667E-4,0.0,-6.774466534435696E-4,0.0,9.867083263805458E-5,-1.345174373244168E-4,-1.3291200703640155E-4,2.751741490355994E-4,-1.241016238822601E-4,-9.666416034505548E-5,-6.490411145985246E-5,-1.4515493606290572E-4,-8.355551321407808E-4,0.0,0.0,0.0,-0.0031562963405762246,0.0,0.0,-1.2381444214072632E-4,1.4368590970099238E-4,-7.619483523034831E-4,1.256569944723904E-4,2.4695204335144303E-4,-6.496976945629104E-4,0.010541546737744958,-0.002151990076253845,0.0,0.0,0.0,0.0,-0.002697237707375636,-0.002404897571446099,0.0,-0.00338711768743893,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0010975938419123558,0.0,-2.2530852477858668E-4,0.0,0.0,-1.1412316272077245E-4,4.9426753235026324E-5,1.0325801339019014E-4] Intercept: 0.21054139666514562[0m
[0m2021.07.15 09:15:37 INFO  numIterations: 0[0m
[0m2021.07.15 09:15:37 INFO  objectiveHistory: [0.0][0m
[0m2021.07.15 09:15:37 INFO  RMSE: 0.3817729869930512[0m
[0m2021.07.15 09:15:37 INFO  r2: 0.013178296985180693[0m
[0m2021.07.15 09:16:14 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
Jul 15, 2021 9:16:15 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6506
Jul 15, 2021 9:16:20 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6512
[0m2021.07.15 09:16:38 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:16:39 INFO  time: compiled ht3 in 1.26s[0m
[0m2021.07.15 09:16:39 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:16:39 INFO  time: compiled ht3 in 0.42s[0m
[0m2021.07.15 09:16:51 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:16:51 INFO  time: compiled ht3 in 0.17s[0m
[0m2021.07.15 09:16:53 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:16:53 INFO  time: compiled ht3 in 0.25s[0m
[0m2021.07.15 09:17:05 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:17:06 INFO  time: compiled ht3 in 1.24s[0m
[0m2021.07.15 09:17:06 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:17:06 INFO  time: compiled ht3 in 0.1s[0m
Jul 15, 2021 9:17:09 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6598
Jul 15, 2021 9:17:13 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6610
[0m2021.07.15 09:17:17 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 09:17:19 INFO  time: compiled ht3 in 1.5s[0m
Jul 15, 2021 9:17:51 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6634
[0m2021.07.15 09:19:58 INFO  compiling ht3 (1 scala source)[0m
Jul 15, 2021 9:20:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6897
[0m2021.07.15 09:20:01 INFO  time: compiled ht3 in 3.2s[0m
Jul 15, 2021 9:20:14 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6907
Jul 15, 2021 9:21:39 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7098
Jul 15, 2021 9:22:32 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7113
Jul 15, 2021 9:22:34 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7119
Jul 15, 2021 9:23:06 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7146
Jul 15, 2021 9:24:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7294
[0m2021.07.15 09:24:04 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:24:04 INFO  time: compiled ht3 in 0.2s[0m
[0m2021.07.15 09:24:04 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:24:04 INFO  time: compiled ht3 in 0.17s[0m
Jul 15, 2021 9:24:10 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7306
[0m2021.07.15 09:24:15 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:24:15 INFO  time: compiled ht3 in 0.2s[0m
Jul 15, 2021 9:24:16 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7320
[0m2021.07.15 09:24:26 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:24:26 INFO  time: compiled ht3 in 0.46s[0m
Jul 15, 2021 9:24:36 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7364
[0m2021.07.15 09:24:45 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:24:46 INFO  time: compiled ht3 in 1.79s[0m
[0m2021.07.15 09:24:46 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 09:24:46 INFO  time: compiled ht3 in 0.64s[0m
[0m2021.07.15 09:24:53 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:24:53 INFO  time: compiled ht3 in 1s[0m
Jul 15, 2021 9:24:54 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7404
Jul 15, 2021 9:25:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7411
Jul 15, 2021 9:25:04 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7418
Jul 15, 2021 9:25:07 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7425
Jul 15, 2021 9:25:10 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7434
Jul 15, 2021 9:26:19 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7491
Jul 15, 2021 9:26:24 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7503
[0m2021.07.15 09:26:31 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:26:31 INFO  time: compiled ht3 in 0.96s[0m
[0m2021.07.15 09:26:33 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.5' (since 2h 45m 52.083s)[0m
[0m2021.07.15 09:26:33 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.15 09:26:33 INFO  Listening for transport dt_socket at address: 50599[0m
[0m2021.07.15 09:26:33 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.15 09:26:34 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
[0m2021.07.15 09:26:33 INFO  Trying to attach to remote debuggee VM localhost:50599 .[0m
[0m2021.07.15 09:26:33 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.15 09:26:35 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.15 09:26:35 ERROR 21/07/15 09:26:35 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.29.131.255 instead (on interface eth0)[0m
[0m2021.07.15 09:26:35 ERROR 21/07/15 09:26:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.15 09:26:36 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.15 09:26:36 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.15 09:26:36 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.15 09:26:36 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.15 09:26:36 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.15 09:26:36 ERROR 21/07/15 09:26:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.15 09:26:46 INFO  Day 2018-02-01 loaded 360130.0 rows.[0m
[0m2021.07.15 09:26:48 INFO  Data: loaded [360130.0:181][0m
[0m2021.07.15 09:26:48 ERROR 21/07/15 09:26:49 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.15 09:26:57 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 09:26:57 INFO  |            features|instanceId_objectType_num_enc|audit_resourceType_num_enc|metadata_ownerType_num_enc|membership_status_num_enc|log_ownerId_enc|log_authorId_enc|[0m
[0m2021.07.15 09:26:57 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 09:26:57 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:26:57 INFO  |(179,[0,6,9,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[1],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:26:57 INFO  |(179,[0,6,8,11,23...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:26:57 INFO  |(179,[0,5,8,18,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:26:57 INFO  |(179,[0,7,8,18,23...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[4],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:26:57 INFO  |(179,[0,7,8,18,24...|                (3,[0],[1.0])|             (5,[4],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:26:57 INFO  |(179,[1,3,9,18,24...|                (3,[1],[1.0])|             (5,[0],[1.0])|             (2,[1],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:26:57 INFO  |(179,[0,6,8,18,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[8],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:26:57 INFO  |(179,[0,6,8,11,24...|                (3,[0],[1.0])|             (5,[3],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:26:57 INFO  |(179,[0,5,8,11,24...|                (3,[0],[1.0])|             (5,[2],[1.0])|             (2,[0],[1.0])|            (9,[1],[1.0])|  (6,[5],[1.0])|   (7,[6],[1.0])|[0m
[0m2021.07.15 09:26:57 INFO  +--------------------+-----------------------------+--------------------------+--------------------------+-------------------------+---------------+----------------+[0m
[0m2021.07.15 09:26:57 INFO  only showing top 10 rows[0m
[0m2021.07.15 09:26:57 INFO  [0m
[0m2021.07.15 09:27:05 ERROR 21/07/15 09:27:05 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.15 09:27:05 ERROR 21/07/15 09:27:05 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.15 09:27:05 ERROR 21/07/15 09:27:05 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.15 09:27:05 ERROR 21/07/15 09:27:05 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.15 09:27:12 INFO  Coefficients: [0.009130436908289897,-0.00834616122862215,-0.00912322873110316,-0.00834616122862217,-0.009123228731103168,-0.021694887468464374,0.012020728581990544,-6.358890205125824E-4,0.005824021990680592,-0.00582402199068071,2.7994668007986037E-4,-0.00995084110756787,-0.01624861618814586,0.016498618168301088,0.044021944339682366,0.011359978306070315,0.0,-0.016506592638379853,0.00976066308476033,0.0,-0.006810969652474216,-0.009558176956569285,-0.007006567068696456,-5.542236554375846E-5,8.139027026653705E-4,0.0020647568146879448,-0.03491275933718076,-0.019973780288067552,-0.0063114962927788626,-0.006408180762048964,-4.0151698435840266E-4,0.0015050794038399767,-2.53340811519005E-4,-2.1763942147126495E-4,-6.055136527021207E-5,1.2176487427323293E-4,-7.194690673383563E-5,-6.907053350928248E-5,-2.9164892194650866E-5,-8.692334460785217E-4,-0.005688784667934936,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-3.751101316839056E-4,-4.454121093097423E-4,-1.246310944640308E-4,1.7550166896164633E-4,3.1894707387730684E-5,0.009869780585934972,-9.27418593752465E-4,-8.164104071414046E-4,-5.836512356532031E-4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4.576318888588872E-4,0.003079422443541922,5.273464061285001E-4,-0.0018170831914479414,-5.70818706799447E-4,2.485707182823101E-4,-0.001743006664750665,-3.0775339454273325E-4,6.076287220910254E-5,-0.0017591088069716175,-2.8847660065498004E-5,-2.9223050579437E-4,-0.0014582492522515569,7.087688004678934E-5,0.0,1.8144223960696692E-4,9.15279125309079E-5,3.668335192221866E-5,8.36623638707088E-5,-9.107358164012245E-4,8.888091181832138E-5,-0.0010730804390293954,-0.0010505122656497732,-1.5371751319881747E-4,-1.5371377505576995E-4,0.0,-8.272387807117944E-4,0.0,9.990452626758994E-5,-1.480834011096533E-4,-1.432936263939866E-4,2.756626645281736E-4,-9.90830443203045E-5,-8.71514250056541E-5,-5.620262022340575E-5,-1.251188627917052E-4,-5.849129783306857E-4,0.0,0.0,0.0,-0.0016204297769650774,0.0,0.0,-1.0348761068878344E-4,1.4397465718673526E-4,-8.952027684950608E-4,1.2689159935204482E-4,6.271016852346384E-4,-4.3975948566388814E-4,0.011332035340281769,-9.394126481169746E-4,0.0,0.0,0.0,0.0,-0.0021192521667907902,-0.0012469037183949969,0.0,-0.0031589416662900563,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-6.223831843925006E-4,0.0,-3.6412702822834E-4,0.0,0.0,-1.2608647640710392E-4,5.1798958578283435E-5,1.0762566332198847E-4] Intercept: 0.1928624937224643[0m
[0m2021.07.15 09:27:12 INFO  numIterations: 0[0m
[0m2021.07.15 09:27:12 INFO  objectiveHistory: [0.0][0m
[0m2021.07.15 09:27:12 INFO  RMSE: 0.38046556383875324[0m
[0m2021.07.15 09:27:12 INFO  r2: 0.019925680291205006[0m
[0m2021.07.15 09:27:32 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.SparkMLModelTraining][0m
Jul 15, 2021 9:27:43 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7532
Jul 15, 2021 9:27:46 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7539
Jul 15, 2021 9:28:04 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7549
Jul 15, 2021 9:28:13 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7568
Jul 15, 2021 9:28:52 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7618
Jul 15, 2021 9:29:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7628
Jul 15, 2021 9:29:15 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7679
Jul 15, 2021 9:29:18 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7685
[0m2021.07.15 09:29:22 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:29:22 INFO  time: compiled ht3 in 0.28s[0m
Jul 15, 2021 9:29:32 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7699
Jul 15, 2021 9:29:34 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7706
[0m2021.07.15 09:29:39 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:29:41 INFO  time: compiled ht3 in 1.58s[0m
[0m2021.07.15 09:30:00 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:30:01 INFO  time: compiled ht3 in 1.48s[0m
[0m2021.07.15 09:32:34 INFO  Deduplicating compilation of ht3 from bsp client 'Metals 0.10.5' (since 2h 51m 53.1s)[0m
[0m2021.07.15 09:32:35 INFO  tracing is disabled for protocol dap-server, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-server.trace.json[0m
[0m2021.07.15 09:32:34 INFO  Listening for transport dt_socket at address: 56011[0m
[0m2021.07.15 09:32:35 INFO  tracing is disabled for protocol dap-client, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/hwait/.cache/metals/dap-client.trace.json[0m
[0m2021.07.15 09:32:35 INFO  Starting debug proxy for [ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka][0m
[0m2021.07.15 09:32:34 INFO  Trying to attach to remote debuggee VM localhost:56011 .[0m
[0m2021.07.15 09:32:34 INFO  Attaching to debuggee VM succeeded.[0m
[0m2021.07.15 09:32:36 ERROR Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m2021.07.15 09:32:36 ERROR 21/07/15 09:32:36 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.29.131.255 instead (on interface eth0)[0m
[0m2021.07.15 09:32:36 ERROR 21/07/15 09:32:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m2021.07.15 09:32:37 ERROR WARNING: An illegal reflective access operation has occurred[0m
[0m2021.07.15 09:32:37 ERROR WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hwait/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/3.1.1/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)[0m
[0m2021.07.15 09:32:37 ERROR WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform[0m
[0m2021.07.15 09:32:37 ERROR WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations[0m
[0m2021.07.15 09:32:37 ERROR WARNING: All illegal access operations will be denied in a future release[0m
[0m2021.07.15 09:32:37 ERROR 21/07/15 09:32:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m2021.07.15 09:32:39 ERROR 21/07/15 09:32:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.[0m
[0m2021.07.15 09:32:39 ERROR 21/07/15 09:32:40 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.[0m
[0m2021.07.15 09:32:40 ERROR 21/07/15 09:32:40 WARN KafkaUtils: overriding enable.auto.commit to false for executor[0m
[0m2021.07.15 09:32:40 ERROR 21/07/15 09:32:40 WARN KafkaUtils: overriding auto.offset.reset to none for executor[0m
[0m2021.07.15 09:32:40 ERROR 21/07/15 09:32:40 WARN KafkaUtils: overriding executor group.id to spark-executor-spark_ml_testing[0m
[0m2021.07.15 09:32:40 ERROR 21/07/15 09:32:40 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135[0m
[0m2021.07.15 09:33:05 ERROR 21/07/15 09:33:05 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.[0m
[0m2021.07.15 09:33:06 INFO  Data: loaded [14.0:154][0m
[0m2021.07.15 09:33:10 ERROR 21/07/15 09:33:10 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS[0m
[0m2021.07.15 09:33:10 ERROR 21/07/15 09:33:10 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS[0m
[0m2021.07.15 09:33:10 ERROR 21/07/15 09:33:10 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK[0m
[0m2021.07.15 09:33:10 ERROR 21/07/15 09:33:10 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK[0m
[0m2021.07.15 09:33:10 INFO  Coefficients: [0.0,0.0,0.0,0.0,0.0,-0.015357359499108063,0.015357359499108063,0.0,0.0031911166324822375,-0.0031911166324821725,0.0,0.007752056578233475,0.0,0.0,0.0,0.0,0.0,0.0,-0.0077520565782335,0.0,0.0,0.0,0.0,-0.006579286219752006,0.006579286219752017,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.002168974170056218,0.0,-0.00219706554242747,0.002040118243067591,0.021170137590557605,0.02092149455788762,0.030139967237572115,0.0,-0.003148043905232804,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.005805395510010588,-0.005844161392854591,0.0049821394534083135,0.006580163812384492,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.006539475820468376,-0.013992842228608538,0.005309748855978061,0.01909336898701727,0.0013615217914239122,-0.008897898656522778,0.001375066634390933,0.0,0.0,0.0013750666343909325,0.0026959696383693685,0.0,-0.008405445696380808,-0.007443446976621948,0.0,-0.003911622740073693,-0.0037004058529109198,-0.003871865794988387,0.0023189834858339357,0.0,0.01811450445056881,0.0,0.0,-0.005047956928525112,-0.0050479569285251085,0.0,0.0,0.0,0.002139622448188066,-0.0019215472176595581,-0.005647996038176698,-0.004583537492243907,0.0,-0.006776601562378743,-0.0034396618155247576,-0.008018859671915186,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.006898025562185485,0.0,0.010696580665486847,-0.005805395510010585,0.0,0.007159770603601047,0.014953111540046115,0.0,0.0,0.0,0.0,0.010851462844734937,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.001706756808125768,0.0,0.0,-0.0018978783654197834,-0.006172607526524945,0.0013750666343909047] Intercept: -0.09565885388902438[0m
[0m2021.07.15 09:33:10 INFO  RMSE: 0.14496085849880072, r2: 0.683178807895767[0m
[0m2021.07.15 09:33:30 INFO  Data: loaded [12.0:154][0m
[0m2021.07.15 09:33:32 INFO  Coefficients: [0.017061336721700623,0.0,-0.01706133672170084,0.0,-0.017061336721700803,-0.02763419596610852,0.029181187344488622,0.0,0.033382172948896845,-0.03338217294889681,0.0,-0.01693110073895371,0.0,0.0,0.0,0.0,0.0,0.0,0.016931100738953476,0.0,0.0,0.0,0.1027465084345128,0.0061472707623665395,-0.042042689670702536,0.0,0.0,0.0,0.0,-0.014867188391910318,0.0,0.014867188391910256,-0.0231602164281222,0.0,0.0017075582654631032,0.0,0.0034156884489660354,0.003004244808099278,-0.009636603269090101,-0.004925183674504411,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.009636603269090086,-0.00963660326909009,0.0,0.0,0.0,0.01970084508354777,-0.012685462269875884,0.044874845369766936,-0.009636603269090088,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.008665906748085138,0.013595921610321079,-0.023074448721345082,0.028300142360571365,-1.7347529370665045E-4,0.05173814904756319,2.6890642244742065E-4,0.0,0.0,2.689064224474282E-4,0.008705695685248236,0.0,0.03343278513444289,-0.00800596607128995,0.0,0.021860176642077893,0.018134902227013295,0.021855331461988144,0.012131345736493524,0.022008411553442214,-0.0055153519154797544,-0.00508642149841911,-0.00508642149841911,-0.004925183674504414,-0.004925183674504418,0.0,0.02200841155344217,0.0,2.6890642244743876E-4,-0.004925183674504416,-0.004925183674504415,0.014770179567972939,0.0,-0.005385733070591582,-0.005265357524943205,-0.006117055805700484,0.0,0.0,0.0,0.0,-0.004925183674504417,0.0,0.0,0.0,-0.011288120502135924,0.02200841155344218,-0.011912227261914012,0.0,0.0,-0.004380476229890143,0.023922850884716365,0.0,0.0,0.0,0.0,-0.011117613801715547,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.005011584625450478,-0.044034911149554705,2.6890642244629286E-4] Intercept: -0.1440515802716769[0m
[0m2021.07.15 09:33:32 INFO  RMSE: 0.1586515330831481, r2: 0.8657583522686227[0m
[0m2021.07.15 09:34:00 INFO  Data: loaded [17.0:154][0m
[0m2021.07.15 09:34:02 INFO  Coefficients: [0.011178145284617847,0.0,-0.011178145284617672,0.0,-0.011178145284617646,-0.020928000493717982,0.015956961105667054,0.0,0.010345284573336545,-0.010345284573336255,0.0,0.0042623756295408245,0.0,0.0,0.0,0.0,0.0,0.0,-0.004262375629540747,0.0,0.0,0.0,0.0,-0.0028613020770293165,0.002861302077029434,0.0,0.0,0.0,0.0,-0.02092800049371796,-0.011148866068258824,0.017107662166387955,0.003618566488069105,0.0,-0.005782725098401693,-0.007391351126848415,-0.009359888902711963,-0.00917783919476584,-0.001292945892174916,3.080033804054283E-4,-0.009159231633601111,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.005928097487708141,0.005012792991852247,-0.004692826641346364,0.04720467455190239,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.012822716998370974,-0.00100834691302888,0.013800908452650947,0.0049930490682619875,0.010718846175625985,-0.007042618080080004,0.010639462702901057,0.0,0.0,0.01063946270290107,-0.011627905794340889,0.0,-0.010603156582475222,0.008355474432043904,0.0,-0.0038232119684737513,-0.003653383270683502,0.026070733791337168,0.01021964532602109,0.0,0.027592542577178383,0.0,0.0,0.008192145513957253,0.00819214551395725,0.0,0.0,0.0,0.010510304944180754,0.01704292603479067,0.01249250180285614,0.005496842621802716,0.0,-0.002335833623927291,-0.0036192427129668604,-0.005713605467304103,0.0,0.0,0.0,0.0,-0.003671011122055919,0.0,0.0,0.0,2.5549258332906876E-4,0.0,0.005835762131668618,-0.002703997200311522,0.0,0.0050392329047792995,0.019407187441016136,0.0,0.0,0.0,0.0,-0.022015646348521312,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0016988477663577128,0.0,0.0,5.005026563782503E-4,-0.003140865766557081,0.01063946270290124] Intercept: -0.37550535191706513[0m
[0m2021.07.15 09:34:02 INFO  RMSE: 0.19348833359098475, r2: 0.639349817228304[0m
[0m2021.07.15 09:34:08 INFO  Canceling debug proxy for [ru.otus.bigdataml.ht3.SparkMLRegressionFromKafka][0m
[0m2021.07.15 09:34:08 INFO  Listening for transport dt_socket at address: 50141[0m
Jul 15, 2021 9:34:37 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7776
Jul 15, 2021 9:34:50 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7793
Jul 15, 2021 9:34:51 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7800
Jul 15, 2021 9:34:53 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7813
Jul 15, 2021 9:34:54 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7820
Jul 15, 2021 9:34:55 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7827
[0m2021.07.15 09:40:10 INFO  compiling ht3 (1 scala source)[0m
Jul 15, 2021 9:40:10 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8437
[0m2021.07.15 09:40:12 INFO  time: compiled ht3 in 2.11s[0m
Jul 15, 2021 9:40:56 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8479
Jul 15, 2021 9:41:02 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8488
[0m2021.07.15 09:41:14 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:41:14 INFO  time: compiled ht3 in 0.27s[0m
[0m2021.07.15 09:41:14 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:41:14 INFO  time: compiled ht3 in 0.15s[0m
Jul 15, 2021 9:41:19 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8505
Jul 15, 2021 9:41:55 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8529
Jul 15, 2021 9:41:57 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8535
Jul 15, 2021 9:42:01 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8542
[0m2021.07.15 09:42:07 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:42:08 INFO  time: compiled ht3 in 1.39s[0m
[0m2021.07.15 09:42:08 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:42:08 INFO  time: compiled ht3 in 86ms[0m
Jul 15, 2021 9:44:54 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8623
[0m2021.07.15 09:44:58 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:45:00 INFO  time: compiled ht3 in 1.29s[0m
[0m2021.07.15 09:45:00 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:45:00 INFO  time: compiled ht3 in 71ms[0m
Jul 15, 2021 9:45:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8632
Jul 15, 2021 9:45:19 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8653
[0m2021.07.15 09:45:26 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 09:45:27 INFO  time: compiled ht3 in 1.35s[0m
[0m2021.07.15 09:45:27 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:45:27 INFO  time: compiled ht3 in 86ms[0m
[0m2021.07.15 09:45:49 INFO  compiling ht3 (3 scala sources)[0m
Jul 15, 2021 9:45:50 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8711
[0m2021.07.15 09:45:50 INFO  time: compiled ht3 in 1.53s[0m
[0m2021.07.15 09:45:50 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 09:45:53 INFO  time: compiled ht3 in 2.82s[0m
[0m2021.07.15 09:45:53 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 09:45:53 INFO  time: compiled ht3 in 0.46s[0m
[0m2021.07.15 09:45:53 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 09:45:53 INFO  time: compiled ht3 in 0.16s[0m
[0m2021.07.15 09:46:09 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 09:46:10 INFO  time: compiled ht3 in 1.44s[0m
[0m2021.07.15 09:46:10 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 09:46:10 INFO  time: compiled ht3 in 0.17s[0m
[0m2021.07.15 09:47:48 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 09:47:50 INFO  time: compiled ht3 in 1.46s[0m
[0m2021.07.15 09:47:50 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 09:47:50 INFO  time: compiled ht3 in 0.15s[0m
Jul 15, 2021 9:47:59 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8923
[0m2021.07.15 09:48:15 INFO  compiling ht3 (4 scala sources)[0m
[0m2021.07.15 09:48:16 INFO  time: compiled ht3 in 1.5s[0m
[0m2021.07.15 09:48:16 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 09:48:17 INFO  time: compiled ht3 in 1.09s[0m
[0m2021.07.15 09:49:26 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:49:26 INFO  time: compiled ht3 in 0.98s[0m
[0m2021.07.15 09:49:33 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:49:34 INFO  time: compiled ht3 in 1s[0m
[0m2021.07.15 09:49:41 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:49:41 INFO  time: compiled ht3 in 0.97s[0m
Jul 15, 2021 9:49:50 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9130
[0m2021.07.15 09:50:13 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:50:14 INFO  time: compiled ht3 in 1.08s[0m
Jul 15, 2021 9:50:21 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9174
[0m2021.07.15 09:51:36 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:51:36 INFO  time: compiled ht3 in 0.91s[0m
Jul 15, 2021 9:51:38 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9370
Jul 15, 2021 9:51:48 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9378
[0m2021.07.15 09:55:51 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:55:52 INFO  time: compiled ht3 in 1.19s[0m
Jul 15, 2021 9:55:53 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9968
Jul 15, 2021 9:55:54 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9974
[0m2021.07.15 09:57:55 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:57:56 INFO  time: compiled ht3 in 1.13s[0m
[0m2021.07.15 09:58:56 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 09:58:58 INFO  time: compiled ht3 in 1.48s[0m
Jul 15, 2021 9:59:01 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 10424
[0m2021.07.15 10:02:28 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 10:02:28 INFO  time: compiled ht3 in 0.89s[0m
[0m2021.07.15 10:02:28 INFO  compiling ht3 (4 scala sources)[0m
[0m2021.07.15 10:02:29 INFO  time: compiled ht3 in 0.21s[0m
[0m2021.07.15 10:02:33 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 10:02:33 INFO  time: compiled ht3 in 0.84s[0m
[0m2021.07.15 10:02:33 INFO  compiling ht3 (4 scala sources)[0m
[0m2021.07.15 10:02:35 INFO  time: compiled ht3 in 0.43s[0m
[0m2021.07.15 10:02:49 INFO  compiling ht3 (2 scala sources)[0m
[0m2021.07.15 10:02:50 INFO  time: compiled ht3 in 1.23s[0m
[0m2021.07.15 10:02:50 INFO  compiling ht3 (3 scala sources)[0m
[0m2021.07.15 10:02:50 INFO  time: compiled ht3 in 0.48s[0m
Jul 15, 2021 10:02:56 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 10760
Jul 15, 2021 10:03:10 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 10774
Jul 15, 2021 10:03:11 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 10780
[0m2021.07.15 10:04:26 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 10:04:27 INFO  time: compiled ht3 in 1.06s[0m
Jul 15, 2021 10:04:55 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 10931
[0m2021.07.15 10:05:04 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 10:05:05 INFO  time: compiled ht3 in 1s[0m
Jul 15, 2021 10:05:54 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 11098
Jul 15, 2021 10:06:17 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 11136
Jul 15, 2021 10:06:56 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 11275
[0m2021.07.15 10:11:01 INFO  compiling ht3 (1 scala source)[0m
[0m2021.07.15 10:11:02 INFO  time: compiled ht3 in 1.44s[0m
Jul 15, 2021 10:29:37 AM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTrace
